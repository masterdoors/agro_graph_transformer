{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c41ddf7d-4569-4104-a2b1-3ac816fb97ac",
   "metadata": {},
   "source": [
    "# Load trade and production data (1993-2023)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d87871bd-9e8a-49b1-ab71-af7ec2cc12be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "trade_directory = \"trade_data\"\n",
    "\n",
    "trades = []\n",
    "for file in os.listdir(trade_directory):\n",
    "    if file.endswith(\".csv\"): \n",
    "        trades.append(pd.read_csv(os.path.join(trade_directory, file),sep=\",\",index_col=False,encoding = 'ISO-8859-1'))\n",
    "\n",
    "\n",
    "trade = pd.concat(trades,ignore_index=True, axis=0)\n",
    "trade = trade[[\"refYear\",\"reporterDesc\",\"partnerDesc\",\"flowDesc\",\"qty\",\"isAggregate\"]].fillna(0.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a42c56e-6eef-4d80-abd4-f71a4ec1a89e",
   "metadata": {},
   "source": [
    "# fill gaps with with different imputation strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "00291378-0263-4553-aa1a-00be081d9ae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def need2be_imputed(tmp,target=\"qty\"):\n",
    "    zero_stat = []\n",
    "    if len(tmp) > 0:\n",
    "        right_empty = 0\n",
    "        zeros = 0\n",
    "        flag = True\n",
    "        for i,row in tmp.reindex().sort_index(ascending=False).iterrows():\n",
    "            if  math.isnan(row[target]):\n",
    "                zeros += 1\n",
    "                zero_stat.append(1)\n",
    "                if flag:\n",
    "                    right_empty += 1    \n",
    "            else:\n",
    "                flag = False\n",
    "                zero_stat.append(0)\n",
    "        if right_empty < 3 and zeros > 0:\n",
    "            return True, 0, zero_stat\n",
    "        else:    \n",
    "            if zeros > 0 and right_empty == zeros:\n",
    "                return False, right_empty, zero_stat\n",
    "            else:\n",
    "                if zeros == 0:\n",
    "                    return False, 0, zero_stat\n",
    "                else:    \n",
    "                    return True, right_empty, zero_stat\n",
    "    else:    \n",
    "        return False, 0, zero_stat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d0b7e09-bb4c-49b1-a148-94e0f82c71c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original trade size: 117877\n",
      "Drop duplicates\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 211/211 [2:46:02<00:00, 47.21s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trade size after de-duplication: 91437\n",
      "Add missed values\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|█████████████████▏                  | 101/211 [2:34:37<6:21:13, 207.94s/it]"
     ]
    }
   ],
   "source": [
    "from sktime.transformations.series.impute import Imputer\n",
    "from sktime.forecasting.arima import AutoARIMA\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import warnings\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "import copy\n",
    "\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "warnings.simplefilter(action='ignore', category=UserWarning)\n",
    "np.seterr(divide = 'ignore') \n",
    "\n",
    "reporters = np.unique(trade['reporterDesc'])\n",
    "\n",
    "print(\"Original trade size:\", len(trade))\n",
    "\n",
    "# drop duplicate lines\n",
    "print(\"Drop duplicates\")\n",
    "for r in tqdm(reporters):\n",
    "    for partner in np.unique(trade[trade['reporterDesc'] == r][\"partnerDesc\"].to_numpy()):\n",
    "        for flow in [\"Import\",\"Export\"]:\n",
    "            years = np.unique(trade[\"refYear\"].to_numpy())\n",
    "            for year in years:\n",
    "                year_trade = trade[(trade['reporterDesc']==r)&(trade['flowDesc']==flow)&(trade['refYear']==year)&(trade['partnerDesc']==partner)]\n",
    "                if len(year_trade) > 1:\n",
    "                    remain = year_trade['qty'].idxmax()\n",
    "                    #print(r,partner,len(year_trade))\n",
    "                    for id_,_ in year_trade.iterrows():\n",
    "                        if id_ != remain:\n",
    "                            trade = trade.drop(id_)\n",
    "                   #print(\"Control:\",r,partner, len(trade[(trade['reporterDesc']==r)&(trade['flowDesc']==flow)&(trade['refYear']==year)&(trade['partnerDesc']==partner)]))        \n",
    "print(\"Trade size after de-duplication:\",len(trade))\n",
    "\n",
    "print(\"Add missed values\")\n",
    "for r in tqdm(reporters):\n",
    "    for partner in np.unique(trade[trade['reporterDesc'] == r][\"partnerDesc\"].to_numpy()):\n",
    "        for flow in [\"Import\",\"Export\"]:\n",
    "            for year in years:\n",
    "                if len(trade[(trade['reporterDesc']==r)&(trade['flowDesc']==flow)&(trade['refYear']==year)&(trade['partnerDesc']==partner)]) == 0:\n",
    "                     row =  pd.DataFrame({\"refYear\":year,\t\"reporterDesc\":r,\t\"partnerDesc\":partner,\t\"flowDesc\":flow,\"qty\":math.nan,\t\"isAggregate\":False},index=[0])\n",
    "                     trade = pd.concat([trade.loc[:],row],axis=0).reset_index(drop=True)\n",
    "\n",
    "print(\"Impute nans\")\n",
    "\n",
    "trade_copy = copy.deepcopy(trade)\n",
    "\n",
    "imputers = {\"ARIMA\":Imputer(method=\"forecaster\", forecaster=AutoARIMA(suppress_warnings=True)), \"FF\":Imputer(method=\"ffill\"),\"INTERPOLATION\":Imputer(method=\"linear\"),\"NO_IMP\":None}\n",
    "\n",
    "zero_cnts_years = {\"Import\":[],\"Export\":[]}\n",
    "\n",
    "years = np.sort(np.unique(trade[\"refYear\"].to_numpy())) # do not impute last 5 years\n",
    "\n",
    "for imp_type in imputers:\n",
    "    trade = copy.deepcopy(trade_copy)\n",
    "\n",
    "    for r in tqdm(reporters):\n",
    "        for partner in np.unique(trade[trade['reporterDesc'] == r][\"partnerDesc\"].to_numpy()):\n",
    "            for flow in [\"Import\",\"Export\"]:\n",
    "                tmp = trade[(trade['reporterDesc']==r)&(trade['flowDesc']==flow)&(trade['partnerDesc']==partner)]\n",
    "                flag,zero_len, zero_cnts = need2be_imputed(tmp)\n",
    "                zero_cnts_years[flow].append(zero_cnts)\n",
    "                if flag:\n",
    "                    X = tmp.sort_values(by='refYear')\n",
    "                    #TODO - set index\n",
    "                    transformer = imputers[imp_type]\n",
    "                    if transformer is not None:\n",
    "                        X_ = transformer.fit_transform(X['qty'].to_numpy().flatten()[:len(X) - zero_len])\n",
    "                        #print(X['qty'].to_numpy().flatten(), \"-->\", X_.flatten())\n",
    "                        for i,v in enumerate(X.iterrows()):\n",
    "                            idx, row = v\n",
    "                            if i < len(X_) and row['refYear'] < years[-5]: # do not impute last 5 years\n",
    "                                if X_[i] > 0.:\n",
    "                                    trade.loc[idx,\"qty\"] = X_[i]\n",
    "                                    #print(idx,row,\"->\",X_[i])\n",
    "                            \n",
    "    \n",
    "    trade = trade.fillna(0.) # fill the remaining Nans with 0\n",
    "    \n",
    "    print(\"Final trade size:\",len(trade))\n",
    "    \n",
    "    trade.to_csv(\"trade\" + imp_type + \".csv\",sep=\";\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1775e5e7-aa9e-432f-95ba-0043a7b62d76",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd>DataFrame(zero_cnts_years).to_csv(\"import_export_zero_stat.csv\",sep=\";\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efb6c137-5887-44f9-8a9d-90a980186c6e",
   "metadata": {},
   "source": [
    "# Prepare FAO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c3ec8de-3548-4b44-aba0-ecd91a41e093",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "\n",
    "trade = pd.read_csv(\"tradeARIMA.csv\",sep=\";\",index_col=0)\n",
    "reporters = np.unique(trade['reporterDesc'])\n",
    "\n",
    "area_map = {\"Belgium-Luxembourg\":\"Belgium-Luxembourg (...1998)\",\n",
    "\"Bosnia and Herzegovina\":\"Bosnia Herzegovina\",\n",
    "\"China, Taiwan Province of\":\"Taiwan\",\n",
    "\"China, mainland\":\"China\",\n",
    "\"Democratic People's Republic of Korea\":\"Democratic People's Republic of Korea\",\n",
    "\"Democratic Republic of the Congo\":\"Dem. Rep. of the Congo\",\n",
    "\"Iran (Islamic Republic of)\":\"Iran\",\n",
    "\"Netherlands (Kingdom of the)\":\"Netherlands\",\n",
    "\"Palestine\":\"State of Palestine\",\n",
    "\"Republic of Korea\":\"Rep. of Korea\",\n",
    "\"Republic of Moldova\":\"Rep. of Moldova\",\n",
    "\"Serbia and Montenegro\":\"Serbia and Montenegro (...2005)\",\n",
    "\"Somalia\":\"Somalia\",\n",
    "\"South Sudan\":\"Sudan\",\n",
    "\"Sudan (former)\":\"Sudan (...2011)\",\n",
    "\"Syrian Arab Republic\":\"Syria\",\n",
    "\"United Kingdom of Great Britain and Northern Ireland\":\"United Kingdom\",\n",
    "\"United Republic of Tanzania\":\"United Rep. of Tanzania\",\n",
    "\"United States of America\":\"USA\",\n",
    "\"Venezuela (Bolivarian Republic of)\":\"Venezuela\"}\n",
    "\n",
    "area_map_inv = {v:k for k,v in area_map.items()}\n",
    "\n",
    "from sktime.transformations.series.impute import Imputer\n",
    "from sktime.forecasting.arima import AutoARIMA\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import warnings\n",
    "import copy\n",
    "\n",
    "fao = pd.read_csv(\"prod_data/FAOSTAT_data_en_1-28-2025.csv\")[[\"Area\", \"Year\", \"Value\"]]\n",
    "\n",
    "years = np.unique(trade[\"refYear\"].to_numpy())\n",
    "areas = np.unique(trade['reporterDesc'].to_numpy())\n",
    "\n",
    "zero_cnts_years = {\"Production\":[]}\n",
    "\n",
    "fao_copy = copy.deepcopy(fao)\n",
    "\n",
    "imputers = {\"ARIMA\":Imputer(method=\"forecaster\", forecaster=AutoARIMA(suppress_warnings=True)), \"FF\":Imputer(method=\"ffill\"),\"INTERPOLATION\":Imputer(method=\"linear\"),\"NO_IMP\":None}\n",
    "\n",
    "for imp_type in imputers:\n",
    "    fao = copy.deepcopy(fao_copy)\n",
    "\n",
    "    for r in areas:\n",
    "        if r in area_map_inv:\n",
    "            print(area_map_inv[r],\"-->\",r)\n",
    "            for idx,row in fao[fao[\"Area\"] == area_map_inv[r]].iterrows():\n",
    "                row[\"Area\"] = r\n",
    "                fao.loc[idx] = row\n",
    "        \n",
    "    \n",
    "        for year in years:\n",
    "            if len(fao[(fao['Area']==r)&(fao['Year']==year)]) == 0:\n",
    "                 row =  pd.DataFrame({\"Area\":r,\t\"Year\":year,\"Value\":math.nan},index=[0])\n",
    "                 fao = pd.concat([fao.loc[:],row],axis=0).reset_index(drop=True)\n",
    "                \n",
    "    \n",
    "        #if empty:        \n",
    "        #    print(\"Zeros: \", r)\n",
    "        tmp = fao[(fao['Area']==r)]\n",
    "        flag,zero_len,zero_cnts = need2be_imputed(tmp,\"Value\")\n",
    "        zero_cnts_years[\"Production\"].append(zero_cnts)\n",
    "        \n",
    "        if flag:\n",
    "            X = tmp.sort_values(by='Year')\n",
    "            transformer = imputers[imp_type]\n",
    "            if transformer is not None:\n",
    "                X_ = transformer.fit_transform(X['Value'].to_numpy().flatten()[:len(X) - zero_len])\n",
    "        \n",
    "                #print(X['Value'].to_numpy().flatten(), \"-->\", X_.flatten())\n",
    "                for i,v in enumerate(X.iterrows()):\n",
    "                    idx, row = v\n",
    "                    if i < len(X_) and row['Year'] < years[-5]: # do not impute last 5 years\n",
    "                        if X_[i] > 0.:\n",
    "                            fao.loc[idx,\"Value\"] = X_[i]  \n",
    "                            print(idx,row,'->',X_[i])\n",
    "    \n",
    "    print(\"Drop duplicates\")\n",
    "    for r in tqdm(areas):\n",
    "        for year in years:\n",
    "            year_trade = fao[(fao['Area']==r)&(fao['Year']==year)]\n",
    "            if len(year_trade) > 1:\n",
    "                remain = year_trade['Value'].idxmax()\n",
    "                print(r,len(year_trade))\n",
    "                for id_,_ in year_trade.iterrows():\n",
    "                    if id_ != remain:\n",
    "                        fao = fao.drop(id_)\n",
    "                        \n",
    "    fao = fao.fillna(0.)\n",
    "    fao.to_csv(\"fao\" + imp_type + \".csv\",sep=\";\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f03403e-3225-4e75-9167-95ccfc8aaef9",
   "metadata": {},
   "source": [
    "## Calc norm constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bedce36c-6d01-4806-95c1-151f84fbfcee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import normalize\n",
    "from sktime.transformations.series.impute import Imputer\n",
    "from sktime.forecasting.arima import AutoARIMA\n",
    "\n",
    "import warnings\n",
    "import pandas as pd\n",
    "\n",
    "for imp_type in [\"ARIMA\", \"FF\",\"INTERPOLATION\",\"NO_IMP\"]:\n",
    "    trade = pd.read_csv(\"trade\" + imp_type + \".csv\",sep=\";\")\n",
    "    fao = pd.read_csv(\"fao\" + imp_type + \".csv\",sep=\";\")\n",
    "    reporters = np.unique(trade['reporterDesc'])\n",
    "\n",
    "    datas = []\n",
    "    part_offset = []\n",
    "    for r in tqdm(reporters):\n",
    "        for partner in np.unique(trade[trade['reporterDesc'] == r][\"partnerDesc\"].to_numpy()):\n",
    "            if r.find(\", nes\")==-1 and r.find(\"North America\")==-1 and r.find(\"World\") == -1 and  r.find(\"Union\") == -1 and r.find(\"19\") == -1 and r.find(\"20\") == -1:\n",
    "                if partner.find(\", nes\")==-1 and partner.find(\"North America\")==-1 and partner.find(\"World\") == -1 and  partner.find(\"Union\") == -1 and partner.find(\"19\") == -1 and partner.find(\"20\") == -1:             \n",
    "                    #print(r,partner)\n",
    "                    data = np.nan_to_num(trade[(trade['reporterDesc'] == r)&(trade['partnerDesc'] == partner)&(trade['flowDesc'] == \"Export\")].sort_values(by='refYear')['qty'].to_numpy().reshape(1,-1,1))\n",
    "                    data2 = np.nan_to_num(trade[(trade['reporterDesc'] == r)&(trade['partnerDesc'] == partner)&(trade['flowDesc'] == \"Import\")].sort_values(by='refYear')['qty'].to_numpy().reshape(1,-1,1))\n",
    "                    data3 = np.nan_to_num(fao[fao[\"Area\"] == r].sort_values(by='Year')[[\"Value\"]].to_numpy().reshape(1,-1,1))\n",
    "                    data4 = np.nan_to_num(fao[fao[\"Area\"] == partner].sort_values(by='Year')[[\"Value\"]].to_numpy().reshape(1,-1,1))\n",
    "                    if data4.shape[1] > 0 and data3.shape[1] > 0:\n",
    "                        #print(data.shape,data2.shape,data3.shape,data4.shape)\n",
    "                        data = np.concatenate([data,data2,data3,data4],axis=2)\n",
    "                        datas.append(data)\n",
    "                        part_offset.append((r,partner,len(datas) - 0))\n",
    "    \n",
    "    base_dataset = np.vstack(datas)\n",
    "    \n",
    "    warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "    warnings.simplefilter(action='ignore', category=UserWarning)\n",
    "    np.seterr(divide = 'ignore')\n",
    "    \n",
    "    base_datasetX = base_dataset[:,:30,:]\n",
    "    base_datasety = base_dataset[:,1:31,0]\n",
    "    \n",
    "    #remove lines with many zeros\n",
    "    remain_idxs = (np.abs(base_datasety) < 500000).sum(axis=1) < 2\n",
    "    \n",
    "    base_datasetX = base_datasetX[remain_idxs].reshape(-1,5,4)\n",
    "    base_datasety = base_datasety[remain_idxs]\n",
    "    \n",
    "    #normalize\n",
    "    nz_idxs_y = (base_datasety.reshape(-1,5).sum(axis=1) > 0).flatten()\n",
    "    nz_idx_x = (base_datasetX.sum(axis=1) > 0).prod(axis=1).astype(bool)\n",
    "    res_idx = nz_idxs_y * nz_idx_x\n",
    "    \n",
    "    features_min = base_datasetX[res_idx].reshape(-1,base_datasetX.shape[2]).min(axis=0)\n",
    "    features_max = base_datasetX[res_idx].reshape(-1,base_datasetX.shape[2]).max(axis=0)\n",
    "    \n",
    "    np.save(\"norm_constants\" + imp_type + \".npy\",np.asarray([features_min, features_max]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7d48487-34e5-4339-961a-1fa4c2e7c510",
   "metadata": {},
   "source": [
    "# Build graph dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0d80fd0-b483-424b-bb03-c2c917a0b1ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Build a trade map for clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32bb5711-823b-4a96-b07d-ede66963e908",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.cluster import AffinityPropagation\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from scipy.sparse import coo_matrix\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import copy\n",
    "from sktime.transformations.series.impute import Imputer\n",
    "from sktime.forecasting.arima import AutoARIMA\n",
    "import pickle\n",
    "\n",
    "def all_not_empty(cluster_year):\n",
    "    for j,k in cluster_year[1]:\n",
    "        a = cluster_year[1][j,k][0]\n",
    "        b = cluster_year[1][j,k][1]\n",
    "        if len(a[a == 0]) > 0:\n",
    "            return False\n",
    "    return True        \n",
    "\n",
    "top10 = np.asarray([\"Russian Federation\", \"Australia\", \"USA\", \"Canada\", \"Ukraine\", \"France\", \"Argentina\", \"Germany\", \"Romania\", \"India\"])\n",
    "imputers = {\"ARIMA\":Imputer(method=\"forecaster\", forecaster=AutoARIMA(suppress_warnings=True)), \"FF\":Imputer(method=\"ffill\"),\"INTERPOLATION\":Imputer(method=\"linear\"),\"NO_IMP\":None}\n",
    "\n",
    "for imp_type in [\"ARIMA\", \"FF\",\"INTERPOLATION\",\"NO_IMP\"]:\n",
    "    trade = pd.read_csv(\"trade\" + imp_type + \".csv\",sep=\";\")\n",
    "    trade_aggregate = trade[trade[\"refYear\"] > 2010][[\"reporterDesc\",\"partnerDesc\",\"qty\"]].groupby([\"reporterDesc\",\"partnerDesc\"]).mean()[\"qty\"]\n",
    "    \n",
    "    trade_map = []\n",
    "    for i,v in trade_aggregate.items():\n",
    "        if i[0].find(\", nes\")==-1 and i[0].find(\"North America\")==-1 and i[0].find(\"World\") == -1 and  i[0].find(\"Union\") == -1 and i[0].find(\"19\") == -1 and i[0].find(\"20\") == -1:\n",
    "            if i[1].find(\", nes\")==-1 and i[1].find(\"North America\")==-1 and i[1].find(\"World\") == -1 and  i[1].find(\"Union\") == -1 and i[1].find(\"19\") == -1 and i[1].find(\"20\") == -1:\n",
    "                trade_map.append([i[0],i[1],v])\n",
    "    \n",
    "    trade_map = np.asarray(trade_map)\n",
    "    \n",
    "    le = OrdinalEncoder(handle_unknown='use_encoded_value',\n",
    "                                     unknown_value=-1)\n",
    "    \n",
    "    le.fit(trade_map[:,:2].reshape(-1,1))\n",
    "    row = le.transform(trade_map[:,0].reshape(-1,1)).astype(int).flatten()\n",
    "    col = le.transform(trade_map[:,1].reshape(-1,1)).astype(int).flatten()\n",
    "    data = trade_map[:,2].astype(float)\n",
    "    num_labels = le.categories_[0].shape[0]\n",
    "    \n",
    "    trade_map_sparse = coo_matrix((data, (row, col)),shape=(num_labels, num_labels)).tolil()\n",
    "    \n",
    "    for i in range(trade_map_sparse.shape[0]):\n",
    "        for j in range(i,trade_map_sparse.shape[1]):\n",
    "            if i == j:\n",
    "                trade_map_sparse[i,j] = 0.\n",
    "            trade_map_sparse[i,j] = (trade_map_sparse[i,j] + trade_map_sparse[j,i]) / 2\n",
    "            if trade_map_sparse[i,j] < 500000.:\n",
    "                trade_map_sparse[i,j] = 0.\n",
    "            trade_map_sparse[j,i] = trade_map_sparse[i,j]\n",
    "            \n",
    "    trade_map_sparse = trade_map_sparse.tocsr()\n",
    "    \n",
    "    trade_export = trade[trade[\"flowDesc\"] == \"Export\"][[\"refYear\",\"reporterDesc\",\"partnerDesc\",\"qty\"]]\n",
    "    trade_export = trade_export.to_numpy()\n",
    "    trade_export = np.hstack([trade_export[:,0].reshape(-1,1),le.transform(trade_export[:,1].reshape(-1,1)).astype(int), le.transform(trade_export[:,2].reshape(-1,1)).astype(int),trade_export[:,3].reshape(-1,1)])    \n",
    "    trade_import = trade[trade[\"flowDesc\"] == \"Import\"][[\"refYear\",\"reporterDesc\",\"partnerDesc\",\"qty\"]]\n",
    "    trade_import = trade_import.to_numpy()\n",
    "    trade_import = np.hstack([trade_import[:,0].reshape(-1,1),le.transform(trade_import[:,1].reshape(-1,1)).astype(int), le.transform(trade_import[:,2].reshape(-1,1)).astype(int),trade_import[:,3].reshape(-1,1)])\n",
    "    preference = - np.ones((num_labels,))\n",
    "    idx = le.transform(top10.reshape(-1,1)).astype(int).flatten()\n",
    "    preference[idx] = 0.\n",
    "\n",
    "    \n",
    "    model = AffinityPropagation(affinity=\"precomputed\",max_iter=10000,preference = None,damping=0.5)\n",
    "    aff_map = trade_map_sparse.toarray()\n",
    "    aff_map = np.nan_to_num(aff_map)\n",
    "    \n",
    "    clustering = model.fit_predict(aff_map)\n",
    "    \n",
    "    ids, counts = np.unique(clustering, return_counts = True)\n",
    "    \n",
    "    avg_weight = trade_map_sparse.mean()\n",
    "    \n",
    "    for i,c in enumerate(ids):\n",
    "        print(c)\n",
    "        G = nx.Graph()\n",
    "    \n",
    "        for j in range(clustering.shape[0]):\n",
    "            if clustering[j] == c:\n",
    "                #print(c,le.inverse_transform([j])[0])\n",
    "                for k in range(clustering.shape[0]):\n",
    "                    if clustering[k] == c:\n",
    "                        if  trade_map_sparse[j,k] > 0.:\n",
    "                            #print(c,le.inverse_transform([j])[0], le.inverse_transform([k])[0], trade_map_sparse[j,k])\n",
    "                            G.add_edge(le.inverse_transform([[j]])[0][0], le.inverse_transform([[k]])[0][0], weight= trade_map_sparse[j,k])\n",
    "                            \n",
    "        elarge = [(u, v) for (u, v, d) in G.edges(data=True) if d[\"weight\"] > avg_weight*2.5]\n",
    "        ealarge = [(u, v) for (u, v, d) in G.edges(data=True) if ((d[\"weight\"] > avg_weight*1.2) and (d[\"weight\"] <= avg_weight*2.5))]\n",
    "        emed = [(u, v) for (u, v, d) in G.edges(data=True) if ((d[\"weight\"] > avg_weight*0.8) and (d[\"weight\"] <= avg_weight*1.2))]\n",
    "        esmall = [(u, v) for (u, v, d) in G.edges(data=True) if d[\"weight\"] <= avg_weight*0.8]\n",
    "    \n",
    "        if len(elarge + esmall+emed+ealarge) > 2 and len(elarge) > 0:\n",
    "            layout = nx.spring_layout(G, seed=7,weight=None)\n",
    "            #nx.draw(G, layout)\n",
    "            nx.draw_networkx_nodes(G, layout, node_size=200,alpha=0.4)\n",
    "        \n",
    "            # edges\n",
    "            nx.draw_networkx_edges(G, layout, edgelist=elarge, width=1.5,alpha=0.5,edge_color=\"r\")\n",
    "            nx.draw_networkx_edges(\n",
    "                G, layout, edgelist=ealarge, width=1.0, alpha=0.7, edge_color=\"y\"\n",
    "            )           \n",
    "            nx.draw_networkx_edges(\n",
    "                G, layout, edgelist=emed, width=0.9, alpha=0.7, edge_color=\"g\"\n",
    "            )        \n",
    "            nx.draw_networkx_edges(\n",
    "                G, layout, edgelist=esmall, width=0.5, alpha=0.7, edge_color=\"b\"\n",
    "            )\n",
    "            \n",
    "            # node labels\n",
    "            nx.draw_networkx_labels(G, layout, font_size=8, font_family=\"sans-serif\")\n",
    "            # edge weight labels\n",
    "            #edge_labels = nx.get_edge_attributes(G, \"weight\")\n",
    "            #nx.draw_networkx_edge_labels(G, layout, edge_labels,font_size=6)\n",
    "            plt.show()   \n",
    "            years = np.unique(trade_export[:,0])\n",
    "\n",
    "    trade_clusters = []\n",
    "    #; un['num_states'] : nb of states, an integer (N)\n",
    "    #; un['production'] : tensor of size N, each element is a production value, a float number > 0\n",
    "    #; un['export'] : tensor of size N x N, each element is a value of export, a float number > 0\n",
    "    #; un['pred_export']\n",
    "    \n",
    "    cluster_year = {}\n",
    "    \n",
    "    for i,c in enumerate(ids):\n",
    "        print(\"Cluster \",c)\n",
    "        for year in range(1993,2023,5):\n",
    "            countries = {}\n",
    "            cdata = {}\n",
    "            for j in range(clustering.shape[0]):\n",
    "                if clustering[j] == c:\n",
    "                    for k in range(clustering.shape[0]):\n",
    "                        if clustering[k] == c:\n",
    "                            ty = trade_export[(trade_export[:,0] >= year)&(trade_export[:,0] < year + 7)&(trade_export[:,1] == j)&(trade_export[:,2] == k)]\n",
    "                            tyi = trade_import[(trade_import[:,0] >= year)&(trade_import[:,0] < year + 7)&(trade_import[:,1] == j)&(trade_import[:,2] == k)]\n",
    "                            if len(ty) > 0:\n",
    "                                countries[j] = True\n",
    "                                countries[k] = True\n",
    "                                datas = np.zeros((5,2))\n",
    "                                targets = np.zeros((5,))\n",
    "                                too_small_flag = 0\n",
    "                                for y in range(year,year + 5):\n",
    "                                    val = ty[ty[:,0] == y]\n",
    "                                    imp_val = tyi[tyi[:,0] == y]\n",
    "                                    if len(val) > 0:\n",
    "                                        if val[0,3] < 500000:\n",
    "                                            too_small_flag += 1    \n",
    "                                        datas[y - year,0] = val[0,3]\n",
    "    \n",
    "                                        if len(imp_val) > 0:\n",
    "                                            datas[y - year,1] = imp_val[0,3]\n",
    "                                        val = ty[ty[:,0] == y + 1]\n",
    "                                        if len(val) > 0:\n",
    "                                            targets[y - year] = val[0,3]# - datas[y - year,0]\n",
    "                                        else:\n",
    "                                            if y > year:\n",
    "                                                print(c,y,j,k,\"got prev target\")\n",
    "                                                targets[y - year] = targets[y - year - 1]\n",
    "                                    else:\n",
    "                                        too_small_flag += 1       \n",
    "                                if (datas[:,0] > 0).sum() > 4 and too_small_flag < 2:\n",
    "                                    cdata[(j,k)] = [datas, targets]  \n",
    "            if len(cdata) > 0:                     \n",
    "                cluster_year[(c,year)] = [countries,cdata]\n",
    "    features_min, features_max = np.load(\"norm_constants\" + imp_type +\".npy\",allow_pickle=True)\n",
    "    cluster_year_ = copy.deepcopy(cluster_year) \n",
    "    for is_normed in [True, False]:\n",
    "        cluster_year = copy.deepcopy(cluster_year_)\n",
    "        if is_normed:\n",
    "            #norm\n",
    "            for c,y in cluster_year:\n",
    "                for j,k in cluster_year[c,y][1]:\n",
    "                    cluster_year[c,y][1][j,k][1] = cluster_year[c,y][1][j,k][1] / (features_max[0])\n",
    "                    cluster_year[c,y][1][j,k][1][cluster_year[c,y][1][j,k][1] > 1.] = 1.\n",
    "                    cluster_year[c,y][1][j,k][1][cluster_year[c,y][1][j,k][1] < -1.] = -1.        \n",
    "                    cluster_year[c,y][1][j,k][0] = (cluster_year[c,y][1][j,k][0] - features_min[:2]) / (features_max[:2] - features_min[:2])\n",
    "        else:\n",
    "            #rescale it anyway to escape overflaws in exp()\n",
    "            for c,y in cluster_year:\n",
    "                for j,k in cluster_year[c,y][1]:\n",
    "                    cluster_year[c,y][1][j,k][1] = cluster_year[c,y][1][j,k][1] / 100000\n",
    "                    cluster_year[c,y][1][j,k][0] = cluster_year[c,y][1][j,k][0] / 100000\n",
    "        \n",
    "        fao = pd.read_csv(\"fao\" + imp_type + \".csv\",sep=\";\")[[\"Area\", \"Year\", \"Value\"]].to_numpy()\n",
    "        fao = np.hstack([le.transform(fao[:,0].reshape(-1,1)).astype(int),fao[:,1:3]])\n",
    "        #add fao\n",
    "        todel = []\n",
    "        for c,y in cluster_year:\n",
    "            for j,k in cluster_year[c,y][1]:\n",
    "                years = []\n",
    "                years2 = []\n",
    "                for yr in range(y, y + 5):\n",
    "                    fcy = fao[(fao[:,0] == j)&(fao[:,1] == yr)]\n",
    "                    if len(fcy) > 0 and not np.isnan(fcy[0,2]):\n",
    "                        years.append(fcy[0,2])\n",
    "                    else:\n",
    "                        #print(\"zero prod:\",c,y,j,k,yr)\n",
    "                        years.append(0.)\n",
    "        \n",
    "                    fc = fao[(fao[:,0] == k).flatten()]\n",
    "                    if len(fc) > 0:\n",
    "                        fcy = fc[fc[:,1] == yr]\n",
    "                        if len(fcy) > 0 and not np.isnan(fcy[0,2]):\n",
    "                            years2.append(fcy[0,2])\n",
    "                        else:\n",
    "                            years2.append(0.)        \n",
    "                years = np.asarray(years)  \n",
    "                years2 = np.asarray(years2)  \n",
    "                if years.sum() == 0:\n",
    "                    todel.append((c,y,j,k))    \n",
    "                else: \n",
    "                    #years[years == 0.] = years.mean()\n",
    "                    \n",
    "                    transformer = imputers[imp_type]\n",
    "                    if transformer is not None and y < 2019: # do not impute test\n",
    "                        years = transformer.fit_transform(years.reshape(1,-1)).flatten()            \n",
    "                    else:\n",
    "                        years = years.flatten()\n",
    "                    tmp =  copy.deepcopy(cluster_year[c,y][1][j,k][0])\n",
    "                    if is_normed:\n",
    "                        cluster_year[c,y][1][j,k][0] = np.hstack([ tmp, np.asarray(years - features_min[2]).reshape(-1,1) / (features_max[2] - features_min[2]),np.asarray(years2 - features_min[2]).reshape(-1,1) / (features_max[2] - features_min[2])])\n",
    "                    else:\n",
    "                        cluster_year[c,y][1][j,k][0] = np.hstack([ tmp, np.asarray(years).reshape(-1,1),np.asarray(years2).reshape(-1,1)])\n",
    "             \n",
    "        #remove small or empty clusters\n",
    "        for c,y,j,k in todel:\n",
    "            if (c,y) in cluster_year:\n",
    "                del cluster_year[c,y][1][j,k]    \n",
    "                if len(cluster_year[c,y][1]) < 3:\n",
    "                    del cluster_year[c,y]\n",
    "    \n",
    "        cluster_year_train = {}\n",
    "        cluster_year_test = {}\n",
    "        for c,y in cluster_year:\n",
    "            if y > 2018:\n",
    "                if all_not_empty(cluster_year[c,y]): # leave only the clusters that does not need imputation\n",
    "                    cluster_year_test[(c,y)] = cluster_year[c,y]\n",
    "            else:    \n",
    "                cluster_year_train[(c,y)] = cluster_year[c,y]\n",
    "                \n",
    "        with open('cluster_year' + imp_type + str(is_normed) +  '_train.pkl', 'wb') as f:\n",
    "            pickle.dump(cluster_year_train, f)\n",
    "    \n",
    "        with open('cluster_year' + imp_type + str(is_normed) +  '_test.pkl', 'wb') as f:\n",
    "            pickle.dump(cluster_year_test, f)\n",
    "        \n",
    "        with open('name_encoder' + imp_type + '.pkl', 'wb') as f:\n",
    "            pickle.dump(le, f)                             \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9173b3e-e9ac-4558-a81f-4ad1f77f228d",
   "metadata": {},
   "source": [
    "## Draw export distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65369dea-e427-4545-b15b-8e8a4c1caa0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "targs = []\n",
    "for c, y in cluster_year:\n",
    "    for j,k in cluster_year[c,y][1]:\n",
    "        targs += [cluster_year[c,y][1][j,k][1]]\n",
    "\n",
    "atargs = np.hstack(targs)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Create the histogram\n",
    "plt.hist(atargs.flatten(), bins=50)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
