{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66bdca18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2025-10-30 06:32:17,807]\u001b[0m A new study created in memory with name: no-name-cb9c0847-d76f-47a7-a4a2-c81be942188b\u001b[0m\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "\u001b[32m[I 2025-10-30 06:32:30,426]\u001b[0m Trial 0 finished with value: 0.025834024596648936 and parameters: {'lr': 0.0063998081895040235, 'hs': 9, 'dropout': 0.1261859878547436}. Best is trial 0 with value: 0.025834024596648936.\u001b[0m\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "\u001b[32m[I 2025-10-30 06:32:43,050]\u001b[0m Trial 1 finished with value: 0.02387006797095512 and parameters: {'lr': 0.001956527098068071, 'hs': 21, 'dropout': 0.18857670897901263}. Best is trial 1 with value: 0.02387006797095512.\u001b[0m\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "\u001b[32m[I 2025-10-30 06:32:51,129]\u001b[0m Trial 2 finished with value: 0.022108211930333452 and parameters: {'lr': 0.00399881558587156, 'hs': 27, 'dropout': 0.16155827748484375}. Best is trial 2 with value: 0.022108211930333452.\u001b[0m\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "\u001b[32m[I 2025-10-30 06:33:00,174]\u001b[0m Trial 3 finished with value: 0.02117078225906513 and parameters: {'lr': 0.008619496468160579, 'hs': 28, 'dropout': 0.0880153985339012}. Best is trial 3 with value: 0.02117078225906513.\u001b[0m\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "\u001b[32m[I 2025-10-30 06:33:11,759]\u001b[0m Trial 4 finished with value: 0.02139645952080906 and parameters: {'lr': 0.004160668472686476, 'hs': 29, 'dropout': 0.18773825708680603}. Best is trial 3 with value: 0.02117078225906513.\u001b[0m\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "\u001b[32m[I 2025-10-30 06:33:20,339]\u001b[0m Trial 5 finished with value: 0.022062423136398102 and parameters: {'lr': 0.00537611484636999, 'hs': 27, 'dropout': 0.18607492792779906}. Best is trial 3 with value: 0.02117078225906513.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "\u001b[32m[I 2025-10-30 06:33:47,325]\u001b[0m Trial 6 finished with value: 0.02972473909904493 and parameters: {'lr': 0.004345293686860648, 'hs': 4, 'dropout': 0.14778252160749175}. Best is trial 3 with value: 0.02117078225906513.\u001b[0m\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "\u001b[32m[I 2025-10-30 06:33:56,013]\u001b[0m Trial 7 finished with value: 0.0223919751269782 and parameters: {'lr': 0.004725879611409879, 'hs': 23, 'dropout': 0.11059613383237779}. Best is trial 3 with value: 0.02117078225906513.\u001b[0m\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "\u001b[32m[I 2025-10-30 06:34:07,093]\u001b[0m Trial 8 finished with value: 0.024723338755398408 and parameters: {'lr': 0.0033129268368465438, 'hs': 13, 'dropout': 0.13943324179663755}. Best is trial 3 with value: 0.02117078225906513.\u001b[0m\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "\u001b[32m[I 2025-10-30 06:34:18,079]\u001b[0m Trial 9 finished with value: 0.021741826913445467 and parameters: {'lr': 0.003661676901020181, 'hs': 23, 'dropout': 0.18673613077856666}. Best is trial 3 with value: 0.02117078225906513.\u001b[0m\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "\u001b[32m[I 2025-10-30 06:34:24,966]\u001b[0m Trial 10 finished with value: 0.021610780363338872 and parameters: {'lr': 0.009983052882627377, 'hs': 16, 'dropout': 0.06005517074834035}. Best is trial 3 with value: 0.02117078225906513.\u001b[0m\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "\u001b[32m[I 2025-10-30 06:34:31,191]\u001b[0m Trial 11 finished with value: 0.021055908836339815 and parameters: {'lr': 0.008138472749525437, 'hs': 32, 'dropout': 0.08822261962984981}. Best is trial 11 with value: 0.021055908836339815.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "\u001b[32m[I 2025-10-30 06:34:38,182]\u001b[0m Trial 12 finished with value: 0.02170064450993282 and parameters: {'lr': 0.008569093158635218, 'hs': 31, 'dropout': 0.08278860632796659}. Best is trial 11 with value: 0.021055908836339815.\u001b[0m\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "\u001b[32m[I 2025-10-30 06:34:46,071]\u001b[0m Trial 13 finished with value: 0.02122527716548393 and parameters: {'lr': 0.0069813000871600025, 'hs': 32, 'dropout': 0.09497412480675112}. Best is trial 11 with value: 0.021055908836339815.\u001b[0m\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "\u001b[32m[I 2025-10-30 06:34:55,383]\u001b[0m Trial 14 finished with value: 0.02250882069507942 and parameters: {'lr': 0.007954240617693332, 'hs': 20, 'dropout': 0.052483330974564125}. Best is trial 11 with value: 0.021055908836339815.\u001b[0m\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "\u001b[32m[I 2025-10-30 06:35:03,194]\u001b[0m Trial 15 finished with value: 0.020778157364856085 and parameters: {'lr': 0.00934737215707447, 'hs': 26, 'dropout': 0.07587114992452812}. Best is trial 15 with value: 0.020778157364856085.\u001b[0m\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "\u001b[32m[I 2025-10-30 06:35:12,079]\u001b[0m Trial 16 finished with value: 0.02333727407511621 and parameters: {'lr': 0.009637549203372595, 'hs': 24, 'dropout': 0.07234253242553876}. Best is trial 15 with value: 0.020778157364856085.\u001b[0m\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "\u001b[32m[I 2025-10-30 06:35:15,765]\u001b[0m Trial 17 finished with value: 0.1965172947492427 and parameters: {'lr': 3.787344198973204e-05, 'hs': 32, 'dropout': 0.10739142331145751}. Best is trial 15 with value: 0.020778157364856085.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "\u001b[32m[I 2025-10-30 06:35:23,699]\u001b[0m Trial 18 finished with value: 0.0217853610916303 and parameters: {'lr': 0.006947469805762938, 'hs': 18, 'dropout': 0.07092390781770902}. Best is trial 15 with value: 0.020778157364856085.\u001b[0m\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "\u001b[32m[I 2025-10-30 06:35:32,364]\u001b[0m Trial 19 finished with value: 0.022123908426815497 and parameters: {'lr': 0.00776259565135981, 'hs': 25, 'dropout': 0.10496309308455336}. Best is trial 15 with value: 0.020778157364856085.\u001b[0m\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "\u001b[32m[I 2025-10-30 06:35:40,148]\u001b[0m Trial 20 finished with value: 0.025453789769146026 and parameters: {'lr': 0.009177844707821124, 'hs': 14, 'dropout': 0.07472474198680673}. Best is trial 15 with value: 0.020778157364856085.\u001b[0m\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "\u001b[32m[I 2025-10-30 06:35:48,473]\u001b[0m Trial 21 finished with value: 0.022334734768291092 and parameters: {'lr': 0.008704743742323927, 'hs': 28, 'dropout': 0.09002536367392659}. Best is trial 15 with value: 0.020778157364856085.\u001b[0m\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "\u001b[32m[I 2025-10-30 06:35:57,904]\u001b[0m Trial 22 finished with value: 0.020911131535334828 and parameters: {'lr': 0.008075724631635315, 'hs': 30, 'dropout': 0.09784464181351275}. Best is trial 15 with value: 0.020778157364856085.\u001b[0m\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "\u001b[32m[I 2025-10-30 06:36:06,584]\u001b[0m Trial 23 finished with value: 0.02041803031704106 and parameters: {'lr': 0.007763886974216883, 'hs': 30, 'dropout': 0.11602089339772631}. Best is trial 23 with value: 0.02041803031704106.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "\u001b[32m[I 2025-10-30 06:36:14,715]\u001b[0m Trial 24 finished with value: 0.02108429593677757 and parameters: {'lr': 0.005827215426684054, 'hs': 30, 'dropout': 0.1224662661751359}. Best is trial 23 with value: 0.02041803031704106.\u001b[0m\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "\u001b[32m[I 2025-10-30 06:36:22,114]\u001b[0m Trial 25 finished with value: 0.02129424840313049 and parameters: {'lr': 0.007173192654120161, 'hs': 26, 'dropout': 0.12456342155418174}. Best is trial 23 with value: 0.02041803031704106.\u001b[0m\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "\u001b[32m[I 2025-10-30 06:36:28,927]\u001b[0m Trial 26 finished with value: 0.021921377029010505 and parameters: {'lr': 0.009220891487766293, 'hs': 21, 'dropout': 0.09912021729384438}. Best is trial 23 with value: 0.02041803031704106.\u001b[0m\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "\u001b[32m[I 2025-10-30 06:36:38,125]\u001b[0m Trial 27 finished with value: 0.02085439943698224 and parameters: {'lr': 0.007498761617073117, 'hs': 29, 'dropout': 0.11501675531026491}. Best is trial 23 with value: 0.02041803031704106.\u001b[0m\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "\u001b[32m[I 2025-10-30 06:36:46,381]\u001b[0m Trial 28 finished with value: 0.02040131894504135 and parameters: {'lr': 0.0063092713949378446, 'hs': 25, 'dropout': 0.1537643295297499}. Best is trial 28 with value: 0.02040131894504135.\u001b[0m\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "\u001b[32m[I 2025-10-30 06:36:56,478]\u001b[0m Trial 29 finished with value: 0.02435148284052963 and parameters: {'lr': 0.006240667319275852, 'hs': 10, 'dropout': 0.16850138946928828}. Best is trial 28 with value: 0.02040131894504135.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "\u001b[32m[I 2025-10-30 06:37:10,031]\u001b[0m Trial 30 finished with value: 0.031244137354833872 and parameters: {'lr': 0.006412330111468324, 'hs': 3, 'dropout': 0.13980266010308334}. Best is trial 28 with value: 0.02040131894504135.\u001b[0m\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "\u001b[32m[I 2025-10-30 06:37:17,727]\u001b[0m Trial 31 finished with value: 0.025058127105236156 and parameters: {'lr': 0.007576378190550826, 'hs': 25, 'dropout': 0.11582212571091441}. Best is trial 28 with value: 0.02040131894504135.\u001b[0m\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "\u001b[32m[I 2025-10-30 06:37:27,636]\u001b[0m Trial 32 finished with value: 0.02137073235065881 and parameters: {'lr': 0.006414227728565339, 'hs': 22, 'dropout': 0.13529361601460185}. Best is trial 28 with value: 0.02040131894504135.\u001b[0m\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "\u001b[32m[I 2025-10-30 06:37:37,355]\u001b[0m Trial 33 finished with value: 0.024678514726122532 and parameters: {'lr': 0.005515920041717172, 'hs': 19, 'dropout': 0.15654701849828573}. Best is trial 28 with value: 0.02040131894504135.\u001b[0m\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "\u001b[32m[I 2025-10-30 06:37:45,450]\u001b[0m Trial 34 finished with value: 0.022779388374269904 and parameters: {'lr': 0.0073808165030538045, 'hs': 27, 'dropout': 0.15715089163162246}. Best is trial 28 with value: 0.02040131894504135.\u001b[0m\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "\u001b[32m[I 2025-10-30 06:37:52,058]\u001b[0m Trial 35 finished with value: 0.023626128303128324 and parameters: {'lr': 0.00915264701721135, 'hs': 29, 'dropout': 0.16937584382724788}. Best is trial 28 with value: 0.02040131894504135.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "\u001b[32m[I 2025-10-30 06:37:59,208]\u001b[0m Trial 36 finished with value: 0.022599807417222405 and parameters: {'lr': 0.00861767954990759, 'hs': 26, 'dropout': 0.12927392292629059}. Best is trial 28 with value: 0.02040131894504135.\u001b[0m\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "\u001b[32m[I 2025-10-30 06:38:05,861]\u001b[0m Trial 37 finished with value: 0.0220881554671018 and parameters: {'lr': 0.006682688410414999, 'hs': 29, 'dropout': 0.1479343423692656}. Best is trial 28 with value: 0.02040131894504135.\u001b[0m\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "\u001b[32m[I 2025-10-30 06:38:15,218]\u001b[0m Trial 38 finished with value: 0.02103663707482108 and parameters: {'lr': 0.005878654854353508, 'hs': 28, 'dropout': 0.17559944003581493}. Best is trial 28 with value: 0.02040131894504135.\u001b[0m\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "\u001b[32m[I 2025-10-30 06:38:25,816]\u001b[0m Trial 39 finished with value: 0.021005342786798692 and parameters: {'lr': 0.0028378618754607288, 'hs': 24, 'dropout': 0.19718068401463534}. Best is trial 28 with value: 0.02040131894504135.\u001b[0m\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "\u001b[32m[I 2025-10-30 06:39:30,040]\u001b[0m Trial 40 finished with value: 0.029067124483631582 and parameters: {'lr': 0.004891471713798895, 'hs': 7, 'dropout': 0.11788775708490506}. Best is trial 28 with value: 0.02040131894504135.\u001b[0m\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "\u001b[32m[I 2025-10-30 06:39:36,978]\u001b[0m Trial 41 finished with value: 0.020010322959780336 and parameters: {'lr': 0.008400171743999325, 'hs': 30, 'dropout': 0.09923813904876977}. Best is trial 41 with value: 0.020010322959780336.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "\u001b[32m[I 2025-10-30 06:39:44,674]\u001b[0m Trial 42 finished with value: 0.023913486990013815 and parameters: {'lr': 0.008461250534780002, 'hs': 30, 'dropout': 0.1143068008719891}. Best is trial 41 with value: 0.020010322959780336.\u001b[0m\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "\u001b[32m[I 2025-10-30 06:39:51,405]\u001b[0m Trial 43 finished with value: 0.020586833451348097 and parameters: {'lr': 0.009676471327532453, 'hs': 27, 'dropout': 0.10387304432403759}. Best is trial 41 with value: 0.020010322959780336.\u001b[0m\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "\u001b[32m[I 2025-10-30 06:40:00,042]\u001b[0m Trial 44 finished with value: 0.021910886060437673 and parameters: {'lr': 0.009322605017076989, 'hs': 27, 'dropout': 0.08163009174516575}. Best is trial 41 with value: 0.020010322959780336.\u001b[0m\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "\u001b[32m[I 2025-10-30 06:40:06,666]\u001b[0m Trial 45 finished with value: 0.024580044873458644 and parameters: {'lr': 0.009918093594466965, 'hs': 25, 'dropout': 0.10274117369935117}. Best is trial 41 with value: 0.020010322959780336.\u001b[0m\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "\u001b[32m[I 2025-10-30 06:40:15,143]\u001b[0m Trial 46 finished with value: 0.0204521146375269 and parameters: {'lr': 0.00887772054705944, 'hs': 22, 'dropout': 0.06307477000359693}. Best is trial 41 with value: 0.020010322959780336.\u001b[0m\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "\u001b[32m[I 2025-10-30 06:40:23,065]\u001b[0m Trial 47 finished with value: 0.020684185118223355 and parameters: {'lr': 0.008832523206960733, 'hs': 23, 'dropout': 0.056208629102139356}. Best is trial 41 with value: 0.020010322959780336.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "\u001b[32m[I 2025-10-30 06:40:33,173]\u001b[0m Trial 48 finished with value: 0.021328714886820126 and parameters: {'lr': 0.008268285387427469, 'hs': 21, 'dropout': 0.06489939096012384}. Best is trial 41 with value: 0.020010322959780336.\u001b[0m\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "\u001b[32m[I 2025-10-30 06:40:39,956]\u001b[0m Trial 49 finished with value: 0.023300488343669434 and parameters: {'lr': 0.00975474474528676, 'hs': 16, 'dropout': 0.1328667629007407}. Best is trial 41 with value: 0.020010322959780336.\u001b[0m\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "('LSTM', 1.0927421406613508e+18, 883126739.1740448, -0.4580117463560405, 'ARIMA', True, 'beta')\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "('LSTM', 1.0662118152988095e+18, 885107015.5406271, -0.4337960859656267, 'ARIMA', True, 'beta')\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "('LSTM', 1.1776624472289759e+18, 920578107.7380737, -0.4895234856619981, 'ARIMA', True, 'beta')\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "('LSTM', 1.1069019087305504e+18, 864184206.5800437, -0.33581331386467617, 'ARIMA', True, 'beta')\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "('LSTM', 8.39699806192201e+17, 842120831.8057023, -1.2924682657002373, 'ARIMA', True, 'beta')\n",
      "\u001b[32m[I 2025-10-30 06:45:59,506]\u001b[0m A new study created in memory with name: no-name-25bcc161-45bb-4b11-a83f-d1a2d30a4b29\u001b[0m\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "\u001b[32m[I 2025-10-30 06:46:07,042]\u001b[0m Trial 0 finished with value: 0.00235465897897037 and parameters: {'lr': 0.007536580977427676, 'hs': 29, 'dropout': 0.12994729704541064}. Best is trial 0 with value: 0.00235465897897037.\u001b[0m\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "\u001b[32m[I 2025-10-30 06:46:21,900]\u001b[0m Trial 1 finished with value: 0.0037050262609168824 and parameters: {'lr': 0.0012413191133765675, 'hs': 30, 'dropout': 0.1448928741057417}. Best is trial 0 with value: 0.00235465897897037.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "\u001b[32m[I 2025-10-30 06:46:33,415]\u001b[0m Trial 2 finished with value: 0.0034286828645880944 and parameters: {'lr': 0.004163862591815302, 'hs': 9, 'dropout': 0.13300965914925694}. Best is trial 0 with value: 0.00235465897897037.\u001b[0m\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "\u001b[32m[I 2025-10-30 06:46:43,009]\u001b[0m Trial 3 finished with value: 0.0022945395151650985 and parameters: {'lr': 0.005158914223915607, 'hs': 22, 'dropout': 0.06963816052774775}. Best is trial 3 with value: 0.0022945395151650985.\u001b[0m\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "\u001b[32m[I 2025-10-30 06:46:51,626]\u001b[0m Trial 4 finished with value: 0.0028996731355311503 and parameters: {'lr': 0.009798753804702218, 'hs': 11, 'dropout': 0.07991116477996292}. Best is trial 3 with value: 0.0022945395151650985.\u001b[0m\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "\u001b[32m[I 2025-10-30 06:47:02,239]\u001b[0m Trial 5 finished with value: 0.003555293130166185 and parameters: {'lr': 0.005999617357360666, 'hs': 4, 'dropout': 0.06026690848718266}. Best is trial 3 with value: 0.0022945395151650985.\u001b[0m\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "\u001b[32m[I 2025-10-30 06:47:13,748]\u001b[0m Trial 6 finished with value: 0.0030895946793250257 and parameters: {'lr': 0.003648716659283844, 'hs': 12, 'dropout': 0.10475337779081771}. Best is trial 3 with value: 0.0022945395151650985.\u001b[0m\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "\u001b[32m[I 2025-10-30 06:47:23,592]\u001b[0m Trial 7 finished with value: 0.002467835586890921 and parameters: {'lr': 0.005023493935671683, 'hs': 16, 'dropout': 0.0874397190344825}. Best is trial 3 with value: 0.0022945395151650985.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "\u001b[32m[I 2025-10-30 06:47:32,770]\u001b[0m Trial 8 finished with value: 0.003446738215887767 and parameters: {'lr': 0.009135221220665766, 'hs': 9, 'dropout': 0.16587560209252866}. Best is trial 3 with value: 0.0022945395151650985.\u001b[0m\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "\u001b[32m[I 2025-10-30 06:47:48,471]\u001b[0m Trial 9 finished with value: 0.0031217801730974077 and parameters: {'lr': 0.0015500035249650858, 'hs': 26, 'dropout': 0.084965982545608}. Best is trial 3 with value: 0.0022945395151650985.\u001b[0m\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "\u001b[32m[I 2025-10-30 06:47:55,210]\u001b[0m Trial 10 finished with value: 0.002613503609926978 and parameters: {'lr': 0.007094090211905547, 'hs': 22, 'dropout': 0.1855346174842241}. Best is trial 3 with value: 0.0022945395151650985.\u001b[0m\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "\u001b[32m[I 2025-10-30 06:48:00,615]\u001b[0m Trial 11 finished with value: 0.002250085794637469 and parameters: {'lr': 0.007652171255965646, 'hs': 32, 'dropout': 0.1170396292868759}. Best is trial 11 with value: 0.002250085794637469.\u001b[0m\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "\u001b[32m[I 2025-10-30 06:48:07,911]\u001b[0m Trial 12 finished with value: 0.002191200730187747 and parameters: {'lr': 0.007658273751800081, 'hs': 23, 'dropout': 0.050516240249409955}. Best is trial 12 with value: 0.002191200730187747.\u001b[0m\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "\u001b[32m[I 2025-10-30 06:48:16,816]\u001b[0m Trial 13 finished with value: 0.002343969709945978 and parameters: {'lr': 0.008111877882893038, 'hs': 23, 'dropout': 0.051751142939108534}. Best is trial 12 with value: 0.002191200730187747.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "\u001b[32m[I 2025-10-30 06:48:25,839]\u001b[0m Trial 14 finished with value: 0.002314099623201893 and parameters: {'lr': 0.006951845258638331, 'hs': 32, 'dropout': 0.10379098327996372}. Best is trial 12 with value: 0.002191200730187747.\u001b[0m\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "\u001b[32m[I 2025-10-30 06:48:34,260]\u001b[0m Trial 15 finished with value: 0.002733991912038897 and parameters: {'lr': 0.00896746931979562, 'hs': 17, 'dropout': 0.10777068458093647}. Best is trial 12 with value: 0.002191200730187747.\u001b[0m\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "\u001b[32m[I 2025-10-30 06:48:41,570]\u001b[0m Trial 16 finished with value: 0.002525863076972354 and parameters: {'lr': 0.0060804456134143136, 'hs': 27, 'dropout': 0.1555014696685152}. Best is trial 12 with value: 0.002191200730187747.\u001b[0m\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "\u001b[32m[I 2025-10-30 06:48:52,607]\u001b[0m Trial 17 finished with value: 0.0031750793109985454 and parameters: {'lr': 0.0028803008189848397, 'hs': 24, 'dropout': 0.17365200415501264}. Best is trial 12 with value: 0.002191200730187747.\u001b[0m\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "\u001b[32m[I 2025-10-30 06:48:59,491]\u001b[0m Trial 18 finished with value: 0.0026924671899461947 and parameters: {'lr': 0.008410598271307551, 'hs': 19, 'dropout': 0.19290807850581404}. Best is trial 12 with value: 0.002191200730187747.\u001b[0m\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "\u001b[32m[I 2025-10-30 06:49:39,347]\u001b[0m Trial 19 finished with value: 0.01526645627525966 and parameters: {'lr': 9.206780485355136e-05, 'hs': 32, 'dropout': 0.11653196845569651}. Best is trial 12 with value: 0.002191200730187747.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "\u001b[32m[I 2025-10-30 06:49:46,994]\u001b[0m Trial 20 finished with value: 0.0025183720365582925 and parameters: {'lr': 0.009902928148737718, 'hs': 19, 'dropout': 0.14156567195106431}. Best is trial 12 with value: 0.002191200730187747.\u001b[0m\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "\u001b[32m[I 2025-10-30 06:49:56,085]\u001b[0m Trial 21 finished with value: 0.002183974295126214 and parameters: {'lr': 0.0058440943592461695, 'hs': 26, 'dropout': 0.06705726317415453}. Best is trial 21 with value: 0.002183974295126214.\u001b[0m\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "\u001b[32m[I 2025-10-30 06:50:05,334]\u001b[0m Trial 22 finished with value: 0.002212963162778842 and parameters: {'lr': 0.006062121286814598, 'hs': 27, 'dropout': 0.06879368166561822}. Best is trial 21 with value: 0.002183974295126214.\u001b[0m\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "\u001b[32m[I 2025-10-30 06:50:11,463]\u001b[0m Trial 23 finished with value: 0.0021919473411977472 and parameters: {'lr': 0.006301616786833091, 'hs': 26, 'dropout': 0.06655598041729667}. Best is trial 21 with value: 0.002183974295126214.\u001b[0m\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "\u001b[32m[I 2025-10-30 06:50:21,077]\u001b[0m Trial 24 finished with value: 0.002128556279837506 and parameters: {'lr': 0.006496165860130108, 'hs': 25, 'dropout': 0.050285006173618314}. Best is trial 24 with value: 0.002128556279837506.\u001b[0m\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "\u001b[32m[I 2025-10-30 06:50:30,972]\u001b[0m Trial 25 finished with value: 0.002348126120887396 and parameters: {'lr': 0.004375259687448014, 'hs': 21, 'dropout': 0.05017459429407485}. Best is trial 24 with value: 0.002128556279837506.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "\u001b[32m[I 2025-10-30 06:50:39,900]\u001b[0m Trial 26 finished with value: 0.0024658624151274178 and parameters: {'lr': 0.005328177890917322, 'hs': 25, 'dropout': 0.0909589335990904}. Best is trial 24 with value: 0.002128556279837506.\u001b[0m\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "\u001b[32m[I 2025-10-30 06:50:48,499]\u001b[0m Trial 27 finished with value: 0.0023935727176527507 and parameters: {'lr': 0.0067888679743189915, 'hs': 29, 'dropout': 0.059261361275246636}. Best is trial 24 with value: 0.002128556279837506.\u001b[0m\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "\u001b[32m[I 2025-10-30 06:50:56,789]\u001b[0m Trial 28 finished with value: 0.002393999189180559 and parameters: {'lr': 0.00800793282691575, 'hs': 19, 'dropout': 0.07923325958005795}. Best is trial 24 with value: 0.002128556279837506.\u001b[0m\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "\u001b[32m[I 2025-10-30 06:51:05,768]\u001b[0m Trial 29 finished with value: 0.0022230819615842623 and parameters: {'lr': 0.007404121973750125, 'hs': 29, 'dropout': 0.07381552801441622}. Best is trial 24 with value: 0.002128556279837506.\u001b[0m\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "\u001b[32m[I 2025-10-30 06:51:16,649]\u001b[0m Trial 30 finished with value: 0.0030061809910398606 and parameters: {'lr': 0.0027830682478292754, 'hs': 15, 'dropout': 0.09427476566076651}. Best is trial 24 with value: 0.002128556279837506.\u001b[0m\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "\u001b[32m[I 2025-10-30 06:51:25,691]\u001b[0m Trial 31 finished with value: 0.0023329827584210425 and parameters: {'lr': 0.006434294721416983, 'hs': 25, 'dropout': 0.06174939765747482}. Best is trial 24 with value: 0.002128556279837506.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "\u001b[32m[I 2025-10-30 06:51:35,094]\u001b[0m Trial 32 finished with value: 0.0022151891327599517 and parameters: {'lr': 0.005873481263424454, 'hs': 28, 'dropout': 0.05046302844806919}. Best is trial 24 with value: 0.002128556279837506.\u001b[0m\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "\u001b[32m[I 2025-10-30 06:51:43,099]\u001b[0m Trial 33 finished with value: 0.0024576848460857 and parameters: {'lr': 0.004203991149431666, 'hs': 24, 'dropout': 0.06389527677848868}. Best is trial 24 with value: 0.002128556279837506.\u001b[0m\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "\u001b[32m[I 2025-10-30 06:51:51,839]\u001b[0m Trial 34 finished with value: 0.002398345289220548 and parameters: {'lr': 0.005421139799213294, 'hs': 21, 'dropout': 0.07300514655284375}. Best is trial 24 with value: 0.002128556279837506.\u001b[0m\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "\u001b[32m[I 2025-10-30 06:51:59,601]\u001b[0m Trial 35 finished with value: 0.002248257228425783 and parameters: {'lr': 0.006571917482511979, 'hs': 26, 'dropout': 0.057737692009864686}. Best is trial 24 with value: 0.002128556279837506.\u001b[0m\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "\u001b[32m[I 2025-10-30 06:52:07,962]\u001b[0m Trial 36 finished with value: 0.0022581244218617337 and parameters: {'lr': 0.004668397014014714, 'hs': 20, 'dropout': 0.07890112451627561}. Best is trial 24 with value: 0.002128556279837506.\u001b[0m\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "\u001b[32m[I 2025-10-30 06:52:16,396]\u001b[0m Trial 37 finished with value: 0.002104776521535009 and parameters: {'lr': 0.008786167388712728, 'hs': 30, 'dropout': 0.06690505254916872}. Best is trial 37 with value: 0.002104776521535009.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "\u001b[32m[I 2025-10-30 06:52:24,287]\u001b[0m Trial 38 finished with value: 0.0031469232273247487 and parameters: {'lr': 0.008894613726822117, 'hs': 4, 'dropout': 0.09590003714557825}. Best is trial 37 with value: 0.002104776521535009.\u001b[0m\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "\u001b[32m[I 2025-10-30 06:52:32,829]\u001b[0m Trial 39 finished with value: 0.0021611150417767595 and parameters: {'lr': 0.008613874742267328, 'hs': 30, 'dropout': 0.05771806372304085}. Best is trial 37 with value: 0.002104776521535009.\u001b[0m\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "\u001b[32m[I 2025-10-30 06:52:40,749]\u001b[0m Trial 40 finished with value: 0.0022265167837430956 and parameters: {'lr': 0.009486882847758999, 'hs': 30, 'dropout': 0.07411649501594371}. Best is trial 37 with value: 0.002104776521535009.\u001b[0m\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "\u001b[32m[I 2025-10-30 06:52:50,031]\u001b[0m Trial 41 finished with value: 0.0021694111130156654 and parameters: {'lr': 0.00855452503324251, 'hs': 30, 'dropout': 0.05648131366997193}. Best is trial 37 with value: 0.002104776521535009.\u001b[0m\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "\u001b[32m[I 2025-10-30 06:52:58,576]\u001b[0m Trial 42 finished with value: 0.0022072333316871147 and parameters: {'lr': 0.008500630391464476, 'hs': 30, 'dropout': 0.05669964171733944}. Best is trial 37 with value: 0.002104776521535009.\u001b[0m\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "\u001b[32m[I 2025-10-30 06:53:06,726]\u001b[0m Trial 43 finished with value: 0.002406143964532289 and parameters: {'lr': 0.009255558017419936, 'hs': 30, 'dropout': 0.0674491375801014}. Best is trial 37 with value: 0.002104776521535009.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "\u001b[32m[I 2025-10-30 06:53:14,474]\u001b[0m Trial 44 finished with value: 0.009656367603658704 and parameters: {'lr': 0.008574538765234868, 'hs': 2, 'dropout': 0.08150111918566302}. Best is trial 37 with value: 0.002104776521535009.\u001b[0m\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "\u001b[32m[I 2025-10-30 06:53:21,579]\u001b[0m Trial 45 finished with value: 0.002333596630823207 and parameters: {'lr': 0.007389364542578484, 'hs': 31, 'dropout': 0.05939968659468844}. Best is trial 37 with value: 0.002104776521535009.\u001b[0m\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "\u001b[32m[I 2025-10-30 06:53:28,194]\u001b[0m Trial 46 finished with value: 0.0023942042520601696 and parameters: {'lr': 0.007896766399416161, 'hs': 28, 'dropout': 0.07127378006534549}. Best is trial 37 with value: 0.002104776521535009.\u001b[0m\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "\u001b[32m[I 2025-10-30 06:53:39,627]\u001b[0m Trial 47 finished with value: 0.0025254444977458935 and parameters: {'lr': 0.003605929644419375, 'hs': 28, 'dropout': 0.08665186700197754}. Best is trial 37 with value: 0.002104776521535009.\u001b[0m\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "\u001b[32m[I 2025-10-30 06:53:47,399]\u001b[0m Trial 48 finished with value: 0.002268328919277352 and parameters: {'lr': 0.009370638778533625, 'hs': 31, 'dropout': 0.056254135289796545}. Best is trial 37 with value: 0.002104776521535009.\u001b[0m\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "\u001b[32m[I 2025-10-30 06:53:55,097]\u001b[0m Trial 49 finished with value: 0.0022264169683208546 and parameters: {'lr': 0.00997771390154512, 'hs': 31, 'dropout': 0.06376077970199892}. Best is trial 37 with value: 0.002104776521535009.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "('LSTM', 1.7009880978826768e+17, 160554549.5048026, 0.7864915674771878, 'ARIMA', True, 'bce')\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "('LSTM', 1.4280981057223627e+17, 163217873.84986198, 0.8226182515571584, 'ARIMA', True, 'bce')\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "('LSTM', 9.2244588249776e+16, 135439789.07065275, 0.8088188559292857, 'ARIMA', True, 'bce')\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "('LSTM', 6.454218328443199e+16, 130359780.88279203, 0.8438834336939052, 'ARIMA', True, 'bce')\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "('LSTM', 1.777379012703171e+17, 169022123.03643182, 0.7307776966661192, 'ARIMA', True, 'bce')\n",
      "\u001b[32m[I 2025-10-30 06:56:18,611]\u001b[0m A new study created in memory with name: no-name-24119d69-cdca-425a-9b12-dee3c09bc6c2\u001b[0m\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1394: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.gru(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1394: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.gru(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1394: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.gru(\n",
      "\u001b[32m[I 2025-10-30 06:56:29,954]\u001b[0m Trial 0 finished with value: 0.02556765002560717 and parameters: {'lr': 0.003940241646682268, 'hs': 10, 'dropout': 0.11597374819937235}. Best is trial 0 with value: 0.02556765002560717.\u001b[0m\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1394: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.gru(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1394: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.gru(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1394: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.gru(\n",
      "\u001b[32m[I 2025-10-30 06:56:38,326]\u001b[0m Trial 1 finished with value: 0.023879473715407922 and parameters: {'lr': 0.008436947807244815, 'hs': 9, 'dropout': 0.17086761381191337}. Best is trial 1 with value: 0.023879473715407922.\u001b[0m\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1394: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.gru(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1394: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.gru(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1394: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.gru(\n",
      "\u001b[32m[I 2025-10-30 06:56:48,998]\u001b[0m Trial 2 finished with value: 0.024970798111783866 and parameters: {'lr': 0.001953209685759818, 'hs': 18, 'dropout': 0.19782526388498345}. Best is trial 1 with value: 0.023879473715407922.\u001b[0m\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1394: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.gru(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1394: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.gru(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1394: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.gru(\n",
      "\u001b[32m[I 2025-10-30 06:56:59,454]\u001b[0m Trial 3 finished with value: 0.023815077665751524 and parameters: {'lr': 0.008789558185709628, 'hs': 20, 'dropout': 0.08595390359554886}. Best is trial 3 with value: 0.023815077665751524.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1394: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.gru(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1394: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.gru(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1394: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.gru(\n",
      "\u001b[32m[I 2025-10-30 06:57:25,710]\u001b[0m Trial 4 finished with value: 0.03314149049263374 and parameters: {'lr': 0.0013501853088902732, 'hs': 3, 'dropout': 0.11715142581304074}. Best is trial 3 with value: 0.023815077665751524.\u001b[0m\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1394: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.gru(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1394: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.gru(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1394: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.gru(\n",
      "\u001b[32m[I 2025-10-30 06:57:32,897]\u001b[0m Trial 5 finished with value: 0.02485875378406942 and parameters: {'lr': 0.008515075787380354, 'hs': 19, 'dropout': 0.10619196154397939}. Best is trial 3 with value: 0.023815077665751524.\u001b[0m\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1394: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.gru(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1394: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.gru(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1394: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.gru(\n",
      "\u001b[32m[I 2025-10-30 06:57:40,814]\u001b[0m Trial 6 finished with value: 0.02224486364538847 and parameters: {'lr': 0.006043172326956716, 'hs': 16, 'dropout': 0.12525356888878603}. Best is trial 6 with value: 0.02224486364538847.\u001b[0m\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1394: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.gru(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1394: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.gru(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1394: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.gru(\n",
      "\u001b[32m[I 2025-10-30 06:57:49,889]\u001b[0m Trial 7 finished with value: 0.02535442161610596 and parameters: {'lr': 0.005661009669362265, 'hs': 12, 'dropout': 0.12883799849424554}. Best is trial 6 with value: 0.02224486364538847.\u001b[0m\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1394: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.gru(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1394: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.gru(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1394: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.gru(\n",
      "\u001b[32m[I 2025-10-30 06:57:58,950]\u001b[0m Trial 8 finished with value: 0.026783116001767766 and parameters: {'lr': 0.008904045201036869, 'hs': 10, 'dropout': 0.16609487212856505}. Best is trial 6 with value: 0.02224486364538847.\u001b[0m\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1394: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.gru(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1394: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.gru(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1394: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.gru(\n",
      "\u001b[32m[I 2025-10-30 06:58:07,007]\u001b[0m Trial 9 finished with value: 0.022081630162957725 and parameters: {'lr': 0.004220243049174236, 'hs': 24, 'dropout': 0.1401558290294213}. Best is trial 9 with value: 0.022081630162957725.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1394: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.gru(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1394: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.gru(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1394: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.gru(\n",
      "\u001b[32m[I 2025-10-30 06:58:15,795]\u001b[0m Trial 10 finished with value: 0.01969486003686791 and parameters: {'lr': 0.0036034732410865794, 'hs': 30, 'dropout': 0.06667770623617954}. Best is trial 10 with value: 0.01969486003686791.\u001b[0m\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1394: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.gru(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1394: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.gru(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1394: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.gru(\n",
      "\u001b[32m[I 2025-10-30 06:58:25,483]\u001b[0m Trial 11 finished with value: 0.020872679690659274 and parameters: {'lr': 0.003971563338117171, 'hs': 30, 'dropout': 0.07146607167600838}. Best is trial 10 with value: 0.01969486003686791.\u001b[0m\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1394: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.gru(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1394: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.gru(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1394: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.gru(\n",
      "\u001b[32m[I 2025-10-30 06:58:35,701]\u001b[0m Trial 12 finished with value: 0.021885857431733075 and parameters: {'lr': 0.0026994954114202764, 'hs': 30, 'dropout': 0.05058509702604504}. Best is trial 10 with value: 0.01969486003686791.\u001b[0m\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1394: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.gru(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1394: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.gru(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1394: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.gru(\n",
      "\u001b[32m[I 2025-10-30 06:58:55,142]\u001b[0m Trial 13 finished with value: 0.02350631826730473 and parameters: {'lr': 0.0005770668042771234, 'hs': 32, 'dropout': 0.054944922968261756}. Best is trial 10 with value: 0.01969486003686791.\u001b[0m\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1394: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.gru(\n"
     ]
    }
   ],
   "source": [
    "!python3.10 experiments.py --imputation ARIMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26317921",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3.10 install torchdata==0.8.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c8a0b7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "print(torch.version.cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9aa77da",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3.10 install torchvision==0.18.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd258879",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3.10 install torchtext==0.18.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc103f1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12ced98c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3.10 install torch==2.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e9856a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3.10 install sktime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0b91876",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3.10 install -U dgl -f https://data.dgl.ai/wheels/torch-2.3/cu121/repo.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c5a8cb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3.10 install pydantic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6389887e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3.10 install pytorch_forecasting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e25ece39",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3.10 install timekan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a3d6682",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3.10 install torchvision==0.24.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeea73a0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
