{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "66bdca18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/keen/.pyenv/versions/3.9.13/lib/python3.9/site-packages/torch/utils/_pytree.py:185: FutureWarning: optree is installed but the version is too old to support PyTorch Dynamo in C++ pytree. C++ pytree support is disabled. Please consider upgrading optree using `python3 -m pip install --upgrade 'optree>=0.13.0'`.\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2025-11-01 00:58:20,175]\u001b[0m A new study created in memory with name: no-name-1b92bc2d-b62a-4ccc-b1e1-5bf59129a0b5\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 00:58:23,885]\u001b[0m Trial 0 finished with value: 2.3572586178992725 and parameters: {'lr': 0.053599579242339684, 'hs': 15, 'dropout': 0.19090822201686192}. Best is trial 0 with value: 2.3572586178992725.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 00:58:29,358]\u001b[0m Trial 1 finished with value: 2.0571613021922297 and parameters: {'lr': 0.03789059836305676, 'hs': 23, 'dropout': 0.1675207453966957}. Best is trial 1 with value: 2.0571613021922297.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 00:58:33,955]\u001b[0m Trial 2 finished with value: 2.23456592866091 and parameters: {'lr': 0.05000514650347145, 'hs': 13, 'dropout': 0.1968958762023249}. Best is trial 1 with value: 2.0571613021922297.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 00:58:38,302]\u001b[0m Trial 3 finished with value: 2.040135336285409 and parameters: {'lr': 0.09352564183532136, 'hs': 19, 'dropout': 0.15510626450436887}. Best is trial 3 with value: 2.040135336285409.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 00:58:45,037]\u001b[0m Trial 4 finished with value: 1.8438673390765732 and parameters: {'lr': 0.018550025196586023, 'hs': 5, 'dropout': 0.06252388965408463}. Best is trial 4 with value: 1.8438673390765732.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 00:58:49,283]\u001b[0m Trial 5 finished with value: 2.2447921529541808 and parameters: {'lr': 0.036667579413554904, 'hs': 6, 'dropout': 0.1631533497442236}. Best is trial 4 with value: 1.8438673390765732.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 00:58:54,539]\u001b[0m Trial 6 finished with value: 1.892907318350663 and parameters: {'lr': 0.06968535367130035, 'hs': 31, 'dropout': 0.08020213289187463}. Best is trial 4 with value: 1.8438673390765732.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 00:59:00,677]\u001b[0m Trial 7 finished with value: 1.6968554818475916 and parameters: {'lr': 0.06891577571428475, 'hs': 19, 'dropout': 0.059927400037095756}. Best is trial 7 with value: 1.6968554818475916.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 00:59:08,049]\u001b[0m Trial 8 finished with value: 4.80505836971061 and parameters: {'lr': 0.002166404743535579, 'hs': 5, 'dropout': 0.16297751051655918}. Best is trial 7 with value: 1.6968554818475916.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 00:59:13,133]\u001b[0m Trial 9 finished with value: 1.8689216214422875 and parameters: {'lr': 0.08819968738370473, 'hs': 19, 'dropout': 0.050531532625800714}. Best is trial 7 with value: 1.6968554818475916.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 00:59:18,690]\u001b[0m Trial 10 finished with value: 1.863205750715012 and parameters: {'lr': 0.07764998730748747, 'hs': 27, 'dropout': 0.10338290711264433}. Best is trial 7 with value: 1.6968554818475916.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 00:59:24,350]\u001b[0m Trial 11 finished with value: 2.463916947981643 and parameters: {'lr': 0.006047283200751218, 'hs': 10, 'dropout': 0.05277483564159283}. Best is trial 7 with value: 1.6968554818475916.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 00:59:27,861]\u001b[0m Trial 12 finished with value: 3.4522525118801926 and parameters: {'lr': 0.019115984552989157, 'hs': 3, 'dropout': 0.08156461284952207}. Best is trial 7 with value: 1.6968554818475916.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 00:59:30,999]\u001b[0m Trial 13 finished with value: 2.2074448748189712 and parameters: {'lr': 0.06560832950926382, 'hs': 10, 'dropout': 0.12588061466579076}. Best is trial 7 with value: 1.6968554818475916.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 00:59:36,278]\u001b[0m Trial 14 finished with value: 1.924273996877935 and parameters: {'lr': 0.025654534348093, 'hs': 23, 'dropout': 0.08159000624155174}. Best is trial 7 with value: 1.6968554818475916.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 00:59:39,913]\u001b[0m Trial 15 finished with value: 2.2087061700984285 and parameters: {'lr': 0.0583829637499786, 'hs': 10, 'dropout': 0.113772444123974}. Best is trial 7 with value: 1.6968554818475916.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 00:59:43,022]\u001b[0m Trial 16 finished with value: 2.151655618163391 and parameters: {'lr': 0.04185807265668015, 'hs': 22, 'dropout': 0.06662391824843306}. Best is trial 7 with value: 1.6968554818475916.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 00:59:46,533]\u001b[0m Trial 17 finished with value: 2.3467243289001285 and parameters: {'lr': 0.022722964638074068, 'hs': 16, 'dropout': 0.13496562293133257}. Best is trial 7 with value: 1.6968554818475916.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 00:59:50,980]\u001b[0m Trial 18 finished with value: 2.4871757579979885 and parameters: {'lr': 0.08074951164424397, 'hs': 2, 'dropout': 0.09888028088091236}. Best is trial 7 with value: 1.6968554818475916.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 00:59:57,141]\u001b[0m Trial 19 finished with value: 2.1008667457762917 and parameters: {'lr': 0.010555771042035948, 'hs': 28, 'dropout': 0.06557661526857324}. Best is trial 7 with value: 1.6968554818475916.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 01:00:00,861]\u001b[0m Trial 20 finished with value: 2.0545530520498785 and parameters: {'lr': 0.06570774666310025, 'hs': 13, 'dropout': 0.09147516791159133}. Best is trial 7 with value: 1.6968554818475916.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 01:00:04,597]\u001b[0m Trial 21 finished with value: 1.986535835643635 and parameters: {'lr': 0.07655083395715503, 'hs': 27, 'dropout': 0.10652026677437401}. Best is trial 7 with value: 1.6968554818475916.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 01:00:08,954]\u001b[0m Trial 22 finished with value: 1.9242440058235648 and parameters: {'lr': 0.0787151231739191, 'hs': 26, 'dropout': 0.06440526875327118}. Best is trial 7 with value: 1.6968554818475916.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 01:00:14,490]\u001b[0m Trial 23 finished with value: 1.7764298626123407 and parameters: {'lr': 0.09838634947223185, 'hs': 32, 'dropout': 0.14115392921418668}. Best is trial 7 with value: 1.6968554818475916.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 01:00:20,276]\u001b[0m Trial 24 finished with value: 1.8245952933894107 and parameters: {'lr': 0.0995637601320285, 'hs': 32, 'dropout': 0.14025425093099173}. Best is trial 7 with value: 1.6968554818475916.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 01:00:26,723]\u001b[0m Trial 25 finished with value: 1.8672641636735203 and parameters: {'lr': 0.09499799798616204, 'hs': 31, 'dropout': 0.14382782048903195}. Best is trial 7 with value: 1.6968554818475916.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 01:00:31,926]\u001b[0m Trial 26 finished with value: 1.8446874253158179 and parameters: {'lr': 0.09929747979103506, 'hs': 30, 'dropout': 0.12105103678925967}. Best is trial 7 with value: 1.6968554818475916.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 01:00:38,246]\u001b[0m Trial 27 finished with value: 1.8611992139967615 and parameters: {'lr': 0.08597616088172769, 'hs': 32, 'dropout': 0.1792425536202123}. Best is trial 7 with value: 1.6968554818475916.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 01:00:43,299]\u001b[0m Trial 28 finished with value: 1.9295408546617356 and parameters: {'lr': 0.0910328946349659, 'hs': 25, 'dropout': 0.14420709150364924}. Best is trial 7 with value: 1.6968554818475916.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 01:00:48,555]\u001b[0m Trial 29 finished with value: 2.064430055264056 and parameters: {'lr': 0.09995204762980955, 'hs': 29, 'dropout': 0.1804655505288579}. Best is trial 7 with value: 1.6968554818475916.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 01:00:52,728]\u001b[0m Trial 30 finished with value: 1.9978172693845992 and parameters: {'lr': 0.055088302730080335, 'hs': 18, 'dropout': 0.12935530610970827}. Best is trial 7 with value: 1.6968554818475916.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 01:00:56,032]\u001b[0m Trial 31 finished with value: 2.190938242522618 and parameters: {'lr': 0.084471743828765, 'hs': 8, 'dropout': 0.14708059665727596}. Best is trial 7 with value: 1.6968554818475916.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 01:01:01,118]\u001b[0m Trial 32 finished with value: 1.9300830271074405 and parameters: {'lr': 0.04871912076989823, 'hs': 24, 'dropout': 0.13913897658233093}. Best is trial 7 with value: 1.6968554818475916.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 01:01:05,162]\u001b[0m Trial 33 finished with value: 2.0284319646449567 and parameters: {'lr': 0.07089602056558686, 'hs': 21, 'dropout': 0.1162174063761772}. Best is trial 7 with value: 1.6968554818475916.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 01:01:09,414]\u001b[0m Trial 34 finished with value: 2.058768626975219 and parameters: {'lr': 0.034321657818929395, 'hs': 14, 'dropout': 0.08931830026803066}. Best is trial 7 with value: 1.6968554818475916.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 01:01:13,784]\u001b[0m Trial 35 finished with value: 1.9893674614572312 and parameters: {'lr': 0.045672596322226686, 'hs': 32, 'dropout': 0.15543084370500906}. Best is trial 7 with value: 1.6968554818475916.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 01:01:18,017]\u001b[0m Trial 36 finished with value: 2.162923774409617 and parameters: {'lr': 0.030284590332212053, 'hs': 21, 'dropout': 0.07217383419233649}. Best is trial 7 with value: 1.6968554818475916.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 01:01:22,780]\u001b[0m Trial 37 finished with value: 2.2617398333271868 and parameters: {'lr': 0.01511795104394175, 'hs': 16, 'dropout': 0.1532865675090465}. Best is trial 7 with value: 1.6968554818475916.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 01:01:26,381]\u001b[0m Trial 38 finished with value: 2.2841009618051067 and parameters: {'lr': 0.06160965169343569, 'hs': 6, 'dropout': 0.05864228932047595}. Best is trial 7 with value: 1.6968554818475916.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 01:01:31,204]\u001b[0m Trial 39 finished with value: 2.1184156334352355 and parameters: {'lr': 0.0954989289725562, 'hs': 30, 'dropout': 0.1731382907546688}. Best is trial 7 with value: 1.6968554818475916.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 01:01:37,368]\u001b[0m Trial 40 finished with value: 1.8760406427399907 and parameters: {'lr': 0.09062703980787222, 'hs': 29, 'dropout': 0.1307819438200067}. Best is trial 7 with value: 1.6968554818475916.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 01:01:44,593]\u001b[0m Trial 41 finished with value: 1.7220890280911867 and parameters: {'lr': 0.09971789902852443, 'hs': 30, 'dropout': 0.11919385294987565}. Best is trial 7 with value: 1.6968554818475916.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 01:01:49,786]\u001b[0m Trial 42 finished with value: 1.841703941038002 and parameters: {'lr': 0.09530931973865527, 'hs': 32, 'dropout': 0.1602746135300552}. Best is trial 7 with value: 1.6968554818475916.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 01:01:53,510]\u001b[0m Trial 43 finished with value: 2.1354416584058806 and parameters: {'lr': 0.09499057250226442, 'hs': 32, 'dropout': 0.1620599832410374}. Best is trial 7 with value: 1.6968554818475916.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 01:01:58,905]\u001b[0m Trial 44 finished with value: 2.0249024800392887 and parameters: {'lr': 0.08449962538000882, 'hs': 28, 'dropout': 0.19760739333493726}. Best is trial 7 with value: 1.6968554818475916.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 01:02:03,063]\u001b[0m Trial 45 finished with value: 1.9643167694883032 and parameters: {'lr': 0.09948737256015246, 'hs': 30, 'dropout': 0.1865393051258932}. Best is trial 7 with value: 1.6968554818475916.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 01:02:09,369]\u001b[0m Trial 46 finished with value: 1.8616739597747654 and parameters: {'lr': 0.07299326845872833, 'hs': 31, 'dropout': 0.16784393252589527}. Best is trial 7 with value: 1.6968554818475916.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 01:02:13,423]\u001b[0m Trial 47 finished with value: 2.0972316903137416 and parameters: {'lr': 0.09031780218123178, 'hs': 26, 'dropout': 0.15044366115416835}. Best is trial 7 with value: 1.6968554818475916.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 01:02:19,049]\u001b[0m Trial 48 finished with value: 1.997858017665384 and parameters: {'lr': 0.08119294947928037, 'hs': 28, 'dropout': 0.15925044827933393}. Best is trial 7 with value: 1.6968554818475916.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 01:02:25,113]\u001b[0m Trial 49 finished with value: 1.8217055841398366 and parameters: {'lr': 0.09459938209751317, 'hs': 32, 'dropout': 0.13913319233326787}. Best is trial 7 with value: 1.6968554818475916.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 01:02:31,052]\u001b[0m Trial 50 finished with value: 1.7674251212622938 and parameters: {'lr': 0.08817050815824058, 'hs': 29, 'dropout': 0.110745395448787}. Best is trial 7 with value: 1.6968554818475916.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 01:02:36,284]\u001b[0m Trial 51 finished with value: 2.0709933039220942 and parameters: {'lr': 0.08748831899564284, 'hs': 30, 'dropout': 0.12223289730577176}. Best is trial 7 with value: 1.6968554818475916.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 01:02:41,288]\u001b[0m Trial 52 finished with value: 1.862746208599096 and parameters: {'lr': 0.09208559051422988, 'hs': 29, 'dropout': 0.13835683225827583}. Best is trial 7 with value: 1.6968554818475916.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 01:02:47,482]\u001b[0m Trial 53 finished with value: 1.872116631134716 and parameters: {'lr': 0.09690214393891552, 'hs': 27, 'dropout': 0.10958346350969242}. Best is trial 7 with value: 1.6968554818475916.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 01:02:53,656]\u001b[0m Trial 54 finished with value: 1.8450765207414346 and parameters: {'lr': 0.07517315748391493, 'hs': 31, 'dropout': 0.10132278445234442}. Best is trial 7 with value: 1.6968554818475916.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 01:02:57,952]\u001b[0m Trial 55 finished with value: 1.9396117616746038 and parameters: {'lr': 0.0829406297500033, 'hs': 25, 'dropout': 0.13383420287566336}. Best is trial 7 with value: 1.6968554818475916.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 01:03:03,232]\u001b[0m Trial 56 finished with value: 1.8326021598559705 and parameters: {'lr': 0.08803769905952719, 'hs': 31, 'dropout': 0.11606956542574895}. Best is trial 7 with value: 1.6968554818475916.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 01:03:06,553]\u001b[0m Trial 57 finished with value: 2.187416464168164 and parameters: {'lr': 0.09235378794139776, 'hs': 29, 'dropout': 0.13937780989833776}. Best is trial 7 with value: 1.6968554818475916.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 01:03:10,551]\u001b[0m Trial 58 finished with value: 1.9535008790838322 and parameters: {'lr': 0.09756040793097943, 'hs': 19, 'dropout': 0.09517309651022217}. Best is trial 7 with value: 1.6968554818475916.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 01:03:15,020]\u001b[0m Trial 59 finished with value: 1.8939358678588958 and parameters: {'lr': 0.08722190273320439, 'hs': 28, 'dropout': 0.1247841496764688}. Best is trial 7 with value: 1.6968554818475916.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 01:03:18,537]\u001b[0m Trial 60 finished with value: 2.2675293629330877 and parameters: {'lr': 0.06489841696600986, 'hs': 11, 'dropout': 0.1104522976420911}. Best is trial 7 with value: 1.6968554818475916.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 01:03:24,938]\u001b[0m Trial 61 finished with value: 1.8232486918593651 and parameters: {'lr': 0.0885318286726438, 'hs': 31, 'dropout': 0.11724248822880727}. Best is trial 7 with value: 1.6968554818475916.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 01:03:30,872]\u001b[0m Trial 62 finished with value: 1.8610058880882159 and parameters: {'lr': 0.09341085126511255, 'hs': 32, 'dropout': 0.11874137274355413}. Best is trial 7 with value: 1.6968554818475916.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 01:03:36,022]\u001b[0m Trial 63 finished with value: 2.0513873461904844 and parameters: {'lr': 0.08027575545407217, 'hs': 30, 'dropout': 0.1264911932664757}. Best is trial 7 with value: 1.6968554818475916.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 01:03:41,979]\u001b[0m Trial 64 finished with value: 1.985274681023526 and parameters: {'lr': 0.09044423297987218, 'hs': 31, 'dropout': 0.13294995667528414}. Best is trial 7 with value: 1.6968554818475916.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 01:03:48,153]\u001b[0m Trial 65 finished with value: 1.9681537210516291 and parameters: {'lr': 0.09630090943779339, 'hs': 29, 'dropout': 0.14493129167518107}. Best is trial 7 with value: 1.6968554818475916.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 01:03:52,462]\u001b[0m Trial 66 finished with value: 1.9606177736228647 and parameters: {'lr': 0.09979225043058527, 'hs': 26, 'dropout': 0.11085820708187447}. Best is trial 7 with value: 1.6968554818475916.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 01:03:57,550]\u001b[0m Trial 67 finished with value: 1.9038862508014713 and parameters: {'lr': 0.08489581766389628, 'hs': 32, 'dropout': 0.07635959198932527}. Best is trial 7 with value: 1.6968554818475916.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 01:04:02,372]\u001b[0m Trial 68 finished with value: 1.9946910634300261 and parameters: {'lr': 0.07748120315921661, 'hs': 23, 'dropout': 0.1050837325444422}. Best is trial 7 with value: 1.6968554818475916.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 01:04:07,523]\u001b[0m Trial 69 finished with value: 2.1034208370470453 and parameters: {'lr': 0.08871956853398681, 'hs': 27, 'dropout': 0.14022700652544737}. Best is trial 7 with value: 1.6968554818475916.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 01:04:12,714]\u001b[0m Trial 70 finished with value: 1.9043891128428083 and parameters: {'lr': 0.09336622744171988, 'hs': 30, 'dropout': 0.08683705416683543}. Best is trial 7 with value: 1.6968554818475916.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 01:04:17,512]\u001b[0m Trial 71 finished with value: 2.0888932758924295 and parameters: {'lr': 0.08811524229857923, 'hs': 31, 'dropout': 0.11597987190859911}. Best is trial 7 with value: 1.6968554818475916.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 01:04:23,147]\u001b[0m Trial 72 finished with value: 1.8119788143032007 and parameters: {'lr': 0.09748164294100833, 'hs': 31, 'dropout': 0.1273365983440858}. Best is trial 7 with value: 1.6968554818475916.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 01:04:28,706]\u001b[0m Trial 73 finished with value: 1.7921115039773543 and parameters: {'lr': 0.096622789084204, 'hs': 32, 'dropout': 0.1260044146708103}. Best is trial 7 with value: 1.6968554818475916.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 01:04:34,483]\u001b[0m Trial 74 finished with value: 1.9221767789134259 and parameters: {'lr': 0.09738112421858995, 'hs': 29, 'dropout': 0.12769081921456074}. Best is trial 7 with value: 1.6968554818475916.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 01:04:39,847]\u001b[0m Trial 75 finished with value: 1.851320216766254 and parameters: {'lr': 0.08237727827537851, 'hs': 28, 'dropout': 0.12089159081345555}. Best is trial 7 with value: 1.6968554818475916.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 01:04:44,953]\u001b[0m Trial 76 finished with value: 1.8192903388887463 and parameters: {'lr': 0.09403624021602014, 'hs': 32, 'dropout': 0.13675167477512912}. Best is trial 7 with value: 1.6968554818475916.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 01:04:51,506]\u001b[0m Trial 77 finished with value: 1.738090648825895 and parameters: {'lr': 0.0932581293983101, 'hs': 32, 'dropout': 0.1486791558195846}. Best is trial 7 with value: 1.6968554818475916.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 01:04:56,226]\u001b[0m Trial 78 finished with value: 2.0877416043190045 and parameters: {'lr': 0.06870256933651403, 'hs': 30, 'dropout': 0.1341368857413885}. Best is trial 7 with value: 1.6968554818475916.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 01:05:01,241]\u001b[0m Trial 79 finished with value: 1.9766067139641554 and parameters: {'lr': 0.09760921464379427, 'hs': 31, 'dropout': 0.1537298436022581}. Best is trial 7 with value: 1.6968554818475916.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 01:05:06,566]\u001b[0m Trial 80 finished with value: 1.8781311933098255 and parameters: {'lr': 0.0922362102486492, 'hs': 32, 'dropout': 0.1471992065564472}. Best is trial 7 with value: 1.6968554818475916.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 01:05:13,293]\u001b[0m Trial 81 finished with value: 1.8233206083314923 and parameters: {'lr': 0.09430723931330288, 'hs': 32, 'dropout': 0.1372023675147731}. Best is trial 7 with value: 1.6968554818475916.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 01:05:18,076]\u001b[0m Trial 82 finished with value: 1.982101075074959 and parameters: {'lr': 0.09462229801466523, 'hs': 32, 'dropout': 0.1307715582749126}. Best is trial 7 with value: 1.6968554818475916.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 01:05:23,619]\u001b[0m Trial 83 finished with value: 1.8264296660474704 and parameters: {'lr': 0.09046944514593776, 'hs': 29, 'dropout': 0.12314569987857088}. Best is trial 7 with value: 1.6968554818475916.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 01:05:29,730]\u001b[0m Trial 84 finished with value: 1.9002911860104854 and parameters: {'lr': 0.09768278318253583, 'hs': 30, 'dropout': 0.14999006743734292}. Best is trial 7 with value: 1.6968554818475916.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 01:05:34,612]\u001b[0m Trial 85 finished with value: 1.9333534093624483 and parameters: {'lr': 0.08588500427512438, 'hs': 17, 'dropout': 0.14418955109024387}. Best is trial 7 with value: 1.6968554818475916.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 01:05:39,984]\u001b[0m Trial 86 finished with value: 1.9135290131775495 and parameters: {'lr': 0.05406776030478674, 'hs': 30, 'dropout': 0.056250173183212085}. Best is trial 7 with value: 1.6968554818475916.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 01:05:44,978]\u001b[0m Trial 87 finished with value: 1.9941548149370079 and parameters: {'lr': 0.09942246576480271, 'hs': 31, 'dropout': 0.12923244122056407}. Best is trial 7 with value: 1.6968554818475916.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 01:05:49,625]\u001b[0m Trial 88 finished with value: 2.0563831878079006 and parameters: {'lr': 0.05753580288828209, 'hs': 21, 'dropout': 0.16823499187131535}. Best is trial 7 with value: 1.6968554818475916.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 01:05:55,868]\u001b[0m Trial 89 finished with value: 1.93238735620087 and parameters: {'lr': 0.09543859939796989, 'hs': 28, 'dropout': 0.13588160603827534}. Best is trial 7 with value: 1.6968554818475916.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 01:06:00,494]\u001b[0m Trial 90 finished with value: 1.9511598079475672 and parameters: {'lr': 0.0930313498897758, 'hs': 32, 'dropout': 0.14191168906934962}. Best is trial 7 with value: 1.6968554818475916.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 01:06:05,087]\u001b[0m Trial 91 finished with value: 1.9577849820106945 and parameters: {'lr': 0.08960632955199754, 'hs': 31, 'dropout': 0.12615117723391553}. Best is trial 7 with value: 1.6968554818475916.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 01:06:09,472]\u001b[0m Trial 92 finished with value: 2.07942461002188 and parameters: {'lr': 0.08362611059187773, 'hs': 31, 'dropout': 0.11808509034703199}. Best is trial 7 with value: 1.6968554818475916.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 01:06:15,547]\u001b[0m Trial 93 finished with value: 1.8962612828560477 and parameters: {'lr': 0.07979198643409735, 'hs': 29, 'dropout': 0.11241968803581531}. Best is trial 7 with value: 1.6968554818475916.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 01:06:20,170]\u001b[0m Trial 94 finished with value: 2.010693557795764 and parameters: {'lr': 0.09185425332842337, 'hs': 27, 'dropout': 0.10720169544377481}. Best is trial 7 with value: 1.6968554818475916.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 01:06:26,481]\u001b[0m Trial 95 finished with value: 1.8263880784271638 and parameters: {'lr': 0.09654832537422127, 'hs': 32, 'dropout': 0.15756353065299705}. Best is trial 7 with value: 1.6968554818475916.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 01:06:32,136]\u001b[0m Trial 96 finished with value: 1.862984970630878 and parameters: {'lr': 0.07470050384617286, 'hs': 31, 'dropout': 0.14987765773138587}. Best is trial 7 with value: 1.6968554818475916.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 01:06:37,421]\u001b[0m Trial 97 finished with value: 1.7478595801280534 and parameters: {'lr': 0.0863181182893637, 'hs': 30, 'dropout': 0.13153820559946236}. Best is trial 7 with value: 1.6968554818475916.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 01:06:43,581]\u001b[0m Trial 98 finished with value: 1.8396368829577086 and parameters: {'lr': 0.08589818443577998, 'hs': 30, 'dropout': 0.13345624332444017}. Best is trial 7 with value: 1.6968554818475916.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 01:06:48,626]\u001b[0m Trial 99 finished with value: 2.0183884072658906 and parameters: {'lr': 0.09795597593506912, 'hs': 25, 'dropout': 0.14730582263116748}. Best is trial 7 with value: 1.6968554818475916.\u001b[0m\n",
      "('LSTM', 2.5387874848611738e+17, 192520884.00251636, 2.813823634557451, 0.47859389043224887, 0.6562966511643253, 'FF', True, 'mse')\n",
      "('LSTM', 3.359439926119917e+17, 227790953.6642562, 1.6629984786504175, 0.4299417896726926, 0.5923216397919433, 'FF', True, 'mse')\n",
      "('LSTM', 1.671602658678375e+17, 166337294.0595599, 1.892881082767124, 0.5457346074397412, 0.688074356528553, 'FF', True, 'mse')\n",
      "('LSTM', 2.1280544014037434e+17, 179686652.83533794, 2.479213093067609, 0.38683285572579057, 0.7543585723106143, 'FF', True, 'mse')\n",
      "('LSTM', 1.6458758360390595e+17, 173330144.12146538, 2.828451536053283, 0.4021360705936577, 0.7750808825194911, 'FF', True, 'mse')\n",
      "\u001b[32m[I 2025-11-01 01:08:29,004]\u001b[0m A new study created in memory with name: no-name-5bb5774c-3699-436f-9d92-e104485950d3\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 01:08:33,028]\u001b[0m Trial 0 finished with value: 0.0041105212054181476 and parameters: {'lr': 0.02711928489103498, 'hs': 6, 'dropout': 0.1862614441193794}. Best is trial 0 with value: 0.0041105212054181476.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 01:08:36,896]\u001b[0m Trial 1 finished with value: 0.0027771955435384166 and parameters: {'lr': 0.016953069153302042, 'hs': 17, 'dropout': 0.15835603713694546}. Best is trial 1 with value: 0.0027771955435384166.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 01:08:40,620]\u001b[0m Trial 2 finished with value: 0.0022557903128986993 and parameters: {'lr': 0.06873386833246801, 'hs': 16, 'dropout': 0.08759421272191578}. Best is trial 2 with value: 0.0022557903128986993.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 01:08:43,624]\u001b[0m Trial 3 finished with value: 0.0035246716359463356 and parameters: {'lr': 0.07591392666951334, 'hs': 8, 'dropout': 0.14575831301088982}. Best is trial 2 with value: 0.0022557903128986993.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 01:08:49,771]\u001b[0m Trial 4 finished with value: 0.0023914614150476726 and parameters: {'lr': 0.012629996912054003, 'hs': 32, 'dropout': 0.12633209657780187}. Best is trial 2 with value: 0.0022557903128986993.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 01:08:54,994]\u001b[0m Trial 5 finished with value: 0.0024627955995625487 and parameters: {'lr': 0.06510107829910665, 'hs': 19, 'dropout': 0.08334591800836835}. Best is trial 2 with value: 0.0022557903128986993.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 01:08:59,334]\u001b[0m Trial 6 finished with value: 0.0023284125311813465 and parameters: {'lr': 0.09561842504451348, 'hs': 17, 'dropout': 0.15117974731040512}. Best is trial 2 with value: 0.0022557903128986993.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 01:09:03,782]\u001b[0m Trial 7 finished with value: 0.0025650033806714403 and parameters: {'lr': 0.07734893405773326, 'hs': 10, 'dropout': 0.11279366706357268}. Best is trial 2 with value: 0.0022557903128986993.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 01:09:08,749]\u001b[0m Trial 8 finished with value: 0.0025305845845287546 and parameters: {'lr': 0.03621268289614422, 'hs': 29, 'dropout': 0.1522195037605517}. Best is trial 2 with value: 0.0022557903128986993.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 01:09:14,019]\u001b[0m Trial 9 finished with value: 0.002350682962939213 and parameters: {'lr': 0.016348406970797712, 'hs': 29, 'dropout': 0.11559951590835128}. Best is trial 2 with value: 0.0022557903128986993.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 01:09:16,719]\u001b[0m Trial 10 finished with value: 0.007638841782825874 and parameters: {'lr': 0.053235118501825586, 'hs': 2, 'dropout': 0.05171800633497473}. Best is trial 2 with value: 0.0022557903128986993.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 01:09:20,999]\u001b[0m Trial 11 finished with value: 0.00254950306011155 and parameters: {'lr': 0.09903782848540432, 'hs': 17, 'dropout': 0.08738562062799823}. Best is trial 2 with value: 0.0022557903128986993.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 01:09:26,196]\u001b[0m Trial 12 finished with value: 0.002988112024201482 and parameters: {'lr': 0.0972261884564748, 'hs': 21, 'dropout': 0.1992081778865065}. Best is trial 2 with value: 0.0022557903128986993.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 01:09:30,410]\u001b[0m Trial 13 finished with value: 0.0026993016598599374 and parameters: {'lr': 0.08414315840840642, 'hs': 13, 'dropout': 0.0802836101109075}. Best is trial 2 with value: 0.0022557903128986993.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 01:09:35,599]\u001b[0m Trial 14 finished with value: 0.002236367148337398 and parameters: {'lr': 0.04907398888581763, 'hs': 24, 'dropout': 0.05201359216638517}. Best is trial 14 with value: 0.002236367148337398.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 01:09:39,871]\u001b[0m Trial 15 finished with value: 0.002434863587724535 and parameters: {'lr': 0.04854117863378703, 'hs': 24, 'dropout': 0.05132781135824402}. Best is trial 14 with value: 0.002236367148337398.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 01:09:45,178]\u001b[0m Trial 16 finished with value: 0.0023551259578762427 and parameters: {'lr': 0.05160544319187405, 'hs': 24, 'dropout': 0.07030061892844197}. Best is trial 14 with value: 0.002236367148337398.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 01:09:48,453]\u001b[0m Trial 17 finished with value: 0.0025988610231138768 and parameters: {'lr': 0.06343879711259434, 'hs': 13, 'dropout': 0.1004606566562681}. Best is trial 14 with value: 0.002236367148337398.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 01:10:11,760]\u001b[0m Trial 18 finished with value: 0.004001597968509486 and parameters: {'lr': 0.00042418158359697894, 'hs': 23, 'dropout': 0.0663239224027164}. Best is trial 14 with value: 0.002236367148337398.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 01:10:15,916]\u001b[0m Trial 19 finished with value: 0.0021740873398419618 and parameters: {'lr': 0.06244825400577417, 'hs': 26, 'dropout': 0.09593879115982873}. Best is trial 19 with value: 0.0021740873398419618.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 01:10:20,909]\u001b[0m Trial 20 finished with value: 0.0023177824931231825 and parameters: {'lr': 0.03749015339488035, 'hs': 26, 'dropout': 0.10176005467694084}. Best is trial 19 with value: 0.0021740873398419618.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 01:10:25,723]\u001b[0m Trial 21 finished with value: 0.002309073673160076 and parameters: {'lr': 0.06255720932307057, 'hs': 27, 'dropout': 0.06422468016644724}. Best is trial 19 with value: 0.0021740873398419618.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 01:10:29,628]\u001b[0m Trial 22 finished with value: 0.0022273824103753817 and parameters: {'lr': 0.07447616466206029, 'hs': 21, 'dropout': 0.0931232937031839}. Best is trial 19 with value: 0.0021740873398419618.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 01:10:34,502]\u001b[0m Trial 23 finished with value: 0.002524165963584693 and parameters: {'lr': 0.08551993463473061, 'hs': 20, 'dropout': 0.13079329904496692}. Best is trial 19 with value: 0.0021740873398419618.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 01:10:39,633]\u001b[0m Trial 24 finished with value: 0.0022748811952308175 and parameters: {'lr': 0.04326100155141965, 'hs': 32, 'dropout': 0.10107404005602198}. Best is trial 19 with value: 0.0021740873398419618.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 01:10:44,322]\u001b[0m Trial 25 finished with value: 0.0024095240947926053 and parameters: {'lr': 0.05617310338290578, 'hs': 22, 'dropout': 0.07438817626762115}. Best is trial 19 with value: 0.0021740873398419618.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 01:10:48,439]\u001b[0m Trial 26 finished with value: 0.002195511282808199 and parameters: {'lr': 0.07537452724830308, 'hs': 26, 'dropout': 0.09369382655000218}. Best is trial 19 with value: 0.0021740873398419618.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 01:10:54,755]\u001b[0m Trial 27 finished with value: 0.002239552406391043 and parameters: {'lr': 0.07726115234311767, 'hs': 27, 'dropout': 0.114223924110048}. Best is trial 19 with value: 0.0021740873398419618.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 01:11:00,187]\u001b[0m Trial 28 finished with value: 0.002227006185969708 and parameters: {'lr': 0.0852951391189108, 'hs': 30, 'dropout': 0.0941148670281986}. Best is trial 19 with value: 0.0021740873398419618.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 01:11:06,269]\u001b[0m Trial 29 finished with value: 0.0023961972352546834 and parameters: {'lr': 0.08483753153911125, 'hs': 30, 'dropout': 0.1385398781823966}. Best is trial 19 with value: 0.0021740873398419618.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 01:11:11,893]\u001b[0m Trial 30 finished with value: 0.0024699302285193523 and parameters: {'lr': 0.09198261664722897, 'hs': 30, 'dropout': 0.1661564168561805}. Best is trial 19 with value: 0.0021740873398419618.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 01:11:16,794]\u001b[0m Trial 31 finished with value: 0.0024594691317011224 and parameters: {'lr': 0.07088090802296926, 'hs': 26, 'dropout': 0.09393359691942481}. Best is trial 19 with value: 0.0021740873398419618.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 01:11:20,771]\u001b[0m Trial 32 finished with value: 0.0021943879531156705 and parameters: {'lr': 0.08985030304552709, 'hs': 26, 'dropout': 0.10527584125313441}. Best is trial 19 with value: 0.0021740873398419618.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 01:11:26,523]\u001b[0m Trial 33 finished with value: 0.0024100626361156317 and parameters: {'lr': 0.08862057629247996, 'hs': 28, 'dropout': 0.10881765609653021}. Best is trial 19 with value: 0.0021740873398419618.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 01:11:31,549]\u001b[0m Trial 34 finished with value: 0.0023503417993495468 and parameters: {'lr': 0.08032703397219706, 'hs': 25, 'dropout': 0.11888683129848794}. Best is trial 19 with value: 0.0021740873398419618.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 01:11:36,285]\u001b[0m Trial 35 finished with value: 0.002559766492256966 and parameters: {'lr': 0.07178065353912884, 'hs': 31, 'dropout': 0.10759128079385842}. Best is trial 19 with value: 0.0021740873398419618.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 01:11:42,566]\u001b[0m Trial 36 finished with value: 0.002468942125271544 and parameters: {'lr': 0.059652725734111796, 'hs': 28, 'dropout': 0.12587903775907247}. Best is trial 19 with value: 0.0021740873398419618.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 01:11:47,811]\u001b[0m Trial 37 finished with value: 0.0022188632108754275 and parameters: {'lr': 0.0907860152709837, 'hs': 32, 'dropout': 0.09386564938663233}. Best is trial 19 with value: 0.0021740873398419618.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 01:11:52,500]\u001b[0m Trial 38 finished with value: 0.0024199099295078557 and parameters: {'lr': 0.09170077070009988, 'hs': 32, 'dropout': 0.08094518275437636}. Best is trial 19 with value: 0.0021740873398419618.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 01:11:57,652]\u001b[0m Trial 39 finished with value: 0.0022729507573257496 and parameters: {'lr': 0.06803328029369803, 'hs': 19, 'dropout': 0.07815916636139644}. Best is trial 19 with value: 0.0021740873398419618.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 01:12:02,152]\u001b[0m Trial 40 finished with value: 0.0025839585050626804 and parameters: {'lr': 0.0801310084505517, 'hs': 15, 'dropout': 0.08936709959921563}. Best is trial 19 with value: 0.0021740873398419618.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 01:12:07,362]\u001b[0m Trial 41 finished with value: 0.002356977423975081 and parameters: {'lr': 0.09397915728347148, 'hs': 30, 'dropout': 0.0986492145188169}. Best is trial 19 with value: 0.0021740873398419618.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 01:12:12,159]\u001b[0m Trial 42 finished with value: 0.002155191678256346 and parameters: {'lr': 0.08666034769916786, 'hs': 28, 'dropout': 0.10682685020440104}. Best is trial 42 with value: 0.002155191678256346.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 01:12:17,437]\u001b[0m Trial 43 finished with value: 0.00224691856793312 and parameters: {'lr': 0.08968721749750456, 'hs': 28, 'dropout': 0.10697460717236863}. Best is trial 42 with value: 0.002155191678256346.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 01:12:22,645]\u001b[0m Trial 44 finished with value: 0.002493210802815298 and parameters: {'lr': 0.08099741901536121, 'hs': 25, 'dropout': 0.11989967750604448}. Best is trial 42 with value: 0.002155191678256346.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 01:12:26,689]\u001b[0m Trial 45 finished with value: 0.002531351450050941 and parameters: {'lr': 0.09826387803277792, 'hs': 26, 'dropout': 0.13249943007170417}. Best is trial 42 with value: 0.002155191678256346.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 01:12:31,556]\u001b[0m Trial 46 finished with value: 0.002358188019670093 and parameters: {'lr': 0.07404468850627481, 'hs': 32, 'dropout': 0.08831743133190237}. Best is trial 42 with value: 0.002155191678256346.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 01:12:33,939]\u001b[0m Trial 47 finished with value: 0.008415066810051351 and parameters: {'lr': 0.09999990830609583, 'hs': 3, 'dropout': 0.10788988913500848}. Best is trial 42 with value: 0.002155191678256346.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 01:12:38,197]\u001b[0m Trial 48 finished with value: 0.0021495756823468925 and parameters: {'lr': 0.06733973006549941, 'hs': 23, 'dropout': 0.08587263964252698}. Best is trial 48 with value: 0.0021495756823468925.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 01:12:44,334]\u001b[0m Trial 49 finished with value: 0.002261373224701871 and parameters: {'lr': 0.057159248030342794, 'hs': 23, 'dropout': 0.08469964599395831}. Best is trial 48 with value: 0.0021495756823468925.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 01:12:49,248]\u001b[0m Trial 50 finished with value: 0.002468938592372501 and parameters: {'lr': 0.06701354192015756, 'hs': 22, 'dropout': 0.06178210076534262}. Best is trial 48 with value: 0.0021495756823468925.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 01:12:55,731]\u001b[0m Trial 51 finished with value: 0.0023288607435401534 and parameters: {'lr': 0.08765203358419768, 'hs': 29, 'dropout': 0.09478049123996525}. Best is trial 48 with value: 0.0021495756823468925.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 01:13:01,355]\u001b[0m Trial 52 finished with value: 0.002285394572743247 and parameters: {'lr': 0.07808409807252159, 'hs': 27, 'dropout': 0.07529251752187066}. Best is trial 48 with value: 0.0021495756823468925.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 01:13:05,681]\u001b[0m Trial 53 finished with value: 0.0023606382205858963 and parameters: {'lr': 0.026060618748834072, 'hs': 25, 'dropout': 0.10484576434772729}. Best is trial 48 with value: 0.0021495756823468925.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 01:13:10,017]\u001b[0m Trial 54 finished with value: 0.0021967734848935996 and parameters: {'lr': 0.0944257301011744, 'hs': 29, 'dropout': 0.08595970792051746}. Best is trial 48 with value: 0.0021495756823468925.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 01:13:14,360]\u001b[0m Trial 55 finished with value: 0.0022400671439634745 and parameters: {'lr': 0.09512546587661845, 'hs': 23, 'dropout': 0.08382965263856429}. Best is trial 48 with value: 0.0021495756823468925.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 01:13:19,599]\u001b[0m Trial 56 finished with value: 0.0024543570169751313 and parameters: {'lr': 0.06420267626629528, 'hs': 28, 'dropout': 0.1121353905379498}. Best is trial 48 with value: 0.0021495756823468925.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 01:13:24,305]\u001b[0m Trial 57 finished with value: 0.0028222407109444545 and parameters: {'lr': 0.0823029475296196, 'hs': 26, 'dropout': 0.05811125096983327}. Best is trial 48 with value: 0.0021495756823468925.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 01:13:27,935]\u001b[0m Trial 58 finished with value: 0.0026554206159702873 and parameters: {'lr': 0.07084734902041022, 'hs': 24, 'dropout': 0.09994031174574657}. Best is trial 48 with value: 0.0021495756823468925.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 01:13:32,868]\u001b[0m Trial 59 finished with value: 0.0021018143223849473 and parameters: {'lr': 0.06037302518978309, 'hs': 20, 'dropout': 0.06961158140852369}. Best is trial 59 with value: 0.0021018143223849473.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 01:13:37,758]\u001b[0m Trial 60 finished with value: 0.0024124024831216413 and parameters: {'lr': 0.05924144481786713, 'hs': 18, 'dropout': 0.07059638637885451}. Best is trial 59 with value: 0.0021018143223849473.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 01:13:41,375]\u001b[0m Trial 61 finished with value: 0.0024202744871598533 and parameters: {'lr': 0.04523111053983963, 'hs': 21, 'dropout': 0.07064364088467949}. Best is trial 59 with value: 0.0021018143223849473.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 01:13:46,237]\u001b[0m Trial 62 finished with value: 0.0023339648423336694 and parameters: {'lr': 0.062429315064048245, 'hs': 22, 'dropout': 0.0887008967308925}. Best is trial 59 with value: 0.0021018143223849473.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 01:13:50,451]\u001b[0m Trial 63 finished with value: 0.002234589194333762 and parameters: {'lr': 0.05427509021040289, 'hs': 19, 'dropout': 0.07706575981079615}. Best is trial 59 with value: 0.0021018143223849473.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 01:13:56,213]\u001b[0m Trial 64 finished with value: 0.0021929189865170493 and parameters: {'lr': 0.07579892301518412, 'hs': 27, 'dropout': 0.09723605805655658}. Best is trial 59 with value: 0.0021018143223849473.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 01:14:01,237]\u001b[0m Trial 65 finished with value: 0.0027772441602618935 and parameters: {'lr': 0.0727202415144356, 'hs': 24, 'dropout': 0.11945135696030151}. Best is trial 59 with value: 0.0021018143223849473.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 01:14:05,865]\u001b[0m Trial 66 finished with value: 0.0023127692880039945 and parameters: {'lr': 0.07607056733573753, 'hs': 20, 'dropout': 0.09907067603581379}. Best is trial 59 with value: 0.0021018143223849473.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 01:14:11,040]\u001b[0m Trial 67 finished with value: 0.0025518685270641502 and parameters: {'lr': 0.0673061674414445, 'hs': 27, 'dropout': 0.17229669871579012}. Best is trial 59 with value: 0.0021018143223849473.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 01:14:15,285]\u001b[0m Trial 68 finished with value: 0.0021475793945821124 and parameters: {'lr': 0.06139763945685378, 'hs': 25, 'dropout': 0.11263744608534663}. Best is trial 59 with value: 0.0021018143223849473.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 01:14:20,116]\u001b[0m Trial 69 finished with value: 0.0022788572836289953 and parameters: {'lr': 0.04962048037366418, 'hs': 23, 'dropout': 0.11207391745072977}. Best is trial 59 with value: 0.0021018143223849473.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 01:14:24,269]\u001b[0m Trial 70 finished with value: 0.002529013097207779 and parameters: {'lr': 0.05998525681592573, 'hs': 16, 'dropout': 0.10308479853528649}. Best is trial 59 with value: 0.0021018143223849473.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 01:14:29,331]\u001b[0m Trial 71 finished with value: 0.002537143213879451 and parameters: {'lr': 0.07011061667335299, 'hs': 25, 'dropout': 0.11590424582241572}. Best is trial 59 with value: 0.0021018143223849473.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 01:14:34,371]\u001b[0m Trial 72 finished with value: 0.0023270196343344457 and parameters: {'lr': 0.06595196739596722, 'hs': 26, 'dropout': 0.09765123689781838}. Best is trial 59 with value: 0.0021018143223849473.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 01:14:40,460]\u001b[0m Trial 73 finished with value: 0.0025838110715418476 and parameters: {'lr': 0.062300198354904, 'hs': 27, 'dropout': 0.09066670999228603}. Best is trial 59 with value: 0.0021018143223849473.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 01:14:43,929]\u001b[0m Trial 74 finished with value: 0.0022697212266926435 and parameters: {'lr': 0.054629784707971786, 'hs': 24, 'dropout': 0.12976676975884027}. Best is trial 59 with value: 0.0021018143223849473.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 01:14:48,510]\u001b[0m Trial 75 finished with value: 0.002700946432742235 and parameters: {'lr': 0.07811797826817395, 'hs': 25, 'dropout': 0.08105390785940517}. Best is trial 59 with value: 0.0021018143223849473.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 01:14:53,744]\u001b[0m Trial 76 finished with value: 0.002603842927093193 and parameters: {'lr': 0.08349010100716696, 'hs': 28, 'dropout': 0.11047609291687623}. Best is trial 59 with value: 0.0021018143223849473.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 01:14:58,613]\u001b[0m Trial 77 finished with value: 0.0024457958792203417 and parameters: {'lr': 0.07542930655331626, 'hs': 27, 'dropout': 0.12311229691790235}. Best is trial 59 with value: 0.0021018143223849473.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 01:15:04,254]\u001b[0m Trial 78 finished with value: 0.0029968427910058598 and parameters: {'lr': 0.05762787751050555, 'hs': 21, 'dropout': 0.10346637472126578}. Best is trial 59 with value: 0.0021018143223849473.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 01:15:09,442]\u001b[0m Trial 79 finished with value: 0.002283042674850733 and parameters: {'lr': 0.08648640036462463, 'hs': 29, 'dropout': 0.09617996408524808}. Best is trial 59 with value: 0.0021018143223849473.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 01:15:14,780]\u001b[0m Trial 80 finished with value: 0.002506760526727079 and parameters: {'lr': 0.052976994880819896, 'hs': 20, 'dropout': 0.09196232130250909}. Best is trial 59 with value: 0.0021018143223849473.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 01:15:18,972]\u001b[0m Trial 81 finished with value: 0.0023792090660866593 and parameters: {'lr': 0.06890756523467954, 'hs': 29, 'dropout': 0.0860267611639408}. Best is trial 59 with value: 0.0021018143223849473.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 01:15:24,384]\u001b[0m Trial 82 finished with value: 0.002314274624227331 and parameters: {'lr': 0.09265599014786906, 'hs': 31, 'dropout': 0.06688500927502278}. Best is trial 59 with value: 0.0021018143223849473.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 01:15:28,574]\u001b[0m Trial 83 finished with value: 0.0023950608325598744 and parameters: {'lr': 0.08810539993766298, 'hs': 26, 'dropout': 0.10470094382911628}. Best is trial 59 with value: 0.0021018143223849473.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 01:15:34,136]\u001b[0m Trial 84 finished with value: 0.0022866998651940597 and parameters: {'lr': 0.0968301127936649, 'hs': 31, 'dropout': 0.057444189935268196}. Best is trial 59 with value: 0.0021018143223849473.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 01:15:41,860]\u001b[0m Trial 85 finished with value: 0.0033370407792940697 and parameters: {'lr': 0.003074917639209293, 'hs': 10, 'dropout': 0.08150356513284303}. Best is trial 59 with value: 0.0021018143223849473.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 01:15:46,258]\u001b[0m Trial 86 finished with value: 0.0022468392752797665 and parameters: {'lr': 0.06487854679625885, 'hs': 22, 'dropout': 0.11528613304464552}. Best is trial 59 with value: 0.0021018143223849473.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 01:15:50,958]\u001b[0m Trial 87 finished with value: 0.0021697044476106987 and parameters: {'lr': 0.07306636472046014, 'hs': 29, 'dropout': 0.08655787778111212}. Best is trial 59 with value: 0.0021018143223849473.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 01:15:55,392]\u001b[0m Trial 88 finished with value: 0.0023017874138713886 and parameters: {'lr': 0.07914565081781576, 'hs': 25, 'dropout': 0.09361460837246428}. Best is trial 59 with value: 0.0021018143223849473.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 01:16:00,588]\u001b[0m Trial 89 finished with value: 0.0020776678813666737 and parameters: {'lr': 0.07327318389046214, 'hs': 28, 'dropout': 0.07326934099961249}. Best is trial 89 with value: 0.0020776678813666737.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 01:16:06,452]\u001b[0m Trial 90 finished with value: 0.00262110477163585 and parameters: {'lr': 0.060918151914113434, 'hs': 28, 'dropout': 0.07305452585464486}. Best is trial 89 with value: 0.0020776678813666737.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 01:16:11,868]\u001b[0m Trial 91 finished with value: 0.0022270799286916226 and parameters: {'lr': 0.07298181428438347, 'hs': 27, 'dropout': 0.09761390425159952}. Best is trial 89 with value: 0.0020776678813666737.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 01:16:17,667]\u001b[0m Trial 92 finished with value: 0.002456110901175598 and parameters: {'lr': 0.06843340874319538, 'hs': 30, 'dropout': 0.07833052147799446}. Best is trial 89 with value: 0.0020776678813666737.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 01:16:23,041]\u001b[0m Trial 93 finished with value: 0.0023218346568782324 and parameters: {'lr': 0.07642045840570214, 'hs': 26, 'dropout': 0.06679289988482447}. Best is trial 89 with value: 0.0020776678813666737.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 01:16:28,421]\u001b[0m Trial 94 finished with value: 0.002110281584460978 and parameters: {'lr': 0.07422598792577713, 'hs': 28, 'dropout': 0.08402475845130256}. Best is trial 89 with value: 0.0020776678813666737.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 01:16:33,958]\u001b[0m Trial 95 finished with value: 0.0020650314648512236 and parameters: {'lr': 0.08135332036951907, 'hs': 28, 'dropout': 0.07259261750847998}. Best is trial 95 with value: 0.0020650314648512236.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 01:16:38,891]\u001b[0m Trial 96 finished with value: 0.0020390629085101485 and parameters: {'lr': 0.08205877050505453, 'hs': 30, 'dropout': 0.06016488900319909}. Best is trial 96 with value: 0.0020390629085101485.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 01:16:43,981]\u001b[0m Trial 97 finished with value: 0.0022737688630177573 and parameters: {'lr': 0.08213181871089617, 'hs': 29, 'dropout': 0.05832668325972904}. Best is trial 96 with value: 0.0020390629085101485.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 01:16:50,253]\u001b[0m Trial 98 finished with value: 0.0021869576159764 and parameters: {'lr': 0.07248667750970093, 'hs': 31, 'dropout': 0.05325761132423211}. Best is trial 96 with value: 0.0020390629085101485.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 01:16:54,126]\u001b[0m Trial 99 finished with value: 0.0022144152341729245 and parameters: {'lr': 0.06530948995956677, 'hs': 30, 'dropout': 0.07360682210474802}. Best is trial 96 with value: 0.0020390629085101485.\u001b[0m\n",
      "('LSTM', 2.4935537959779098e+17, 183610449.6314492, 10.961045129236997, 0.3743923351246592, 0.6672671051846117, 'FF', True, 'bce')\n",
      "('LSTM', 1.976398093864768e+17, 158174604.01293978, 12.65423636711139, 0.30792913426011903, 0.7473385555560434, 'FF', True, 'bce')\n",
      "('LSTM', 1.816689674878889e+17, 161090361.8485807, 6.420186081392113, 0.36355310658807005, 0.7342721185638454, 'FF', True, 'bce')\n",
      "('LSTM', 1.6690089062889837e+17, 164885231.990536, 12.16449480780625, 0.31038369369373525, 0.7856431015800149, 'FF', True, 'bce')\n",
      "('LSTM', 1.8486189914626768e+17, 159075124.85512042, 3.9099391028034414, 0.35530188505511967, 0.7713114293019713, 'FF', True, 'bce')\n",
      "\u001b[32m[I 2025-11-01 01:18:48,592]\u001b[0m A new study created in memory with name: no-name-266782ae-0ea4-40bd-bd89-19798e8f1594\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 01:18:53,114]\u001b[0m Trial 0 finished with value: 2.3546643619102134 and parameters: {'lr': 0.00881422970221201, 'hs': 17, 'dropout': 0.16646744698972032}. Best is trial 0 with value: 2.3546643619102134.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 01:18:56,917]\u001b[0m Trial 1 finished with value: 2.074231706206597 and parameters: {'lr': 0.02216576749340208, 'hs': 17, 'dropout': 0.08118566327457553}. Best is trial 1 with value: 2.074231706206597.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 01:19:00,898]\u001b[0m Trial 2 finished with value: 2.115970602365747 and parameters: {'lr': 0.03139997602831336, 'hs': 12, 'dropout': 0.1605933602119341}. Best is trial 1 with value: 2.074231706206597.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 01:19:05,024]\u001b[0m Trial 3 finished with value: 1.8390182053272301 and parameters: {'lr': 0.07785509260157611, 'hs': 15, 'dropout': 0.12716461379013982}. Best is trial 3 with value: 1.8390182053272301.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 01:19:10,285]\u001b[0m Trial 4 finished with value: 1.8621907379307796 and parameters: {'lr': 0.07243750850660491, 'hs': 13, 'dropout': 0.1967344164597984}. Best is trial 3 with value: 1.8390182053272301.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 01:19:14,043]\u001b[0m Trial 5 finished with value: 2.089615927199443 and parameters: {'lr': 0.03603494084250682, 'hs': 12, 'dropout': 0.13266025650702892}. Best is trial 3 with value: 1.8390182053272301.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 01:19:17,949]\u001b[0m Trial 6 finished with value: 2.2774750564238784 and parameters: {'lr': 0.020299813892822986, 'hs': 18, 'dropout': 0.19044400588187943}. Best is trial 3 with value: 1.8390182053272301.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 01:19:21,988]\u001b[0m Trial 7 finished with value: 2.001031491288542 and parameters: {'lr': 0.02770868732935914, 'hs': 18, 'dropout': 0.09882138261779473}. Best is trial 3 with value: 1.8390182053272301.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 01:19:25,525]\u001b[0m Trial 8 finished with value: 2.139465414141325 and parameters: {'lr': 0.08976497373378786, 'hs': 12, 'dropout': 0.19183500158108208}. Best is trial 3 with value: 1.8390182053272301.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 01:19:28,985]\u001b[0m Trial 9 finished with value: 1.943161139931049 and parameters: {'lr': 0.04315098001539001, 'hs': 11, 'dropout': 0.06622141417187691}. Best is trial 3 with value: 1.8390182053272301.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 01:19:34,046]\u001b[0m Trial 10 finished with value: 1.8499590627109843 and parameters: {'lr': 0.06187866950968371, 'hs': 32, 'dropout': 0.12178170741289351}. Best is trial 3 with value: 1.8390182053272301.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 01:19:38,749]\u001b[0m Trial 11 finished with value: 1.7687043731577914 and parameters: {'lr': 0.06567586629215877, 'hs': 31, 'dropout': 0.11973157651790471}. Best is trial 11 with value: 1.7687043731577914.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 01:19:42,913]\u001b[0m Trial 12 finished with value: 1.7711645762901942 and parameters: {'lr': 0.09600583966741458, 'hs': 28, 'dropout': 0.12136672465867408}. Best is trial 11 with value: 1.7687043731577914.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 01:19:48,958]\u001b[0m Trial 13 finished with value: 1.77335075851249 and parameters: {'lr': 0.0995721754172916, 'hs': 31, 'dropout': 0.10388746570633814}. Best is trial 11 with value: 1.7687043731577914.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 01:19:53,788]\u001b[0m Trial 14 finished with value: 1.7655236794398743 and parameters: {'lr': 0.05917753984474444, 'hs': 26, 'dropout': 0.1484086870944746}. Best is trial 14 with value: 1.7655236794398743.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 01:19:59,048]\u001b[0m Trial 15 finished with value: 1.7975085445354526 and parameters: {'lr': 0.055031618069300736, 'hs': 24, 'dropout': 0.14762595167928236}. Best is trial 14 with value: 1.7655236794398743.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 01:20:02,528]\u001b[0m Trial 16 finished with value: 1.9001170075352116 and parameters: {'lr': 0.0628042842368557, 'hs': 24, 'dropout': 0.15408052605766004}. Best is trial 14 with value: 1.7655236794398743.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 01:20:06,489]\u001b[0m Trial 17 finished with value: 1.9048118807073486 and parameters: {'lr': 0.079267918770371, 'hs': 6, 'dropout': 0.09847398223214417}. Best is trial 14 with value: 1.7655236794398743.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 01:20:10,544]\u001b[0m Trial 18 finished with value: 1.8948981369996314 and parameters: {'lr': 0.04712987740820321, 'hs': 24, 'dropout': 0.1727258407449806}. Best is trial 14 with value: 1.7655236794398743.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 01:20:14,770]\u001b[0m Trial 19 finished with value: 1.936683806611236 and parameters: {'lr': 0.060466468052315994, 'hs': 28, 'dropout': 0.1446896000240726}. Best is trial 14 with value: 1.7655236794398743.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 01:20:19,810]\u001b[0m Trial 20 finished with value: 1.7372595833245363 and parameters: {'lr': 0.07097829074379179, 'hs': 27, 'dropout': 0.05076100573069148}. Best is trial 20 with value: 1.7372595833245363.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 01:20:24,471]\u001b[0m Trial 21 finished with value: 1.78304982519637 and parameters: {'lr': 0.07185740829955518, 'hs': 28, 'dropout': 0.05152499041761027}. Best is trial 20 with value: 1.7372595833245363.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 01:20:28,881]\u001b[0m Trial 22 finished with value: 1.8432627513055977 and parameters: {'lr': 0.0694177987911563, 'hs': 21, 'dropout': 0.07921048302476763}. Best is trial 20 with value: 1.7372595833245363.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 01:20:33,113]\u001b[0m Trial 23 finished with value: 1.7958185372343973 and parameters: {'lr': 0.08467577647295572, 'hs': 29, 'dropout': 0.1781736652975562}. Best is trial 20 with value: 1.7372595833245363.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 01:20:37,741]\u001b[0m Trial 24 finished with value: 1.788851356921861 and parameters: {'lr': 0.05300068786249265, 'hs': 26, 'dropout': 0.0505643962619305}. Best is trial 20 with value: 1.7372595833245363.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 01:20:42,856]\u001b[0m Trial 25 finished with value: 1.7731153403511541 and parameters: {'lr': 0.0674800046367852, 'hs': 21, 'dropout': 0.11170197416018546}. Best is trial 20 with value: 1.7372595833245363.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 01:20:48,459]\u001b[0m Trial 26 finished with value: 1.7968593178688936 and parameters: {'lr': 0.04446186427284004, 'hs': 32, 'dropout': 0.13762366148813582}. Best is trial 20 with value: 1.7372595833245363.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 01:20:53,903]\u001b[0m Trial 27 finished with value: 1.752033951112846 and parameters: {'lr': 0.05575697626828312, 'hs': 30, 'dropout': 0.08479269219837507}. Best is trial 20 with value: 1.7372595833245363.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 01:20:58,486]\u001b[0m Trial 28 finished with value: 1.8602362226237077 and parameters: {'lr': 0.038428199716824846, 'hs': 26, 'dropout': 0.06793022251192013}. Best is trial 20 with value: 1.7372595833245363.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 01:21:02,584]\u001b[0m Trial 29 finished with value: 1.852709540362869 and parameters: {'lr': 0.04993390549041985, 'hs': 21, 'dropout': 0.06313052727986702}. Best is trial 20 with value: 1.7372595833245363.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 01:21:10,220]\u001b[0m Trial 30 finished with value: 2.454957537363741 and parameters: {'lr': 0.002727884496778514, 'hs': 25, 'dropout': 0.09026186978829841}. Best is trial 20 with value: 1.7372595833245363.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 01:21:14,740]\u001b[0m Trial 31 finished with value: 1.7594515454787236 and parameters: {'lr': 0.05706761326122014, 'hs': 30, 'dropout': 0.11302235271292223}. Best is trial 20 with value: 1.7372595833245363.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 01:21:19,574]\u001b[0m Trial 32 finished with value: 1.922332272094015 and parameters: {'lr': 0.05629934458286635, 'hs': 30, 'dropout': 0.08357779637914196}. Best is trial 20 with value: 1.7372595833245363.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 01:21:25,245]\u001b[0m Trial 33 finished with value: 1.7995850586657463 and parameters: {'lr': 0.07886671697254939, 'hs': 27, 'dropout': 0.07593756437005746}. Best is trial 20 with value: 1.7372595833245363.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 01:21:30,737]\u001b[0m Trial 34 finished with value: 1.8288501826855008 and parameters: {'lr': 0.057235241851144474, 'hs': 30, 'dropout': 0.10957665872414496}. Best is trial 20 with value: 1.7372595833245363.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 01:21:35,430]\u001b[0m Trial 35 finished with value: 1.8129985716432604 and parameters: {'lr': 0.07482702971481246, 'hs': 23, 'dropout': 0.09073031360998714}. Best is trial 20 with value: 1.7372595833245363.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 01:21:38,596]\u001b[0m Trial 36 finished with value: 2.843823254616368 and parameters: {'lr': 0.05146872470312663, 'hs': 2, 'dropout': 0.15928882683925671}. Best is trial 20 with value: 1.7372595833245363.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 01:21:42,262]\u001b[0m Trial 37 finished with value: 1.7547330724183894 and parameters: {'lr': 0.08546525639815021, 'hs': 22, 'dropout': 0.060231041610251106}. Best is trial 20 with value: 1.7372595833245363.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 01:21:46,184]\u001b[0m Trial 38 finished with value: 1.8143836652303422 and parameters: {'lr': 0.08597602538870312, 'hs': 22, 'dropout': 0.0605767480056386}. Best is trial 20 with value: 1.7372595833245363.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 01:21:50,694]\u001b[0m Trial 39 finished with value: 1.8910109338072993 and parameters: {'lr': 0.08983134557853571, 'hs': 15, 'dropout': 0.07425258101124542}. Best is trial 20 with value: 1.7372595833245363.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 01:21:56,132]\u001b[0m Trial 40 finished with value: 1.6789074671088653 and parameters: {'lr': 0.08280039073809771, 'hs': 19, 'dropout': 0.060446471585336184}. Best is trial 40 with value: 1.6789074671088653.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 01:22:00,505]\u001b[0m Trial 41 finished with value: 1.8250026614043493 and parameters: {'lr': 0.08270556706423156, 'hs': 20, 'dropout': 0.057788177526476414}. Best is trial 40 with value: 1.6789074671088653.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 01:22:04,692]\u001b[0m Trial 42 finished with value: 1.799596728365511 and parameters: {'lr': 0.09128349148747546, 'hs': 19, 'dropout': 0.06880542285221937}. Best is trial 40 with value: 1.6789074671088653.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 01:22:09,326]\u001b[0m Trial 43 finished with value: 1.7619916375476314 and parameters: {'lr': 0.07562229998853877, 'hs': 16, 'dropout': 0.05636425057663843}. Best is trial 40 with value: 1.6789074671088653.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 01:22:13,323]\u001b[0m Trial 44 finished with value: 1.8079122824443614 and parameters: {'lr': 0.064973771435244, 'hs': 29, 'dropout': 0.08830193451294263}. Best is trial 40 with value: 1.6789074671088653.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 01:22:17,977]\u001b[0m Trial 45 finished with value: 1.9696036585021872 and parameters: {'lr': 0.017436080300710394, 'hs': 18, 'dropout': 0.07263696120337296}. Best is trial 40 with value: 1.6789074671088653.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 01:22:22,987]\u001b[0m Trial 46 finished with value: 1.721402085325214 and parameters: {'lr': 0.09551014716788432, 'hs': 32, 'dropout': 0.08447829333534769}. Best is trial 40 with value: 1.6789074671088653.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 01:22:27,975]\u001b[0m Trial 47 finished with value: 1.7895113105585168 and parameters: {'lr': 0.09373848579388244, 'hs': 27, 'dropout': 0.08198135078879061}. Best is trial 40 with value: 1.6789074671088653.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 01:22:32,019]\u001b[0m Trial 48 finished with value: 1.877364433861298 and parameters: {'lr': 0.0991645983806575, 'hs': 32, 'dropout': 0.05629016893513701}. Best is trial 40 with value: 1.6789074671088653.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 01:22:35,714]\u001b[0m Trial 49 finished with value: 1.830194303388333 and parameters: {'lr': 0.08151960920561556, 'hs': 17, 'dropout': 0.06600329391564924}. Best is trial 40 with value: 1.6789074671088653.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 01:22:40,122]\u001b[0m Trial 50 finished with value: 1.9333628248051442 and parameters: {'lr': 0.08749769102906968, 'hs': 14, 'dropout': 0.09927943860286886}. Best is trial 40 with value: 1.6789074671088653.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 01:22:45,097]\u001b[0m Trial 51 finished with value: 1.7532429213702425 and parameters: {'lr': 0.07154743218654265, 'hs': 31, 'dropout': 0.1118915507294267}. Best is trial 40 with value: 1.6789074671088653.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 01:22:49,960]\u001b[0m Trial 52 finished with value: 1.7221964789213535 and parameters: {'lr': 0.0712363077902643, 'hs': 31, 'dropout': 0.0947461200451129}. Best is trial 40 with value: 1.6789074671088653.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 01:22:55,501]\u001b[0m Trial 53 finished with value: 1.7144288213179355 and parameters: {'lr': 0.06938231169269866, 'hs': 31, 'dropout': 0.09560972637779297}. Best is trial 40 with value: 1.6789074671088653.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 01:22:58,808]\u001b[0m Trial 54 finished with value: 2.018039859188249 and parameters: {'lr': 0.07521238992223285, 'hs': 10, 'dropout': 0.09434503493984112}. Best is trial 40 with value: 1.6789074671088653.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 01:23:03,169]\u001b[0m Trial 55 finished with value: 1.7963730696645124 and parameters: {'lr': 0.06414461755030144, 'hs': 32, 'dropout': 0.0847165253150908}. Best is trial 40 with value: 1.6789074671088653.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 01:23:07,151]\u001b[0m Trial 56 finished with value: 1.7188455044891668 and parameters: {'lr': 0.09377366895201451, 'hs': 29, 'dropout': 0.09615312473597248}. Best is trial 40 with value: 1.6789074671088653.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 01:23:12,051]\u001b[0m Trial 57 finished with value: 1.672113594514655 and parameters: {'lr': 0.09368138906977355, 'hs': 28, 'dropout': 0.10529096738414716}. Best is trial 57 with value: 1.672113594514655.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 01:23:16,498]\u001b[0m Trial 58 finished with value: 1.9060543025829517 and parameters: {'lr': 0.09459939510362907, 'hs': 29, 'dropout': 0.10351000344815497}. Best is trial 57 with value: 1.672113594514655.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 01:23:20,913]\u001b[0m Trial 59 finished with value: 1.8718672751775471 and parameters: {'lr': 0.0957447839771255, 'hs': 31, 'dropout': 0.10346381192853937}. Best is trial 57 with value: 1.672113594514655.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 01:23:26,548]\u001b[0m Trial 60 finished with value: 1.7440584093848017 and parameters: {'lr': 0.0914533318183153, 'hs': 28, 'dropout': 0.09617471551727769}. Best is trial 57 with value: 1.672113594514655.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 01:23:31,820]\u001b[0m Trial 61 finished with value: 1.777588200676498 and parameters: {'lr': 0.09870831082683153, 'hs': 27, 'dropout': 0.12713609320458652}. Best is trial 57 with value: 1.672113594514655.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 01:23:37,089]\u001b[0m Trial 62 finished with value: 1.725148404197595 and parameters: {'lr': 0.08079260357021126, 'hs': 29, 'dropout': 0.10748079105764012}. Best is trial 57 with value: 1.672113594514655.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 01:23:42,382]\u001b[0m Trial 63 finished with value: 1.8081948277542566 and parameters: {'lr': 0.08170985341953106, 'hs': 29, 'dropout': 0.10715391294474859}. Best is trial 57 with value: 1.672113594514655.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 01:23:45,864]\u001b[0m Trial 64 finished with value: 2.0502653526459977 and parameters: {'lr': 0.08877771525348918, 'hs': 9, 'dropout': 0.11475330016020664}. Best is trial 57 with value: 1.672113594514655.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 01:23:50,405]\u001b[0m Trial 65 finished with value: 1.9603504068307949 and parameters: {'lr': 0.092827623265637, 'hs': 25, 'dropout': 0.11773188698790843}. Best is trial 57 with value: 1.672113594514655.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 01:23:55,625]\u001b[0m Trial 66 finished with value: 1.8269430242933327 and parameters: {'lr': 0.09690502615712729, 'hs': 31, 'dropout': 0.13255238844159423}. Best is trial 57 with value: 1.672113594514655.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 01:24:00,843]\u001b[0m Trial 67 finished with value: 1.815130091504135 and parameters: {'lr': 0.07832925184724816, 'hs': 32, 'dropout': 0.09481180174342788}. Best is trial 57 with value: 1.672113594514655.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 01:24:04,323]\u001b[0m Trial 68 finished with value: 1.8680159496279376 and parameters: {'lr': 0.0834093796911641, 'hs': 30, 'dropout': 0.10040967345428689}. Best is trial 57 with value: 1.672113594514655.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 01:24:08,763]\u001b[0m Trial 69 finished with value: 1.9270321385986715 and parameters: {'lr': 0.06916875353336767, 'hs': 28, 'dropout': 0.12291142540638336}. Best is trial 57 with value: 1.672113594514655.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 01:24:14,289]\u001b[0m Trial 70 finished with value: 1.6948854490910683 and parameters: {'lr': 0.08682433760887191, 'hs': 25, 'dropout': 0.10683287215607849}. Best is trial 57 with value: 1.672113594514655.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 01:24:18,602]\u001b[0m Trial 71 finished with value: 1.6781118776748682 and parameters: {'lr': 0.0880342321117388, 'hs': 25, 'dropout': 0.10655354065215038}. Best is trial 57 with value: 1.672113594514655.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 01:24:23,676]\u001b[0m Trial 72 finished with value: 1.7485739421771704 and parameters: {'lr': 0.08826063777507229, 'hs': 26, 'dropout': 0.0892378585727522}. Best is trial 57 with value: 1.672113594514655.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 01:24:28,197]\u001b[0m Trial 73 finished with value: 1.8041436950080485 and parameters: {'lr': 0.08618258223137373, 'hs': 31, 'dropout': 0.11732745046969914}. Best is trial 57 with value: 1.672113594514655.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 01:24:33,546]\u001b[0m Trial 74 finished with value: 1.6786294129599233 and parameters: {'lr': 0.09225393349987333, 'hs': 25, 'dropout': 0.07906459260284243}. Best is trial 57 with value: 1.672113594514655.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 01:24:38,078]\u001b[0m Trial 75 finished with value: 1.6819471891048625 and parameters: {'lr': 0.09669926773618912, 'hs': 24, 'dropout': 0.1019191022502953}. Best is trial 57 with value: 1.672113594514655.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 01:24:41,832]\u001b[0m Trial 76 finished with value: 1.745268379293937 and parameters: {'lr': 0.09243297921342569, 'hs': 25, 'dropout': 0.1046889954840375}. Best is trial 57 with value: 1.672113594514655.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 01:24:45,906]\u001b[0m Trial 77 finished with value: 1.8293389708204906 and parameters: {'lr': 0.09751541360767248, 'hs': 23, 'dropout': 0.07913145072393897}. Best is trial 57 with value: 1.672113594514655.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 01:24:50,030]\u001b[0m Trial 78 finished with value: 1.8181044170869862 and parameters: {'lr': 0.08825375094729648, 'hs': 24, 'dropout': 0.09923456213506444}. Best is trial 57 with value: 1.672113594514655.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 01:24:54,326]\u001b[0m Trial 79 finished with value: 1.803697135126049 and parameters: {'lr': 0.09059053737316931, 'hs': 25, 'dropout': 0.07859842433733821}. Best is trial 57 with value: 1.672113594514655.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 01:24:58,875]\u001b[0m Trial 80 finished with value: 1.7338147217970636 and parameters: {'lr': 0.0851305806286118, 'hs': 23, 'dropout': 0.13337895048568013}. Best is trial 57 with value: 1.672113594514655.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 01:25:03,189]\u001b[0m Trial 81 finished with value: 1.7934337333110715 and parameters: {'lr': 0.09395957545690202, 'hs': 27, 'dropout': 0.08828564809865524}. Best is trial 57 with value: 1.672113594514655.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 01:25:07,334]\u001b[0m Trial 82 finished with value: 1.7765196704487334 and parameters: {'lr': 0.09962872914211879, 'hs': 26, 'dropout': 0.0919676430917247}. Best is trial 57 with value: 1.672113594514655.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 01:25:12,261]\u001b[0m Trial 83 finished with value: 1.704717050460701 and parameters: {'lr': 0.09620441903933051, 'hs': 22, 'dropout': 0.07045229326536054}. Best is trial 57 with value: 1.672113594514655.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 01:25:16,332]\u001b[0m Trial 84 finished with value: 1.824975107012136 and parameters: {'lr': 0.0903903454720766, 'hs': 22, 'dropout': 0.07518342365111177}. Best is trial 57 with value: 1.672113594514655.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 01:25:20,982]\u001b[0m Trial 85 finished with value: 1.6800602194039842 and parameters: {'lr': 0.09614375823389593, 'hs': 24, 'dropout': 0.10789046907912742}. Best is trial 57 with value: 1.672113594514655.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 01:25:24,789]\u001b[0m Trial 86 finished with value: 1.8128790105047112 and parameters: {'lr': 0.09683285743896256, 'hs': 19, 'dropout': 0.07071859416902142}. Best is trial 57 with value: 1.672113594514655.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 01:25:29,427]\u001b[0m Trial 87 finished with value: 1.7920628365717353 and parameters: {'lr': 0.08464349556294073, 'hs': 24, 'dropout': 0.10996663638289361}. Best is trial 57 with value: 1.672113594514655.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 01:25:32,411]\u001b[0m Trial 88 finished with value: 1.9900060204726637 and parameters: {'lr': 0.09972638251753319, 'hs': 20, 'dropout': 0.11987756835485644}. Best is trial 57 with value: 1.672113594514655.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 01:25:36,228]\u001b[0m Trial 89 finished with value: 1.8398647220541857 and parameters: {'lr': 0.07718804253608949, 'hs': 23, 'dropout': 0.10143869162324372}. Best is trial 57 with value: 1.672113594514655.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 01:25:41,190]\u001b[0m Trial 90 finished with value: 1.954621465052001 and parameters: {'lr': 0.02747778977540554, 'hs': 22, 'dropout': 0.06558445390021193}. Best is trial 57 with value: 1.672113594514655.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 01:25:45,811]\u001b[0m Trial 91 finished with value: 1.7612008459730337 and parameters: {'lr': 0.09319368508243674, 'hs': 24, 'dropout': 0.11463048056893996}. Best is trial 57 with value: 1.672113594514655.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 01:25:50,374]\u001b[0m Trial 92 finished with value: 1.6860510008233547 and parameters: {'lr': 0.09507460262591387, 'hs': 25, 'dropout': 0.10867051314866456}. Best is trial 57 with value: 1.672113594514655.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 01:25:54,052]\u001b[0m Trial 93 finished with value: 1.8281113512533451 and parameters: {'lr': 0.08704661333724475, 'hs': 25, 'dropout': 0.10699549365746729}. Best is trial 57 with value: 1.672113594514655.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 01:25:57,921]\u001b[0m Trial 94 finished with value: 1.8689080419600235 and parameters: {'lr': 0.09663409224173276, 'hs': 21, 'dropout': 0.1248597419599157}. Best is trial 57 with value: 1.672113594514655.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 01:26:02,913]\u001b[0m Trial 95 finished with value: 1.8118247357409258 and parameters: {'lr': 0.08999049388343061, 'hs': 26, 'dropout': 0.11037203994744714}. Best is trial 57 with value: 1.672113594514655.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 01:26:06,960]\u001b[0m Trial 96 finished with value: 1.7224763509498981 and parameters: {'lr': 0.09536094735537579, 'hs': 23, 'dropout': 0.11594888257543418}. Best is trial 57 with value: 1.672113594514655.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 01:26:11,348]\u001b[0m Trial 97 finished with value: 1.7296238642320902 and parameters: {'lr': 0.09162350658905967, 'hs': 20, 'dropout': 0.06311975617726517}. Best is trial 57 with value: 1.672113594514655.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 01:26:14,950]\u001b[0m Trial 98 finished with value: 1.8002819312936016 and parameters: {'lr': 0.0800819048076332, 'hs': 24, 'dropout': 0.10552162852914586}. Best is trial 57 with value: 1.672113594514655.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 01:26:19,772]\u001b[0m Trial 99 finished with value: 1.7630638412529853 and parameters: {'lr': 0.0881908271867076, 'hs': 27, 'dropout': 0.10294791742004061}. Best is trial 57 with value: 1.672113594514655.\u001b[0m\n",
      "('GRU', 2.6516713465689398e+17, 190820676.33385405, 2.4011392663061617, 0.5149234291422827, 0.6122820943505991, 'FF', True, 'mse')\n",
      "('GRU', 1.956731892195306e+17, 157833930.59086215, 1.2963030336034822, 0.5070712295256004, 0.6446763162035287, 'FF', True, 'mse')\n",
      "('GRU', 2.864044218686931e+17, 220359576.879548, 4.385213249836653, 0.5419096656606477, 0.6550897312495593, 'FF', True, 'mse')\n",
      "('GRU', 3.567486710030637e+17, 205325268.1998308, 3.639151092257158, 0.43852777749105754, 0.5280900252404634, 'FF', True, 'mse')\n",
      "('GRU', 1.615905991775936e+17, 166075151.46361774, 3.0122090345046963, 0.44396301460177023, 0.7108782579039232, 'FF', True, 'mse')\n",
      "\u001b[32m[I 2025-11-01 01:28:05,919]\u001b[0m A new study created in memory with name: no-name-b8b0a6e9-ede2-4cbe-b264-446a68c41793\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 01:28:11,368]\u001b[0m Trial 0 finished with value: 0.002498293957413532 and parameters: {'lr': 0.017768083107518796, 'hs': 12, 'dropout': 0.12938898765996992}. Best is trial 0 with value: 0.002498293957413532.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 01:28:14,680]\u001b[0m Trial 1 finished with value: 0.0025677027477308267 and parameters: {'lr': 0.01855513066646369, 'hs': 15, 'dropout': 0.055645316554994516}. Best is trial 0 with value: 0.002498293957413532.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 01:28:18,469]\u001b[0m Trial 2 finished with value: 0.003167383015220075 and parameters: {'lr': 0.05449316718897172, 'hs': 4, 'dropout': 0.05566922775165559}. Best is trial 0 with value: 0.002498293957413532.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 01:28:23,481]\u001b[0m Trial 3 finished with value: 0.0025882549625427147 and parameters: {'lr': 0.06296225955889884, 'hs': 25, 'dropout': 0.10905113461563715}. Best is trial 0 with value: 0.002498293957413532.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 01:28:27,991]\u001b[0m Trial 4 finished with value: 0.002512680552129582 and parameters: {'lr': 0.08401989304084306, 'hs': 26, 'dropout': 0.08008214363827429}. Best is trial 0 with value: 0.002498293957413532.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 01:28:33,354]\u001b[0m Trial 5 finished with value: 0.0024356922765870616 and parameters: {'lr': 0.07672390229808546, 'hs': 27, 'dropout': 0.1941600613413571}. Best is trial 5 with value: 0.0024356922765870616.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 01:28:36,714]\u001b[0m Trial 6 finished with value: 0.0039817181484045 and parameters: {'lr': 0.053392754381107466, 'hs': 9, 'dropout': 0.1684144926925516}. Best is trial 5 with value: 0.0024356922765870616.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 01:28:41,731]\u001b[0m Trial 7 finished with value: 0.002579986882740763 and parameters: {'lr': 0.014123025110666858, 'hs': 20, 'dropout': 0.15313999604182105}. Best is trial 5 with value: 0.0024356922765870616.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 01:28:45,615]\u001b[0m Trial 8 finished with value: 0.0024608570980556577 and parameters: {'lr': 0.049672514077289766, 'hs': 14, 'dropout': 0.05339723161331042}. Best is trial 5 with value: 0.0024356922765870616.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 01:28:50,263]\u001b[0m Trial 9 finished with value: 0.0028802530284636114 and parameters: {'lr': 0.015442996547483177, 'hs': 23, 'dropout': 0.18928005064059567}. Best is trial 5 with value: 0.0024356922765870616.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 01:28:53,986]\u001b[0m Trial 10 finished with value: 0.0024885227317979733 and parameters: {'lr': 0.09373567676890933, 'hs': 32, 'dropout': 0.19474592189869197}. Best is trial 5 with value: 0.0024356922765870616.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 01:28:58,638]\u001b[0m Trial 11 finished with value: 0.0025593791099032715 and parameters: {'lr': 0.07391476958755351, 'hs': 30, 'dropout': 0.10229566509059607}. Best is trial 5 with value: 0.0024356922765870616.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 01:29:03,198]\u001b[0m Trial 12 finished with value: 0.0025562521164853194 and parameters: {'lr': 0.03562148413638185, 'hs': 18, 'dropout': 0.14049424945123998}. Best is trial 5 with value: 0.0024356922765870616.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 01:29:06,405]\u001b[0m Trial 13 finished with value: 0.0027498039540187584 and parameters: {'lr': 0.03893670370543971, 'hs': 8, 'dropout': 0.08583311991690495}. Best is trial 5 with value: 0.0024356922765870616.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 01:29:10,653]\u001b[0m Trial 14 finished with value: 0.0025880565352805677 and parameters: {'lr': 0.07273297179787429, 'hs': 16, 'dropout': 0.1538520838890958}. Best is trial 5 with value: 0.0024356922765870616.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 01:29:14,834]\u001b[0m Trial 15 finished with value: 0.0026120258296362373 and parameters: {'lr': 0.09923161085766778, 'hs': 21, 'dropout': 0.1112783773386219}. Best is trial 5 with value: 0.0024356922765870616.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 01:29:19,809]\u001b[0m Trial 16 finished with value: 0.0027336264189584854 and parameters: {'lr': 0.037961102340276796, 'hs': 28, 'dropout': 0.1767269167819757}. Best is trial 5 with value: 0.0024356922765870616.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 01:29:23,204]\u001b[0m Trial 17 finished with value: 0.004433908621986018 and parameters: {'lr': 0.06843125792849944, 'hs': 2, 'dropout': 0.07864405403592428}. Best is trial 5 with value: 0.0024356922765870616.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 01:29:27,169]\u001b[0m Trial 18 finished with value: 0.002561294502696229 and parameters: {'lr': 0.08491242389103913, 'hs': 13, 'dropout': 0.12714925665391194}. Best is trial 5 with value: 0.0024356922765870616.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 01:29:30,902]\u001b[0m Trial 19 finished with value: 0.002804922506583644 and parameters: {'lr': 0.04635172576595166, 'hs': 8, 'dropout': 0.09596775992893533}. Best is trial 5 with value: 0.0024356922765870616.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 01:29:34,964]\u001b[0m Trial 20 finished with value: 0.0021303455691524882 and parameters: {'lr': 0.05910024503781909, 'hs': 18, 'dropout': 0.06866943801169638}. Best is trial 20 with value: 0.0021303455691524882.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 01:29:40,175]\u001b[0m Trial 21 finished with value: 0.0023367871259716475 and parameters: {'lr': 0.057817573367783044, 'hs': 19, 'dropout': 0.06855614644164079}. Best is trial 20 with value: 0.0021303455691524882.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 01:29:45,019]\u001b[0m Trial 22 finished with value: 0.002439463436797374 and parameters: {'lr': 0.06101375725148798, 'hs': 20, 'dropout': 0.07159529887054561}. Best is trial 20 with value: 0.0021303455691524882.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 01:30:12,256]\u001b[0m Trial 23 finished with value: 0.004449359847044711 and parameters: {'lr': 0.00020949083561287024, 'hs': 24, 'dropout': 0.06784479635514945}. Best is trial 20 with value: 0.0021303455691524882.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 01:30:16,747]\u001b[0m Trial 24 finished with value: 0.0022490918288743515 and parameters: {'lr': 0.08247968897277506, 'hs': 28, 'dropout': 0.09226656080735487}. Best is trial 20 with value: 0.0021303455691524882.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 01:30:21,746]\u001b[0m Trial 25 finished with value: 0.0022204224640468063 and parameters: {'lr': 0.08288903634075581, 'hs': 18, 'dropout': 0.09194880086565554}. Best is trial 20 with value: 0.0021303455691524882.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 01:30:27,469]\u001b[0m Trial 26 finished with value: 0.002482739378484621 and parameters: {'lr': 0.08571339779923531, 'hs': 23, 'dropout': 0.0914624983792667}. Best is trial 20 with value: 0.0021303455691524882.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 01:30:31,762]\u001b[0m Trial 27 finished with value: 0.002621487329989668 and parameters: {'lr': 0.09040126644424494, 'hs': 11, 'dropout': 0.11231285974624258}. Best is trial 20 with value: 0.0021303455691524882.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 01:30:35,574]\u001b[0m Trial 28 finished with value: 0.002507001522641951 and parameters: {'lr': 0.0790442356078789, 'hs': 17, 'dropout': 0.0945933990001739}. Best is trial 20 with value: 0.0021303455691524882.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 01:30:39,980]\u001b[0m Trial 29 finished with value: 0.0024266656622590763 and parameters: {'lr': 0.06844857767251794, 'hs': 30, 'dropout': 0.11463094044742307}. Best is trial 20 with value: 0.0021303455691524882.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 01:30:44,347]\u001b[0m Trial 30 finished with value: 0.0024805177716270524 and parameters: {'lr': 0.09863816333306728, 'hs': 22, 'dropout': 0.12389509581038922}. Best is trial 20 with value: 0.0021303455691524882.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 01:30:48,074]\u001b[0m Trial 31 finished with value: 0.0027074066774427402 and parameters: {'lr': 0.060510028277447656, 'hs': 19, 'dropout': 0.06866642295154649}. Best is trial 20 with value: 0.0021303455691524882.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 01:30:52,590]\u001b[0m Trial 32 finished with value: 0.0025764498505009992 and parameters: {'lr': 0.0661386403215953, 'hs': 16, 'dropout': 0.0689311162064536}. Best is trial 20 with value: 0.0021303455691524882.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 01:30:57,579]\u001b[0m Trial 33 finished with value: 0.0026389786051475954 and parameters: {'lr': 0.04471696505469598, 'hs': 18, 'dropout': 0.05961474221580719}. Best is trial 20 with value: 0.0021303455691524882.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 01:31:01,339]\u001b[0m Trial 34 finished with value: 0.002739108972221358 and parameters: {'lr': 0.05614972086886725, 'hs': 11, 'dropout': 0.08029216108417761}. Best is trial 20 with value: 0.0021303455691524882.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 01:31:05,348]\u001b[0m Trial 35 finished with value: 0.002353517199018759 and parameters: {'lr': 0.03204984693993186, 'hs': 15, 'dropout': 0.05083594402096302}. Best is trial 20 with value: 0.0021303455691524882.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 01:31:10,420]\u001b[0m Trial 36 finished with value: 0.0022413194268285975 and parameters: {'lr': 0.0797898349409868, 'hs': 25, 'dropout': 0.06298410510015408}. Best is trial 20 with value: 0.0021303455691524882.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 01:31:14,020]\u001b[0m Trial 37 finished with value: 0.002565935545537244 and parameters: {'lr': 0.0802023926216353, 'hs': 26, 'dropout': 0.06189981378914355}. Best is trial 20 with value: 0.0021303455691524882.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 01:31:18,502]\u001b[0m Trial 38 finished with value: 0.002337986237813819 and parameters: {'lr': 0.09037914955176689, 'hs': 28, 'dropout': 0.10167952971486063}. Best is trial 20 with value: 0.0021303455691524882.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 01:31:21,963]\u001b[0m Trial 39 finished with value: 0.0024322442219146243 and parameters: {'lr': 0.08189883789119862, 'hs': 25, 'dropout': 0.07952079696761638}. Best is trial 20 with value: 0.0021303455691524882.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 01:31:26,946]\u001b[0m Trial 40 finished with value: 0.0022537824715811836 and parameters: {'lr': 0.07382125368647884, 'hs': 32, 'dropout': 0.08764555134931017}. Best is trial 20 with value: 0.0021303455691524882.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 01:31:32,271]\u001b[0m Trial 41 finished with value: 0.002447174193798296 and parameters: {'lr': 0.07293676156816653, 'hs': 32, 'dropout': 0.08196109982827016}. Best is trial 20 with value: 0.0021303455691524882.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 01:31:37,637]\u001b[0m Trial 42 finished with value: 0.0023171548359383035 and parameters: {'lr': 0.07571555095020148, 'hs': 30, 'dropout': 0.08758155059621081}. Best is trial 20 with value: 0.0021303455691524882.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 01:31:42,172]\u001b[0m Trial 43 finished with value: 0.00213598476103441 and parameters: {'lr': 0.08868654559025874, 'hs': 28, 'dropout': 0.1014497977119205}. Best is trial 20 with value: 0.0021303455691524882.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 01:31:46,856]\u001b[0m Trial 44 finished with value: 0.002548377177128963 and parameters: {'lr': 0.09059956769796598, 'hs': 28, 'dropout': 0.1008752453357887}. Best is trial 20 with value: 0.0021303455691524882.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 01:31:50,605]\u001b[0m Trial 45 finished with value: 0.0024359920929820007 and parameters: {'lr': 0.08603467410630275, 'hs': 26, 'dropout': 0.059297125301637055}. Best is trial 20 with value: 0.0021303455691524882.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 01:31:55,551]\u001b[0m Trial 46 finished with value: 0.00240067471800711 and parameters: {'lr': 0.09439205771179117, 'hs': 24, 'dropout': 0.1189932583881706}. Best is trial 20 with value: 0.0021303455691524882.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 01:32:00,954]\u001b[0m Trial 47 finished with value: 0.002591528778799652 and parameters: {'lr': 0.09493370750592904, 'hs': 29, 'dropout': 0.10742209326844553}. Best is trial 20 with value: 0.0021303455691524882.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 01:32:05,012]\u001b[0m Trial 48 finished with value: 0.0027124144020972806 and parameters: {'lr': 0.08791624606895555, 'hs': 22, 'dropout': 0.13353347662205284}. Best is trial 20 with value: 0.0021303455691524882.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 01:32:10,973]\u001b[0m Trial 49 finished with value: 0.00238755679587904 and parameters: {'lr': 0.026895781669440275, 'hs': 25, 'dropout': 0.0754800754395817}. Best is trial 20 with value: 0.0021303455691524882.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 01:32:15,624]\u001b[0m Trial 50 finished with value: 0.0023754510394539056 and parameters: {'lr': 0.0659006684726977, 'hs': 27, 'dropout': 0.09614105007592366}. Best is trial 20 with value: 0.0021303455691524882.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 01:32:21,042]\u001b[0m Trial 51 finished with value: 0.002313699225960694 and parameters: {'lr': 0.07665059254308766, 'hs': 32, 'dropout': 0.08647557143933822}. Best is trial 20 with value: 0.0021303455691524882.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 01:32:26,676]\u001b[0m Trial 52 finished with value: 0.0025268542138162093 and parameters: {'lr': 0.07070137686928207, 'hs': 30, 'dropout': 0.0898065556689937}. Best is trial 20 with value: 0.0021303455691524882.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 01:32:32,190]\u001b[0m Trial 53 finished with value: 0.0024602358853516343 and parameters: {'lr': 0.08112720447621018, 'hs': 31, 'dropout': 0.10576803527407931}. Best is trial 20 with value: 0.0021303455691524882.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 01:32:37,029]\u001b[0m Trial 54 finished with value: 0.0022525783760421205 and parameters: {'lr': 0.07720734985474367, 'hs': 29, 'dropout': 0.06430667299944512}. Best is trial 20 with value: 0.0021303455691524882.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 01:32:41,616]\u001b[0m Trial 55 finished with value: 0.0023442226956323664 and parameters: {'lr': 0.08296264066048137, 'hs': 27, 'dropout': 0.0629682283945068}. Best is trial 20 with value: 0.0021303455691524882.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 01:32:46,519]\u001b[0m Trial 56 finished with value: 0.0025325495821238403 and parameters: {'lr': 0.05059139886525399, 'hs': 29, 'dropout': 0.05625683986868317}. Best is trial 20 with value: 0.0021303455691524882.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 01:32:49,983]\u001b[0m Trial 57 finished with value: 0.002904557763351384 and parameters: {'lr': 0.07749807116879477, 'hs': 6, 'dropout': 0.07547703445922337}. Best is trial 20 with value: 0.0021303455691524882.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 01:32:54,452]\u001b[0m Trial 58 finished with value: 0.002460752856164558 and parameters: {'lr': 0.06345500093136691, 'hs': 21, 'dropout': 0.05142003747325556}. Best is trial 20 with value: 0.0021303455691524882.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 01:32:58,281]\u001b[0m Trial 59 finished with value: 0.0026515804863537465 and parameters: {'lr': 0.09795026541316058, 'hs': 13, 'dropout': 0.07294200372576752}. Best is trial 20 with value: 0.0021303455691524882.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 01:33:03,468]\u001b[0m Trial 60 finished with value: 0.0024948488849543204 and parameters: {'lr': 0.08813591195755763, 'hs': 24, 'dropout': 0.06379334818124276}. Best is trial 20 with value: 0.0021303455691524882.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 01:33:07,823]\u001b[0m Trial 61 finished with value: 0.0023291117346703853 and parameters: {'lr': 0.07391759968667558, 'hs': 31, 'dropout': 0.08529446435420436}. Best is trial 20 with value: 0.0021303455691524882.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 01:33:12,630]\u001b[0m Trial 62 finished with value: 0.002636696079753703 and parameters: {'lr': 0.06920238978741768, 'hs': 29, 'dropout': 0.09639689264285983}. Best is trial 20 with value: 0.0021303455691524882.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 01:33:17,137]\u001b[0m Trial 63 finished with value: 0.0024943002539439684 and parameters: {'lr': 0.08442830223761849, 'hs': 31, 'dropout': 0.0830531079398927}. Best is trial 20 with value: 0.0021303455691524882.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 01:33:21,939]\u001b[0m Trial 64 finished with value: 0.0021905833256337386 and parameters: {'lr': 0.07862151183254194, 'hs': 27, 'dropout': 0.09190242126485897}. Best is trial 20 with value: 0.0021303455691524882.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 01:33:27,565]\u001b[0m Trial 65 finished with value: 0.0023040309284044656 and parameters: {'lr': 0.09269262700758078, 'hs': 26, 'dropout': 0.07635590325953884}. Best is trial 20 with value: 0.0021303455691524882.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 01:33:32,318]\u001b[0m Trial 66 finished with value: 0.002350986191209926 and parameters: {'lr': 0.08169069671912232, 'hs': 28, 'dropout': 0.1471321625763284}. Best is trial 20 with value: 0.0021303455691524882.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 01:33:36,415]\u001b[0m Trial 67 finished with value: 0.0025093952522684012 and parameters: {'lr': 0.08620847512776075, 'hs': 17, 'dropout': 0.09311549672801886}. Best is trial 20 with value: 0.0021303455691524882.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 01:33:41,127]\u001b[0m Trial 68 finished with value: 0.0025391474525998073 and parameters: {'lr': 0.07868571429124008, 'hs': 23, 'dropout': 0.05594001581062894}. Best is trial 20 with value: 0.0021303455691524882.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 01:33:46,122]\u001b[0m Trial 69 finished with value: 0.002689759417765336 and parameters: {'lr': 0.07020405650555077, 'hs': 20, 'dropout': 0.0647150399162945}. Best is trial 20 with value: 0.0021303455691524882.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 01:33:50,026]\u001b[0m Trial 70 finished with value: 0.0025124166787554845 and parameters: {'lr': 0.06448394590539253, 'hs': 27, 'dropout': 0.1161638280405408}. Best is trial 20 with value: 0.0021303455691524882.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 01:33:54,614]\u001b[0m Trial 71 finished with value: 0.0022711217182137026 and parameters: {'lr': 0.07308320848136697, 'hs': 29, 'dropout': 0.09850470943687037}. Best is trial 20 with value: 0.0021303455691524882.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 01:33:59,295]\u001b[0m Trial 72 finished with value: 0.002346668106117228 and parameters: {'lr': 0.07585770550208733, 'hs': 32, 'dropout': 0.10557192214669302}. Best is trial 20 with value: 0.0021303455691524882.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 01:34:03,498]\u001b[0m Trial 73 finished with value: 0.00231115313145236 and parameters: {'lr': 0.07985478034472782, 'hs': 31, 'dropout': 0.09106292794673618}. Best is trial 20 with value: 0.0021303455691524882.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 01:34:07,832]\u001b[0m Trial 74 finished with value: 0.002411360292089867 and parameters: {'lr': 0.06060237190712406, 'hs': 25, 'dropout': 0.0716610659743261}. Best is trial 20 with value: 0.0021303455691524882.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 01:34:12,701]\u001b[0m Trial 75 finished with value: 0.002570368977442877 and parameters: {'lr': 0.08912430027153714, 'hs': 15, 'dropout': 0.08153054070365102}. Best is trial 20 with value: 0.0021303455691524882.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 01:34:16,497]\u001b[0m Trial 76 finished with value: 0.0024723861184632527 and parameters: {'lr': 0.08412404285839463, 'hs': 28, 'dropout': 0.0683309313482128}. Best is trial 20 with value: 0.0021303455691524882.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 01:34:20,345]\u001b[0m Trial 77 finished with value: 0.002621046092853826 and parameters: {'lr': 0.0750245886777461, 'hs': 18, 'dropout': 0.08927464492585055}. Best is trial 20 with value: 0.0021303455691524882.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 01:34:24,684]\u001b[0m Trial 78 finished with value: 0.002385813437112503 and parameters: {'lr': 0.09638722318187345, 'hs': 26, 'dropout': 0.16441807503936176}. Best is trial 20 with value: 0.0021303455691524882.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 01:34:31,143]\u001b[0m Trial 79 finished with value: 0.0023869506096541324 and parameters: {'lr': 0.09236720197181747, 'hs': 30, 'dropout': 0.11086261950503198}. Best is trial 20 with value: 0.0021303455691524882.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 01:34:35,013]\u001b[0m Trial 80 finished with value: 0.002214976856974925 and parameters: {'lr': 0.07793064522847491, 'hs': 27, 'dropout': 0.07803576488741495}. Best is trial 20 with value: 0.0021303455691524882.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 01:34:40,057]\u001b[0m Trial 81 finished with value: 0.0025108428113895607 and parameters: {'lr': 0.07824074077029981, 'hs': 27, 'dropout': 0.07715112713040295}. Best is trial 20 with value: 0.0021303455691524882.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 01:34:44,032]\u001b[0m Trial 82 finished with value: 0.0021748733193289464 and parameters: {'lr': 0.08662666036238574, 'hs': 29, 'dropout': 0.1011305035134453}. Best is trial 20 with value: 0.0021303455691524882.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 01:34:49,069]\u001b[0m Trial 83 finished with value: 0.002475131371130214 and parameters: {'lr': 0.08691948685509951, 'hs': 29, 'dropout': 0.10064495046459958}. Best is trial 20 with value: 0.0021303455691524882.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 01:34:53,228]\u001b[0m Trial 84 finished with value: 0.0025188785924173383 and parameters: {'lr': 0.08376710165752688, 'hs': 25, 'dropout': 0.06589099581676128}. Best is trial 20 with value: 0.0021303455691524882.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 01:34:57,310]\u001b[0m Trial 85 finished with value: 0.002583223093223373 and parameters: {'lr': 0.0912569564788574, 'hs': 26, 'dropout': 0.1217495306029994}. Best is trial 20 with value: 0.0021303455691524882.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 01:35:03,460]\u001b[0m Trial 86 finished with value: 0.002373301548320406 and parameters: {'lr': 0.009591579991536744, 'hs': 28, 'dropout': 0.10369896229374986}. Best is trial 20 with value: 0.0021303455691524882.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 01:35:08,154]\u001b[0m Trial 87 finished with value: 0.0023768918620496655 and parameters: {'lr': 0.08154359859953392, 'hs': 27, 'dropout': 0.05839425330994905}. Best is trial 20 with value: 0.0021303455691524882.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 01:35:12,482]\u001b[0m Trial 88 finished with value: 0.002674410597102546 and parameters: {'lr': 0.08853451439566598, 'hs': 19, 'dropout': 0.08468907182230251}. Best is trial 20 with value: 0.0021303455691524882.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 01:35:18,694]\u001b[0m Trial 89 finished with value: 0.002455752963291247 and parameters: {'lr': 0.04096194823388894, 'hs': 30, 'dropout': 0.07103469762484521}. Best is trial 20 with value: 0.0021303455691524882.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 01:35:22,224]\u001b[0m Trial 90 finished with value: 0.002544587545627714 and parameters: {'lr': 0.05810811274386703, 'hs': 16, 'dropout': 0.1298138672852781}. Best is trial 20 with value: 0.0021303455691524882.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 01:35:26,545]\u001b[0m Trial 91 finished with value: 0.0023389880021728207 and parameters: {'lr': 0.07934110747887647, 'hs': 24, 'dropout': 0.19980614999873017}. Best is trial 20 with value: 0.0021303455691524882.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 01:35:30,792]\u001b[0m Trial 92 finished with value: 0.0023777832574817187 and parameters: {'lr': 0.06722570409770197, 'hs': 29, 'dropout': 0.09784784248742683}. Best is trial 20 with value: 0.0021303455691524882.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 01:35:36,349]\u001b[0m Trial 93 finished with value: 0.0025343952350301946 and parameters: {'lr': 0.07130328962435747, 'hs': 30, 'dropout': 0.0914539429128815}. Best is trial 20 with value: 0.0021303455691524882.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 01:35:41,676]\u001b[0m Trial 94 finished with value: 0.0023310515355427753 and parameters: {'lr': 0.07719160367618341, 'hs': 31, 'dropout': 0.08770045410964683}. Best is trial 20 with value: 0.0021303455691524882.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 01:35:46,770]\u001b[0m Trial 95 finished with value: 0.0025363124377875924 and parameters: {'lr': 0.08293296634195488, 'hs': 28, 'dropout': 0.07940635267488813}. Best is trial 20 with value: 0.0021303455691524882.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 01:35:52,135]\u001b[0m Trial 96 finished with value: 0.0022612549295968364 and parameters: {'lr': 0.08600136497275795, 'hs': 32, 'dropout': 0.07297691673950694}. Best is trial 20 with value: 0.0021303455691524882.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 01:35:56,456]\u001b[0m Trial 97 finished with value: 0.0023430925416271235 and parameters: {'lr': 0.07226695056681354, 'hs': 23, 'dropout': 0.10914162885379147}. Best is trial 20 with value: 0.0021303455691524882.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 01:36:01,327]\u001b[0m Trial 98 finished with value: 0.002338743605853268 and parameters: {'lr': 0.07427662324055724, 'hs': 22, 'dropout': 0.08283222285514798}. Best is trial 20 with value: 0.0021303455691524882.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 01:36:06,249]\u001b[0m Trial 99 finished with value: 0.0025723415369821217 and parameters: {'lr': 0.0510417046246226, 'hs': 28, 'dropout': 0.09531377961576122}. Best is trial 20 with value: 0.0021303455691524882.\u001b[0m\n",
      "('GRU', 1.8225591888759408e+17, 158217701.65625393, 4.41771011657476, 0.33320735664509904, 0.7464361874273773, 'FF', True, 'bce')\n",
      "('GRU', 1.9474267068000202e+17, 170840552.36943337, 12.848762972819083, 0.31800567248432365, 0.7518472543009573, 'FF', True, 'bce')\n",
      "('GRU', 9.544665829231213e+16, 129756828.8990784, 6.263375075949867, 0.2921945094875962, 0.8337350564385663, 'FF', True, 'bce')\n",
      "('GRU', 2.0903799907935114e+17, 177033309.77976447, 10.273046768311445, 0.3660171843661337, 0.7654626355206238, 'FF', True, 'bce')\n",
      "('GRU', 2.5543009062187184e+17, 207275693.27662843, 19.000740260456066, 0.388408838183928, 0.7048467801339358, 'FF', True, 'bce')\n",
      "\u001b[32m[I 2025-11-01 01:37:44,390]\u001b[0m A new study created in memory with name: no-name-29c331fa-2707-4732-8033-301597f88ca0\u001b[0m\n",
      "\u001b[33m[W 2025-11-01 01:37:44,391]\u001b[0m Trial 0 failed with parameters: {'lr': 0.03125300398951733, 'hs': 11, 'dropout': 0.06514585763042045} because of the following error: TypeError(\"__init__() got an unexpected keyword argument 'inputdim'\").\u001b[0m\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/keen/.pyenv/versions/3.9.13/lib/python3.9/site-packages/optuna/study/_optimize.py\", line 196, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/data/workspace/sapran/agro_graph_transformer/agro_graph_transformer/experiments.py\", line 230, in objective\n",
      "    model = make_model(input_shape=Xtr.shape[2],hidden_size=hidden_size, output_size = outp_size, dropout = do,scaled=is_normed,beta = (loss_type == 'beta'),mse=(loss_type == 'mse'))\n",
      "  File \"/data/workspace/sapran/agro_graph_transformer/agro_graph_transformer/base_models.py\", line 96, in make_modelTKAN\n",
      "    return TKANModel(input_shape, hidden_size,output_size, dropout,scaled,beta, mse)\n",
      "  File \"/data/workspace/sapran/agro_graph_transformer/agro_graph_transformer/base_models.py\", line 80, in __init__\n",
      "    tkan = tKANLSTM(\n",
      "  File \"/home/keen/.pyenv/versions/3.9.13/lib/python3.9/site-packages/timekan/models/tkan_lstm.py\", line 233, in __init__\n",
      "    self.cell = TKANCell(\n",
      "  File \"/home/keen/.pyenv/versions/3.9.13/lib/python3.9/site-packages/timekan/models/tkan_lstm.py\", line 89, in __init__\n",
      "    layer = model(inputdim=self.sub_kan_input_dim, outdim=self.sub_kan_output_dim, **self.sub_kan_configs)\n",
      "TypeError: __init__() got an unexpected keyword argument 'inputdim'\n",
      "\u001b[33m[W 2025-11-01 01:37:44,391]\u001b[0m Trial 0 failed with value None.\u001b[0m\n",
      "Traceback (most recent call last):\n",
      "  File \"/data/workspace/sapran/agro_graph_transformer/agro_graph_transformer/experiments.py\", line 258, in <module>\n",
      "    study.optimize(objective, n_trials=100)    \n",
      "  File \"/home/keen/.pyenv/versions/3.9.13/lib/python3.9/site-packages/optuna/study/study.py\", line 451, in optimize\n",
      "    _optimize(\n",
      "  File \"/home/keen/.pyenv/versions/3.9.13/lib/python3.9/site-packages/optuna/study/_optimize.py\", line 62, in _optimize\n",
      "    _optimize_sequential(\n",
      "  File \"/home/keen/.pyenv/versions/3.9.13/lib/python3.9/site-packages/optuna/study/_optimize.py\", line 159, in _optimize_sequential\n",
      "    frozen_trial = _run_trial(study, func, catch)\n",
      "  File \"/home/keen/.pyenv/versions/3.9.13/lib/python3.9/site-packages/optuna/study/_optimize.py\", line 247, in _run_trial\n",
      "    raise func_err\n",
      "  File \"/home/keen/.pyenv/versions/3.9.13/lib/python3.9/site-packages/optuna/study/_optimize.py\", line 196, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/data/workspace/sapran/agro_graph_transformer/agro_graph_transformer/experiments.py\", line 230, in objective\n",
      "    model = make_model(input_shape=Xtr.shape[2],hidden_size=hidden_size, output_size = outp_size, dropout = do,scaled=is_normed,beta = (loss_type == 'beta'),mse=(loss_type == 'mse'))\n",
      "  File \"/data/workspace/sapran/agro_graph_transformer/agro_graph_transformer/base_models.py\", line 96, in make_modelTKAN\n",
      "    return TKANModel(input_shape, hidden_size,output_size, dropout,scaled,beta, mse)\n",
      "  File \"/data/workspace/sapran/agro_graph_transformer/agro_graph_transformer/base_models.py\", line 80, in __init__\n",
      "    tkan = tKANLSTM(\n",
      "  File \"/home/keen/.pyenv/versions/3.9.13/lib/python3.9/site-packages/timekan/models/tkan_lstm.py\", line 233, in __init__\n",
      "    self.cell = TKANCell(\n",
      "  File \"/home/keen/.pyenv/versions/3.9.13/lib/python3.9/site-packages/timekan/models/tkan_lstm.py\", line 89, in __init__\n",
      "    layer = model(inputdim=self.sub_kan_input_dim, outdim=self.sub_kan_output_dim, **self.sub_kan_configs) \n",
      "TypeError: __init__() got an unexpected keyword argument 'inputdim'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "\u001b[32m[I 2025-10-30 06:37:59,208]\u001b[0m Trial 36 finished with value: 0.022599807417222405 and parameters: {'lr': 0.00861767954990759, 'hs': 26, 'dropout': 0.12927392292629059}. Best is trial 28 with value: 0.02040131894504135.\u001b[0m\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "\u001b[32m[I 2025-10-30 06:38:05,861]\u001b[0m Trial 37 finished with value: 0.0220881554671018 and parameters: {'lr': 0.006682688410414999, 'hs': 29, 'dropout': 0.1479343423692656}. Best is trial 28 with value: 0.02040131894504135.\u001b[0m\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "\u001b[32m[I 2025-10-30 06:38:15,218]\u001b[0m Trial 38 finished with value: 0.02103663707482108 and parameters: {'lr': 0.005878654854353508, 'hs': 28, 'dropout': 0.17559944003581493}. Best is trial 28 with value: 0.02040131894504135.\u001b[0m\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "\u001b[32m[I 2025-10-30 06:38:25,816]\u001b[0m Trial 39 finished with value: 0.021005342786798692 and parameters: {'lr': 0.0028378618754607288, 'hs': 24, 'dropout': 0.19718068401463534}. Best is trial 28 with value: 0.02040131894504135.\u001b[0m\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "\u001b[32m[I 2025-10-30 06:39:30,040]\u001b[0m Trial 40 finished with value: 0.029067124483631582 and parameters: {'lr': 0.004891471713798895, 'hs': 7, 'dropout': 0.11788775708490506}. Best is trial 28 with value: 0.02040131894504135.\u001b[0m\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "\u001b[32m[I 2025-10-30 06:39:36,978]\u001b[0m Trial 41 finished with value: 0.020010322959780336 and parameters: {'lr': 0.008400171743999325, 'hs': 30, 'dropout': 0.09923813904876977}. Best is trial 41 with value: 0.020010322959780336.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "\u001b[32m[I 2025-10-30 06:39:44,674]\u001b[0m Trial 42 finished with value: 0.023913486990013815 and parameters: {'lr': 0.008461250534780002, 'hs': 30, 'dropout': 0.1143068008719891}. Best is trial 41 with value: 0.020010322959780336.\u001b[0m\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "\u001b[32m[I 2025-10-30 06:39:51,405]\u001b[0m Trial 43 finished with value: 0.020586833451348097 and parameters: {'lr': 0.009676471327532453, 'hs': 27, 'dropout': 0.10387304432403759}. Best is trial 41 with value: 0.020010322959780336.\u001b[0m\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "\u001b[32m[I 2025-10-30 06:40:00,042]\u001b[0m Trial 44 finished with value: 0.021910886060437673 and parameters: {'lr': 0.009322605017076989, 'hs': 27, 'dropout': 0.08163009174516575}. Best is trial 41 with value: 0.020010322959780336.\u001b[0m\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "\u001b[32m[I 2025-10-30 06:40:06,666]\u001b[0m Trial 45 finished with value: 0.024580044873458644 and parameters: {'lr': 0.009918093594466965, 'hs': 25, 'dropout': 0.10274117369935117}. Best is trial 41 with value: 0.020010322959780336.\u001b[0m\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "\u001b[32m[I 2025-10-30 06:40:15,143]\u001b[0m Trial 46 finished with value: 0.0204521146375269 and parameters: {'lr': 0.00887772054705944, 'hs': 22, 'dropout': 0.06307477000359693}. Best is trial 41 with value: 0.020010322959780336.\u001b[0m\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "\u001b[32m[I 2025-10-30 06:40:23,065]\u001b[0m Trial 47 finished with value: 0.020684185118223355 and parameters: {'lr': 0.008832523206960733, 'hs': 23, 'dropout': 0.056208629102139356}. Best is trial 41 with value: 0.020010322959780336.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "\u001b[32m[I 2025-10-30 06:40:33,173]\u001b[0m Trial 48 finished with value: 0.021328714886820126 and parameters: {'lr': 0.008268285387427469, 'hs': 21, 'dropout': 0.06489939096012384}. Best is trial 41 with value: 0.020010322959780336.\u001b[0m\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "\u001b[32m[I 2025-10-30 06:40:39,956]\u001b[0m Trial 49 finished with value: 0.023300488343669434 and parameters: {'lr': 0.00975474474528676, 'hs': 16, 'dropout': 0.1328667629007407}. Best is trial 41 with value: 0.020010322959780336.\u001b[0m\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "('LSTM', 1.0927421406613508e+18, 883126739.1740448, -0.4580117463560405, 'ARIMA', True, 'beta')\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "('LSTM', 1.0662118152988095e+18, 885107015.5406271, -0.4337960859656267, 'ARIMA', True, 'beta')\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "('LSTM', 1.1776624472289759e+18, 920578107.7380737, -0.4895234856619981, 'ARIMA', True, 'beta')\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "('LSTM', 1.1069019087305504e+18, 864184206.5800437, -0.33581331386467617, 'ARIMA', True, 'beta')\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "('LSTM', 8.39699806192201e+17, 842120831.8057023, -1.2924682657002373, 'ARIMA', True, 'beta')\n",
      "\u001b[32m[I 2025-10-30 06:45:59,506]\u001b[0m A new study created in memory with name: no-name-25bcc161-45bb-4b11-a83f-d1a2d30a4b29\u001b[0m\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "\u001b[32m[I 2025-10-30 06:46:07,042]\u001b[0m Trial 0 finished with value: 0.00235465897897037 and parameters: {'lr': 0.007536580977427676, 'hs': 29, 'dropout': 0.12994729704541064}. Best is trial 0 with value: 0.00235465897897037.\u001b[0m\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "\u001b[32m[I 2025-10-30 06:46:21,900]\u001b[0m Trial 1 finished with value: 0.0037050262609168824 and parameters: {'lr': 0.0012413191133765675, 'hs': 30, 'dropout': 0.1448928741057417}. Best is trial 0 with value: 0.00235465897897037.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "\u001b[32m[I 2025-10-30 06:46:33,415]\u001b[0m Trial 2 finished with value: 0.0034286828645880944 and parameters: {'lr': 0.004163862591815302, 'hs': 9, 'dropout': 0.13300965914925694}. Best is trial 0 with value: 0.00235465897897037.\u001b[0m\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "\u001b[32m[I 2025-10-30 06:46:43,009]\u001b[0m Trial 3 finished with value: 0.0022945395151650985 and parameters: {'lr': 0.005158914223915607, 'hs': 22, 'dropout': 0.06963816052774775}. Best is trial 3 with value: 0.0022945395151650985.\u001b[0m\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "\u001b[32m[I 2025-10-30 06:46:51,626]\u001b[0m Trial 4 finished with value: 0.0028996731355311503 and parameters: {'lr': 0.009798753804702218, 'hs': 11, 'dropout': 0.07991116477996292}. Best is trial 3 with value: 0.0022945395151650985.\u001b[0m\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "\u001b[32m[I 2025-10-30 06:47:02,239]\u001b[0m Trial 5 finished with value: 0.003555293130166185 and parameters: {'lr': 0.005999617357360666, 'hs': 4, 'dropout': 0.06026690848718266}. Best is trial 3 with value: 0.0022945395151650985.\u001b[0m\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "\u001b[32m[I 2025-10-30 06:47:13,748]\u001b[0m Trial 6 finished with value: 0.0030895946793250257 and parameters: {'lr': 0.003648716659283844, 'hs': 12, 'dropout': 0.10475337779081771}. Best is trial 3 with value: 0.0022945395151650985.\u001b[0m\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "\u001b[32m[I 2025-10-30 06:47:23,592]\u001b[0m Trial 7 finished with value: 0.002467835586890921 and parameters: {'lr': 0.005023493935671683, 'hs': 16, 'dropout': 0.0874397190344825}. Best is trial 3 with value: 0.0022945395151650985.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "\u001b[32m[I 2025-10-30 06:47:32,770]\u001b[0m Trial 8 finished with value: 0.003446738215887767 and parameters: {'lr': 0.009135221220665766, 'hs': 9, 'dropout': 0.16587560209252866}. Best is trial 3 with value: 0.0022945395151650985.\u001b[0m\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "\u001b[32m[I 2025-10-30 06:47:48,471]\u001b[0m Trial 9 finished with value: 0.0031217801730974077 and parameters: {'lr': 0.0015500035249650858, 'hs': 26, 'dropout': 0.084965982545608}. Best is trial 3 with value: 0.0022945395151650985.\u001b[0m\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "\u001b[32m[I 2025-10-30 06:47:55,210]\u001b[0m Trial 10 finished with value: 0.002613503609926978 and parameters: {'lr': 0.007094090211905547, 'hs': 22, 'dropout': 0.1855346174842241}. Best is trial 3 with value: 0.0022945395151650985.\u001b[0m\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "\u001b[32m[I 2025-10-30 06:48:00,615]\u001b[0m Trial 11 finished with value: 0.002250085794637469 and parameters: {'lr': 0.007652171255965646, 'hs': 32, 'dropout': 0.1170396292868759}. Best is trial 11 with value: 0.002250085794637469.\u001b[0m\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "\u001b[32m[I 2025-10-30 06:48:07,911]\u001b[0m Trial 12 finished with value: 0.002191200730187747 and parameters: {'lr': 0.007658273751800081, 'hs': 23, 'dropout': 0.050516240249409955}. Best is trial 12 with value: 0.002191200730187747.\u001b[0m\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "\u001b[32m[I 2025-10-30 06:48:16,816]\u001b[0m Trial 13 finished with value: 0.002343969709945978 and parameters: {'lr': 0.008111877882893038, 'hs': 23, 'dropout': 0.051751142939108534}. Best is trial 12 with value: 0.002191200730187747.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "\u001b[32m[I 2025-10-30 06:48:25,839]\u001b[0m Trial 14 finished with value: 0.002314099623201893 and parameters: {'lr': 0.006951845258638331, 'hs': 32, 'dropout': 0.10379098327996372}. Best is trial 12 with value: 0.002191200730187747.\u001b[0m\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "\u001b[32m[I 2025-10-30 06:48:34,260]\u001b[0m Trial 15 finished with value: 0.002733991912038897 and parameters: {'lr': 0.00896746931979562, 'hs': 17, 'dropout': 0.10777068458093647}. Best is trial 12 with value: 0.002191200730187747.\u001b[0m\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "\u001b[32m[I 2025-10-30 06:48:41,570]\u001b[0m Trial 16 finished with value: 0.002525863076972354 and parameters: {'lr': 0.0060804456134143136, 'hs': 27, 'dropout': 0.1555014696685152}. Best is trial 12 with value: 0.002191200730187747.\u001b[0m\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "\u001b[32m[I 2025-10-30 06:48:52,607]\u001b[0m Trial 17 finished with value: 0.0031750793109985454 and parameters: {'lr': 0.0028803008189848397, 'hs': 24, 'dropout': 0.17365200415501264}. Best is trial 12 with value: 0.002191200730187747.\u001b[0m\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "\u001b[32m[I 2025-10-30 06:48:59,491]\u001b[0m Trial 18 finished with value: 0.0026924671899461947 and parameters: {'lr': 0.008410598271307551, 'hs': 19, 'dropout': 0.19290807850581404}. Best is trial 12 with value: 0.002191200730187747.\u001b[0m\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "\u001b[32m[I 2025-10-30 06:49:39,347]\u001b[0m Trial 19 finished with value: 0.01526645627525966 and parameters: {'lr': 9.206780485355136e-05, 'hs': 32, 'dropout': 0.11653196845569651}. Best is trial 12 with value: 0.002191200730187747.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "\u001b[32m[I 2025-10-30 06:49:46,994]\u001b[0m Trial 20 finished with value: 0.0025183720365582925 and parameters: {'lr': 0.009902928148737718, 'hs': 19, 'dropout': 0.14156567195106431}. Best is trial 12 with value: 0.002191200730187747.\u001b[0m\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "\u001b[32m[I 2025-10-30 06:49:56,085]\u001b[0m Trial 21 finished with value: 0.002183974295126214 and parameters: {'lr': 0.0058440943592461695, 'hs': 26, 'dropout': 0.06705726317415453}. Best is trial 21 with value: 0.002183974295126214.\u001b[0m\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "\u001b[32m[I 2025-10-30 06:50:05,334]\u001b[0m Trial 22 finished with value: 0.002212963162778842 and parameters: {'lr': 0.006062121286814598, 'hs': 27, 'dropout': 0.06879368166561822}. Best is trial 21 with value: 0.002183974295126214.\u001b[0m\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "\u001b[32m[I 2025-10-30 06:50:11,463]\u001b[0m Trial 23 finished with value: 0.0021919473411977472 and parameters: {'lr': 0.006301616786833091, 'hs': 26, 'dropout': 0.06655598041729667}. Best is trial 21 with value: 0.002183974295126214.\u001b[0m\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "\u001b[32m[I 2025-10-30 06:50:21,077]\u001b[0m Trial 24 finished with value: 0.002128556279837506 and parameters: {'lr': 0.006496165860130108, 'hs': 25, 'dropout': 0.050285006173618314}. Best is trial 24 with value: 0.002128556279837506.\u001b[0m\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "\u001b[32m[I 2025-10-30 06:50:30,972]\u001b[0m Trial 25 finished with value: 0.002348126120887396 and parameters: {'lr': 0.004375259687448014, 'hs': 21, 'dropout': 0.05017459429407485}. Best is trial 24 with value: 0.002128556279837506.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "\u001b[32m[I 2025-10-30 06:50:39,900]\u001b[0m Trial 26 finished with value: 0.0024658624151274178 and parameters: {'lr': 0.005328177890917322, 'hs': 25, 'dropout': 0.0909589335990904}. Best is trial 24 with value: 0.002128556279837506.\u001b[0m\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "\u001b[32m[I 2025-10-30 06:50:48,499]\u001b[0m Trial 27 finished with value: 0.0023935727176527507 and parameters: {'lr': 0.0067888679743189915, 'hs': 29, 'dropout': 0.059261361275246636}. Best is trial 24 with value: 0.002128556279837506.\u001b[0m\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "\u001b[32m[I 2025-10-30 06:50:56,789]\u001b[0m Trial 28 finished with value: 0.002393999189180559 and parameters: {'lr': 0.00800793282691575, 'hs': 19, 'dropout': 0.07923325958005795}. Best is trial 24 with value: 0.002128556279837506.\u001b[0m\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "\u001b[32m[I 2025-10-30 06:51:05,768]\u001b[0m Trial 29 finished with value: 0.0022230819615842623 and parameters: {'lr': 0.007404121973750125, 'hs': 29, 'dropout': 0.07381552801441622}. Best is trial 24 with value: 0.002128556279837506.\u001b[0m\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "\u001b[32m[I 2025-10-30 06:51:16,649]\u001b[0m Trial 30 finished with value: 0.0030061809910398606 and parameters: {'lr': 0.0027830682478292754, 'hs': 15, 'dropout': 0.09427476566076651}. Best is trial 24 with value: 0.002128556279837506.\u001b[0m\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "\u001b[32m[I 2025-10-30 06:51:25,691]\u001b[0m Trial 31 finished with value: 0.0023329827584210425 and parameters: {'lr': 0.006434294721416983, 'hs': 25, 'dropout': 0.06174939765747482}. Best is trial 24 with value: 0.002128556279837506.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "\u001b[32m[I 2025-10-30 06:51:35,094]\u001b[0m Trial 32 finished with value: 0.0022151891327599517 and parameters: {'lr': 0.005873481263424454, 'hs': 28, 'dropout': 0.05046302844806919}. Best is trial 24 with value: 0.002128556279837506.\u001b[0m\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "\u001b[32m[I 2025-10-30 06:51:43,099]\u001b[0m Trial 33 finished with value: 0.0024576848460857 and parameters: {'lr': 0.004203991149431666, 'hs': 24, 'dropout': 0.06389527677848868}. Best is trial 24 with value: 0.002128556279837506.\u001b[0m\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "\u001b[32m[I 2025-10-30 06:51:51,839]\u001b[0m Trial 34 finished with value: 0.002398345289220548 and parameters: {'lr': 0.005421139799213294, 'hs': 21, 'dropout': 0.07300514655284375}. Best is trial 24 with value: 0.002128556279837506.\u001b[0m\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "\u001b[32m[I 2025-10-30 06:51:59,601]\u001b[0m Trial 35 finished with value: 0.002248257228425783 and parameters: {'lr': 0.006571917482511979, 'hs': 26, 'dropout': 0.057737692009864686}. Best is trial 24 with value: 0.002128556279837506.\u001b[0m\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "\u001b[32m[I 2025-10-30 06:52:07,962]\u001b[0m Trial 36 finished with value: 0.0022581244218617337 and parameters: {'lr': 0.004668397014014714, 'hs': 20, 'dropout': 0.07890112451627561}. Best is trial 24 with value: 0.002128556279837506.\u001b[0m\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "\u001b[32m[I 2025-10-30 06:52:16,396]\u001b[0m Trial 37 finished with value: 0.002104776521535009 and parameters: {'lr': 0.008786167388712728, 'hs': 30, 'dropout': 0.06690505254916872}. Best is trial 37 with value: 0.002104776521535009.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "\u001b[32m[I 2025-10-30 06:52:24,287]\u001b[0m Trial 38 finished with value: 0.0031469232273247487 and parameters: {'lr': 0.008894613726822117, 'hs': 4, 'dropout': 0.09590003714557825}. Best is trial 37 with value: 0.002104776521535009.\u001b[0m\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "\u001b[32m[I 2025-10-30 06:52:32,829]\u001b[0m Trial 39 finished with value: 0.0021611150417767595 and parameters: {'lr': 0.008613874742267328, 'hs': 30, 'dropout': 0.05771806372304085}. Best is trial 37 with value: 0.002104776521535009.\u001b[0m\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "\u001b[32m[I 2025-10-30 06:52:40,749]\u001b[0m Trial 40 finished with value: 0.0022265167837430956 and parameters: {'lr': 0.009486882847758999, 'hs': 30, 'dropout': 0.07411649501594371}. Best is trial 37 with value: 0.002104776521535009.\u001b[0m\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "\u001b[32m[I 2025-10-30 06:52:50,031]\u001b[0m Trial 41 finished with value: 0.0021694111130156654 and parameters: {'lr': 0.00855452503324251, 'hs': 30, 'dropout': 0.05648131366997193}. Best is trial 37 with value: 0.002104776521535009.\u001b[0m\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "\u001b[32m[I 2025-10-30 06:52:58,576]\u001b[0m Trial 42 finished with value: 0.0022072333316871147 and parameters: {'lr': 0.008500630391464476, 'hs': 30, 'dropout': 0.05669964171733944}. Best is trial 37 with value: 0.002104776521535009.\u001b[0m\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "\u001b[32m[I 2025-10-30 06:53:06,726]\u001b[0m Trial 43 finished with value: 0.002406143964532289 and parameters: {'lr': 0.009255558017419936, 'hs': 30, 'dropout': 0.0674491375801014}. Best is trial 37 with value: 0.002104776521535009.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "\u001b[32m[I 2025-10-30 06:53:14,474]\u001b[0m Trial 44 finished with value: 0.009656367603658704 and parameters: {'lr': 0.008574538765234868, 'hs': 2, 'dropout': 0.08150111918566302}. Best is trial 37 with value: 0.002104776521535009.\u001b[0m\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "\u001b[32m[I 2025-10-30 06:53:21,579]\u001b[0m Trial 45 finished with value: 0.002333596630823207 and parameters: {'lr': 0.007389364542578484, 'hs': 31, 'dropout': 0.05939968659468844}. Best is trial 37 with value: 0.002104776521535009.\u001b[0m\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "\u001b[32m[I 2025-10-30 06:53:28,194]\u001b[0m Trial 46 finished with value: 0.0023942042520601696 and parameters: {'lr': 0.007896766399416161, 'hs': 28, 'dropout': 0.07127378006534549}. Best is trial 37 with value: 0.002104776521535009.\u001b[0m\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "\u001b[32m[I 2025-10-30 06:53:39,627]\u001b[0m Trial 47 finished with value: 0.0025254444977458935 and parameters: {'lr': 0.003605929644419375, 'hs': 28, 'dropout': 0.08665186700197754}. Best is trial 37 with value: 0.002104776521535009.\u001b[0m\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "\u001b[32m[I 2025-10-30 06:53:47,399]\u001b[0m Trial 48 finished with value: 0.002268328919277352 and parameters: {'lr': 0.009370638778533625, 'hs': 31, 'dropout': 0.056254135289796545}. Best is trial 37 with value: 0.002104776521535009.\u001b[0m\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "\u001b[32m[I 2025-10-30 06:53:55,097]\u001b[0m Trial 49 finished with value: 0.0022264169683208546 and parameters: {'lr': 0.00997771390154512, 'hs': 31, 'dropout': 0.06376077970199892}. Best is trial 37 with value: 0.002104776521535009.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "('LSTM', 1.7009880978826768e+17, 160554549.5048026, 0.7864915674771878, 'ARIMA', True, 'bce')\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "('LSTM', 1.4280981057223627e+17, 163217873.84986198, 0.8226182515571584, 'ARIMA', True, 'bce')\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "('LSTM', 9.2244588249776e+16, 135439789.07065275, 0.8088188559292857, 'ARIMA', True, 'bce')\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "('LSTM', 6.454218328443199e+16, 130359780.88279203, 0.8438834336939052, 'ARIMA', True, 'bce')\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "('LSTM', 1.777379012703171e+17, 169022123.03643182, 0.7307776966661192, 'ARIMA', True, 'bce')\n",
      "\u001b[32m[I 2025-10-30 06:56:18,611]\u001b[0m A new study created in memory with name: no-name-24119d69-cdca-425a-9b12-dee3c09bc6c2\u001b[0m\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1394: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.gru(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1394: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.gru(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1394: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.gru(\n",
      "\u001b[32m[I 2025-10-30 06:56:29,954]\u001b[0m Trial 0 finished with value: 0.02556765002560717 and parameters: {'lr': 0.003940241646682268, 'hs': 10, 'dropout': 0.11597374819937235}. Best is trial 0 with value: 0.02556765002560717.\u001b[0m\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1394: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.gru(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1394: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.gru(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1394: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.gru(\n",
      "\u001b[32m[I 2025-10-30 06:56:38,326]\u001b[0m Trial 1 finished with value: 0.023879473715407922 and parameters: {'lr': 0.008436947807244815, 'hs': 9, 'dropout': 0.17086761381191337}. Best is trial 1 with value: 0.023879473715407922.\u001b[0m\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1394: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.gru(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1394: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.gru(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1394: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.gru(\n",
      "\u001b[32m[I 2025-10-30 06:56:48,998]\u001b[0m Trial 2 finished with value: 0.024970798111783866 and parameters: {'lr': 0.001953209685759818, 'hs': 18, 'dropout': 0.19782526388498345}. Best is trial 1 with value: 0.023879473715407922.\u001b[0m\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1394: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.gru(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1394: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.gru(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1394: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.gru(\n",
      "\u001b[32m[I 2025-10-30 06:56:59,454]\u001b[0m Trial 3 finished with value: 0.023815077665751524 and parameters: {'lr': 0.008789558185709628, 'hs': 20, 'dropout': 0.08595390359554886}. Best is trial 3 with value: 0.023815077665751524.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1394: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.gru(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1394: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.gru(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1394: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.gru(\n",
      "\u001b[32m[I 2025-10-30 06:57:25,710]\u001b[0m Trial 4 finished with value: 0.03314149049263374 and parameters: {'lr': 0.0013501853088902732, 'hs': 3, 'dropout': 0.11715142581304074}. Best is trial 3 with value: 0.023815077665751524.\u001b[0m\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1394: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.gru(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1394: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.gru(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1394: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.gru(\n",
      "\u001b[32m[I 2025-10-30 06:57:32,897]\u001b[0m Trial 5 finished with value: 0.02485875378406942 and parameters: {'lr': 0.008515075787380354, 'hs': 19, 'dropout': 0.10619196154397939}. Best is trial 3 with value: 0.023815077665751524.\u001b[0m\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1394: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.gru(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1394: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.gru(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1394: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.gru(\n",
      "\u001b[32m[I 2025-10-30 06:57:40,814]\u001b[0m Trial 6 finished with value: 0.02224486364538847 and parameters: {'lr': 0.006043172326956716, 'hs': 16, 'dropout': 0.12525356888878603}. Best is trial 6 with value: 0.02224486364538847.\u001b[0m\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1394: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.gru(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1394: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.gru(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1394: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.gru(\n",
      "\u001b[32m[I 2025-10-30 06:57:49,889]\u001b[0m Trial 7 finished with value: 0.02535442161610596 and parameters: {'lr': 0.005661009669362265, 'hs': 12, 'dropout': 0.12883799849424554}. Best is trial 6 with value: 0.02224486364538847.\u001b[0m\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1394: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.gru(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1394: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.gru(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1394: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.gru(\n",
      "\u001b[32m[I 2025-10-30 06:57:58,950]\u001b[0m Trial 8 finished with value: 0.026783116001767766 and parameters: {'lr': 0.008904045201036869, 'hs': 10, 'dropout': 0.16609487212856505}. Best is trial 6 with value: 0.02224486364538847.\u001b[0m\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1394: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.gru(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1394: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.gru(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1394: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.gru(\n",
      "\u001b[32m[I 2025-10-30 06:58:07,007]\u001b[0m Trial 9 finished with value: 0.022081630162957725 and parameters: {'lr': 0.004220243049174236, 'hs': 24, 'dropout': 0.1401558290294213}. Best is trial 9 with value: 0.022081630162957725.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1394: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.gru(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1394: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.gru(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1394: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.gru(\n",
      "\u001b[32m[I 2025-10-30 06:58:15,795]\u001b[0m Trial 10 finished with value: 0.01969486003686791 and parameters: {'lr': 0.0036034732410865794, 'hs': 30, 'dropout': 0.06667770623617954}. Best is trial 10 with value: 0.01969486003686791.\u001b[0m\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1394: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.gru(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1394: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.gru(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1394: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.gru(\n",
      "\u001b[32m[I 2025-10-30 06:58:25,483]\u001b[0m Trial 11 finished with value: 0.020872679690659274 and parameters: {'lr': 0.003971563338117171, 'hs': 30, 'dropout': 0.07146607167600838}. Best is trial 10 with value: 0.01969486003686791.\u001b[0m\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1394: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.gru(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1394: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.gru(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1394: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.gru(\n",
      "\u001b[32m[I 2025-10-30 06:58:35,701]\u001b[0m Trial 12 finished with value: 0.021885857431733075 and parameters: {'lr': 0.0026994954114202764, 'hs': 30, 'dropout': 0.05058509702604504}. Best is trial 10 with value: 0.01969486003686791.\u001b[0m\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1394: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.gru(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1394: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.gru(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1394: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.gru(\n",
      "\u001b[32m[I 2025-10-30 06:58:55,142]\u001b[0m Trial 13 finished with value: 0.02350631826730473 and parameters: {'lr': 0.0005770668042771234, 'hs': 32, 'dropout': 0.054944922968261756}. Best is trial 10 with value: 0.01969486003686791.\u001b[0m\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1394: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.gru(\n"
     ]
    }
   ],
   "source": [
    "!python experiments.py --imputation FF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26317921",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3.10 install torchdata==0.8.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c8a0b7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "print(torch.version.cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9aa77da",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3.10 install torchvision==0.18.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd258879",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3.10 install torchtext==0.18.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc103f1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12ced98c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3.10 install torch==2.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e9856a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3.10 install sktime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0b91876",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3.10 install -U dgl -f https://data.dgl.ai/wheels/torch-2.3/cu121/repo.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c5a8cb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3.10 install pydantic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6389887e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3.10 install pytorch_forecasting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e25ece39",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3.10 install timekan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a3d6682",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3.10 install torchvision==0.24.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeea73a0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
