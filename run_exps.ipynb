{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66bdca18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/keen/.pyenv/versions/3.9.13/lib/python3.9/site-packages/torch/utils/_pytree.py:185: FutureWarning: optree is installed but the version is too old to support PyTorch Dynamo in C++ pytree. C++ pytree support is disabled. Please consider upgrading optree using `python3 -m pip install --upgrade 'optree>=0.13.0'`.\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2025-11-01 13:17:10,761]\u001b[0m A new study created in memory with name: no-name-956e75ff-85c1-4b6f-b5fe-6021eb1fb96c\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 13:17:26,191]\u001b[0m Trial 0 finished with value: 0.002902532521644662 and parameters: {'lr': 0.0068471765841184, 'hs': 27, 'dropout': 0.15527182418538327}. Best is trial 0 with value: 0.002902532521644662.\u001b[0m\n",
      "all elements of input should be between 0 and 1\n",
      "all elements of input should be between 0 and 1\n",
      "\u001b[32m[I 2025-11-01 13:17:29,500]\u001b[0m Trial 1 finished with value: 6.666666666666667e+25 and parameters: {'lr': 0.07212905336583822, 'hs': 28, 'dropout': 0.13597080037205714}. Best is trial 0 with value: 0.002902532521644662.\u001b[0m\n",
      "all elements of input should be between 0 and 1\n",
      "\u001b[32m[I 2025-11-01 13:17:35,154]\u001b[0m Trial 2 finished with value: 3.3333333333333335e+25 and parameters: {'lr': 0.05840302182457777, 'hs': 28, 'dropout': 0.08960536373039948}. Best is trial 0 with value: 0.002902532521644662.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 13:17:50,214]\u001b[0m Trial 3 finished with value: 0.0035544852987771437 and parameters: {'lr': 0.003215809223014324, 'hs': 10, 'dropout': 0.17513206169368672}. Best is trial 0 with value: 0.002902532521644662.\u001b[0m\n",
      "all elements of input should be between 0 and 1\n",
      "all elements of input should be between 0 and 1\n",
      "\u001b[32m[I 2025-11-01 13:17:53,519]\u001b[0m Trial 4 finished with value: 6.666666666666667e+25 and parameters: {'lr': 0.08044689562586722, 'hs': 4, 'dropout': 0.05243062487772018}. Best is trial 0 with value: 0.002902532521644662.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 13:18:15,302]\u001b[0m Trial 5 finished with value: 0.003959288938375106 and parameters: {'lr': 0.0017907574502927788, 'hs': 28, 'dropout': 0.12866795025669925}. Best is trial 0 with value: 0.002902532521644662.\u001b[0m\n",
      "all elements of input should be between 0 and 1\n",
      "\u001b[32m[I 2025-11-01 13:18:21,396]\u001b[0m Trial 6 finished with value: 3.3333333333333335e+25 and parameters: {'lr': 0.058758016995975014, 'hs': 5, 'dropout': 0.13384970914120603}. Best is trial 0 with value: 0.002902532521644662.\u001b[0m\n",
      "all elements of input should be between 0 and 1\n",
      "\u001b[32m[I 2025-11-01 13:18:28,163]\u001b[0m Trial 7 finished with value: 3.3333333333333335e+25 and parameters: {'lr': 0.07655509603094424, 'hs': 17, 'dropout': 0.1633426776891217}. Best is trial 0 with value: 0.002902532521644662.\u001b[0m\n",
      "all elements of input should be between 0 and 1\n",
      "all elements of input should be between 0 and 1\n",
      "\u001b[32m[I 2025-11-01 13:18:31,917]\u001b[0m Trial 8 finished with value: 6.666666666666667e+25 and parameters: {'lr': 0.09434291802529056, 'hs': 25, 'dropout': 0.07093639008565211}. Best is trial 0 with value: 0.002902532521644662.\u001b[0m\n",
      "all elements of input should be between 0 and 1\n",
      "\u001b[32m[I 2025-11-01 13:18:40,405]\u001b[0m Trial 9 finished with value: 3.3333333333333335e+25 and parameters: {'lr': 0.057439806941187374, 'hs': 29, 'dropout': 0.17162180936919189}. Best is trial 0 with value: 0.002902532521644662.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 13:18:55,711]\u001b[0m Trial 10 finished with value: 0.0029917315199498255 and parameters: {'lr': 0.028102132004052337, 'hs': 20, 'dropout': 0.19798088876472084}. Best is trial 0 with value: 0.002902532521644662.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 13:19:08,586]\u001b[0m Trial 11 finished with value: 0.0027217339184236835 and parameters: {'lr': 0.026705089855425017, 'hs': 20, 'dropout': 0.19974408878360086}. Best is trial 11 with value: 0.0027217339184236835.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 13:19:18,906]\u001b[0m Trial 12 finished with value: 0.0028090273014460877 and parameters: {'lr': 0.025602780189633082, 'hs': 20, 'dropout': 0.194600953121942}. Best is trial 11 with value: 0.0027217339184236835.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 13:19:29,422]\u001b[0m Trial 13 finished with value: 0.003005914293258863 and parameters: {'lr': 0.028398676630844152, 'hs': 18, 'dropout': 0.1999714633872962}. Best is trial 11 with value: 0.0027217339184236835.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 13:19:40,626]\u001b[0m Trial 14 finished with value: 0.002968309904620166 and parameters: {'lr': 0.03275505720506785, 'hs': 13, 'dropout': 0.10225967162998335}. Best is trial 11 with value: 0.0027217339184236835.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 13:19:51,080]\u001b[0m Trial 15 finished with value: 0.0031074785750172668 and parameters: {'lr': 0.03971038238288349, 'hs': 23, 'dropout': 0.18543287269595765}. Best is trial 11 with value: 0.0027217339184236835.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 13:20:03,215]\u001b[0m Trial 16 finished with value: 0.0028792976123315777 and parameters: {'lr': 0.01627609631419944, 'hs': 32, 'dropout': 0.14653895068713704}. Best is trial 11 with value: 0.0027217339184236835.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 13:20:13,788]\u001b[0m Trial 17 finished with value: 0.002898159717467425 and parameters: {'lr': 0.018037946177694497, 'hs': 14, 'dropout': 0.11076288995472534}. Best is trial 11 with value: 0.0027217339184236835.\u001b[0m\n",
      "all elements of input should be between 0 and 1\n",
      "\u001b[32m[I 2025-11-01 13:20:22,185]\u001b[0m Trial 18 finished with value: 3.3333333333333335e+25 and parameters: {'lr': 0.04485132048223599, 'hs': 22, 'dropout': 0.18341548733821839}. Best is trial 11 with value: 0.0027217339184236835.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 13:20:33,010]\u001b[0m Trial 19 finished with value: 0.002551316241844473 and parameters: {'lr': 0.019738815846411115, 'hs': 9, 'dropout': 0.18774299048310134}. Best is trial 19 with value: 0.002551316241844473.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 13:20:42,335]\u001b[0m Trial 20 finished with value: 0.003002549872716937 and parameters: {'lr': 0.015413673473053748, 'hs': 8, 'dropout': 0.15416481110090927}. Best is trial 19 with value: 0.002551316241844473.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 13:20:51,526]\u001b[0m Trial 21 finished with value: 0.0030114244880957105 and parameters: {'lr': 0.03736574195025179, 'hs': 14, 'dropout': 0.1875872457455507}. Best is trial 19 with value: 0.002551316241844473.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 13:21:01,853]\u001b[0m Trial 22 finished with value: 0.002956372655772067 and parameters: {'lr': 0.022486559214912123, 'hs': 20, 'dropout': 0.19512360299242418}. Best is trial 19 with value: 0.002551316241844473.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 13:21:14,782]\u001b[0m Trial 23 finished with value: 0.005139300744613607 and parameters: {'lr': 0.011712838405549235, 'hs': 2, 'dropout': 0.17247452749917264}. Best is trial 19 with value: 0.002551316241844473.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 13:21:24,205]\u001b[0m Trial 24 finished with value: 0.0031783510191075257 and parameters: {'lr': 0.04713499713027179, 'hs': 10, 'dropout': 0.17966193706763403}. Best is trial 19 with value: 0.002551316241844473.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 13:21:32,347]\u001b[0m Trial 25 finished with value: 0.0031376619473412666 and parameters: {'lr': 0.02406143962883434, 'hs': 19, 'dropout': 0.1646425148441141}. Best is trial 19 with value: 0.002551316241844473.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 13:21:44,306]\u001b[0m Trial 26 finished with value: 0.0029745601071951566 and parameters: {'lr': 0.03501723125915828, 'hs': 15, 'dropout': 0.19011520021145178}. Best is trial 19 with value: 0.002551316241844473.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 13:21:56,926]\u001b[0m Trial 27 finished with value: 0.0032617146762512635 and parameters: {'lr': 0.011377549462128458, 'hs': 24, 'dropout': 0.14570517792893478}. Best is trial 19 with value: 0.002551316241844473.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 13:22:07,222]\u001b[0m Trial 28 finished with value: 0.002964920768078163 and parameters: {'lr': 0.021846564982075815, 'hs': 16, 'dropout': 0.19958381271673264}. Best is trial 19 with value: 0.002551316241844473.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 13:22:21,309]\u001b[0m Trial 29 finished with value: 0.0029359977021346877 and parameters: {'lr': 0.009666334640366502, 'hs': 12, 'dropout': 0.16272084433969586}. Best is trial 19 with value: 0.002551316241844473.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 13:22:29,771]\u001b[0m Trial 30 finished with value: 0.0027638932874728974 and parameters: {'lr': 0.04304195190879927, 'hs': 21, 'dropout': 0.1523627257739883}. Best is trial 19 with value: 0.002551316241844473.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 13:22:41,741]\u001b[0m Trial 31 finished with value: 0.0030386548152015248 and parameters: {'lr': 0.04323832020623814, 'hs': 21, 'dropout': 0.18281162154060665}. Best is trial 19 with value: 0.002551316241844473.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 13:22:52,402]\u001b[0m Trial 32 finished with value: 0.0028250961093506514 and parameters: {'lr': 0.05318052294384979, 'hs': 25, 'dropout': 0.11487405517766698}. Best is trial 19 with value: 0.002551316241844473.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 13:23:03,294]\u001b[0m Trial 33 finished with value: 0.0028220894552750965 and parameters: {'lr': 0.03155833222850375, 'hs': 18, 'dropout': 0.14407793816955144}. Best is trial 19 with value: 0.002551316241844473.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 13:23:13,683]\u001b[0m Trial 34 finished with value: 0.002726592081581644 and parameters: {'lr': 0.05153147077263904, 'hs': 22, 'dropout': 0.155684370922397}. Best is trial 19 with value: 0.002551316241844473.\u001b[0m\n",
      "all elements of input should be between 0 and 1\n",
      "\u001b[32m[I 2025-11-01 13:23:20,674]\u001b[0m Trial 35 finished with value: 3.3333333333333335e+25 and parameters: {'lr': 0.06668105432306365, 'hs': 26, 'dropout': 0.15461801810595904}. Best is trial 19 with value: 0.002551316241844473.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 13:23:29,297]\u001b[0m Trial 36 finished with value: 0.002752619108308492 and parameters: {'lr': 0.04973468089929165, 'hs': 22, 'dropout': 0.17051168429676358}. Best is trial 19 with value: 0.002551316241844473.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 13:23:36,561]\u001b[0m Trial 37 finished with value: 0.003772555552406128 and parameters: {'lr': 0.06668543509148728, 'hs': 7, 'dropout': 0.17412343101074934}. Best is trial 19 with value: 0.002551316241844473.\u001b[0m\n",
      "all elements of input should be between 0 and 1\n",
      "all elements of input should be between 0 and 1\n",
      "\u001b[32m[I 2025-11-01 13:23:40,279]\u001b[0m Trial 38 finished with value: 6.666666666666667e+25 and parameters: {'lr': 0.08390342605643197, 'hs': 23, 'dropout': 0.1706384557931334}. Best is trial 19 with value: 0.002551316241844473.\u001b[0m\n",
      "all elements of input should be between 0 and 1\n",
      "\u001b[32m[I 2025-11-01 13:23:48,040]\u001b[0m Trial 39 finished with value: 3.3333333333333335e+25 and parameters: {'lr': 0.05125038022103663, 'hs': 30, 'dropout': 0.16277930932880402}. Best is trial 19 with value: 0.002551316241844473.\u001b[0m\n",
      "all elements of input should be between 0 and 1\n",
      "\u001b[32m[I 2025-11-01 13:23:54,360]\u001b[0m Trial 40 finished with value: 3.3333333333333335e+25 and parameters: {'lr': 0.06373540025978024, 'hs': 27, 'dropout': 0.1225727726361803}. Best is trial 19 with value: 0.002551316241844473.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 13:24:05,160]\u001b[0m Trial 41 finished with value: 0.0027956987748580507 and parameters: {'lr': 0.05590391563596364, 'hs': 22, 'dropout': 0.13824347351522978}. Best is trial 19 with value: 0.002551316241844473.\u001b[0m\n",
      "all elements of input should be between 0 and 1\n",
      "\u001b[32m[I 2025-11-01 13:24:10,886]\u001b[0m Trial 42 finished with value: 3.3333333333333335e+25 and parameters: {'lr': 0.0407675410647453, 'hs': 17, 'dropout': 0.15521309415335777}. Best is trial 19 with value: 0.002551316241844473.\u001b[0m\n",
      "all elements of input should be between 0 and 1\n",
      "\u001b[32m[I 2025-11-01 13:24:16,577]\u001b[0m Trial 43 finished with value: 3.3333333333333335e+25 and parameters: {'lr': 0.050066464973819606, 'hs': 21, 'dropout': 0.18026108769980959}. Best is trial 19 with value: 0.002551316241844473.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 13:24:30,827]\u001b[0m Trial 44 finished with value: 0.0032211791227851777 and parameters: {'lr': 0.004135274645069702, 'hs': 24, 'dropout': 0.1672772413544473}. Best is trial 19 with value: 0.002551316241844473.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 13:24:39,183]\u001b[0m Trial 45 finished with value: 0.002799281204937519 and parameters: {'lr': 0.047407662391406404, 'hs': 19, 'dropout': 0.18904654743770383}. Best is trial 19 with value: 0.002551316241844473.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 13:24:50,029]\u001b[0m Trial 46 finished with value: 0.0027805055251750727 and parameters: {'lr': 0.05950684490367119, 'hs': 22, 'dropout': 0.12936282461789497}. Best is trial 19 with value: 0.002551316241844473.\u001b[0m\n",
      "all elements of input should be between 0 and 1\n",
      "\u001b[32m[I 2025-11-01 13:24:58,358]\u001b[0m Trial 47 finished with value: 3.3333333333333335e+25 and parameters: {'lr': 0.0360687980328248, 'hs': 26, 'dropout': 0.1771234385367159}. Best is trial 19 with value: 0.002551316241844473.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 13:25:10,346]\u001b[0m Trial 48 finished with value: 0.0024986922996590648 and parameters: {'lr': 0.030472717780952337, 'hs': 16, 'dropout': 0.05198178950287051}. Best is trial 48 with value: 0.0024986922996590648.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 13:25:21,778]\u001b[0m Trial 49 finished with value: 0.0024958718825190697 and parameters: {'lr': 0.03091040834459444, 'hs': 12, 'dropout': 0.09579605653203363}. Best is trial 49 with value: 0.0024958718825190697.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 13:25:33,277]\u001b[0m Trial 50 finished with value: 0.0028468601650689755 and parameters: {'lr': 0.029245946094418163, 'hs': 11, 'dropout': 0.05048202772900319}. Best is trial 49 with value: 0.0024958718825190697.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 13:25:44,378]\u001b[0m Trial 51 finished with value: 0.0025104641227213426 and parameters: {'lr': 0.018558944016106985, 'hs': 8, 'dropout': 0.0631427862844844}. Best is trial 49 with value: 0.0024958718825190697.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 13:25:57,760]\u001b[0m Trial 52 finished with value: 0.0026924734218162043 and parameters: {'lr': 0.019603162366884377, 'hs': 7, 'dropout': 0.06373782741693043}. Best is trial 49 with value: 0.0024958718825190697.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 13:26:08,302]\u001b[0m Trial 53 finished with value: 0.0023280192720198756 and parameters: {'lr': 0.016796803895621923, 'hs': 8, 'dropout': 0.06253374338385245}. Best is trial 53 with value: 0.0023280192720198756.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 13:26:19,228]\u001b[0m Trial 54 finished with value: 0.0026579989960333673 and parameters: {'lr': 0.020238815879572734, 'hs': 8, 'dropout': 0.06293908983340898}. Best is trial 53 with value: 0.0023280192720198756.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 13:26:32,432]\u001b[0m Trial 55 finished with value: 0.003179837755635685 and parameters: {'lr': 0.006711565414922774, 'hs': 9, 'dropout': 0.0887153732808914}. Best is trial 53 with value: 0.0023280192720198756.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 13:26:45,881]\u001b[0m Trial 56 finished with value: 0.003666482680673279 and parameters: {'lr': 0.014574537955317421, 'hs': 4, 'dropout': 0.06401766192032396}. Best is trial 53 with value: 0.0023280192720198756.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 13:26:59,591]\u001b[0m Trial 57 finished with value: 0.0032900730105927794 and parameters: {'lr': 0.01882564447186405, 'hs': 6, 'dropout': 0.08241569439907437}. Best is trial 53 with value: 0.0023280192720198756.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 13:27:13,063]\u001b[0m Trial 58 finished with value: 0.002601814108223088 and parameters: {'lr': 0.02511542754349421, 'hs': 9, 'dropout': 0.05887194304804747}. Best is trial 53 with value: 0.0023280192720198756.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 13:27:23,875]\u001b[0m Trial 59 finished with value: 0.00225441433861844 and parameters: {'lr': 0.024690849892244668, 'hs': 11, 'dropout': 0.05749036591312548}. Best is trial 59 with value: 0.00225441433861844.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 13:27:34,742]\u001b[0m Trial 60 finished with value: 0.002729709416874488 and parameters: {'lr': 0.030385643654406415, 'hs': 12, 'dropout': 0.07121793267760913}. Best is trial 59 with value: 0.00225441433861844.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 13:27:46,830]\u001b[0m Trial 61 finished with value: 0.002988417084183907 and parameters: {'lr': 0.02692193742001093, 'hs': 10, 'dropout': 0.05950130872529161}. Best is trial 59 with value: 0.00225441433861844.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 13:27:57,259]\u001b[0m Trial 62 finished with value: 0.00262123265261386 and parameters: {'lr': 0.024955889410227184, 'hs': 9, 'dropout': 0.07644476716487153}. Best is trial 59 with value: 0.00225441433861844.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 13:28:09,275]\u001b[0m Trial 63 finished with value: 0.0028447377654884655 and parameters: {'lr': 0.013532088381016463, 'hs': 13, 'dropout': 0.05471086627240695}. Best is trial 59 with value: 0.00225441433861844.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 13:28:17,795]\u001b[0m Trial 64 finished with value: 0.003470286573902831 and parameters: {'lr': 0.03283703893614511, 'hs': 11, 'dropout': 0.07261560643070349}. Best is trial 59 with value: 0.00225441433861844.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 13:28:33,427]\u001b[0m Trial 65 finished with value: 0.0032473596506284023 and parameters: {'lr': 0.007969350522651294, 'hs': 4, 'dropout': 0.09643262510710061}. Best is trial 59 with value: 0.00225441433861844.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 13:28:43,589]\u001b[0m Trial 66 finished with value: 0.002566916944260441 and parameters: {'lr': 0.023049182756115786, 'hs': 9, 'dropout': 0.05832312189645536}. Best is trial 59 with value: 0.00225441433861844.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 13:28:53,270]\u001b[0m Trial 67 finished with value: 0.003002557729824411 and parameters: {'lr': 0.02104592007380958, 'hs': 6, 'dropout': 0.07878936259141621}. Best is trial 59 with value: 0.00225441433861844.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 13:29:34,416]\u001b[0m Trial 68 finished with value: 0.005223796269636274 and parameters: {'lr': 0.0005281639522836494, 'hs': 13, 'dropout': 0.06815630292133595}. Best is trial 59 with value: 0.00225441433861844.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 13:29:46,687]\u001b[0m Trial 69 finished with value: 0.0024471598060364767 and parameters: {'lr': 0.015303658600067153, 'hs': 11, 'dropout': 0.0540419550148781}. Best is trial 59 with value: 0.00225441433861844.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 13:29:59,278]\u001b[0m Trial 70 finished with value: 0.00274934357616546 and parameters: {'lr': 0.01715629946825066, 'hs': 15, 'dropout': 0.055207024887903106}. Best is trial 59 with value: 0.00225441433861844.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 13:30:09,932]\u001b[0m Trial 71 finished with value: 0.0029032198786052735 and parameters: {'lr': 0.013284684086505618, 'hs': 11, 'dropout': 0.05536387013088151}. Best is trial 59 with value: 0.00225441433861844.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 13:30:20,214]\u001b[0m Trial 72 finished with value: 0.0028548218045270136 and parameters: {'lr': 0.0228896735909329, 'hs': 8, 'dropout': 0.0678811180471001}. Best is trial 59 with value: 0.00225441433861844.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 13:30:37,463]\u001b[0m Trial 73 finished with value: 0.00282139614756903 and parameters: {'lr': 0.0051279369824669754, 'hs': 10, 'dropout': 0.051908158916587205}. Best is trial 59 with value: 0.00225441433861844.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 13:30:53,339]\u001b[0m Trial 74 finished with value: 0.0029961192357143792 and parameters: {'lr': 0.01065265413214674, 'hs': 12, 'dropout': 0.06052721078731085}. Best is trial 59 with value: 0.00225441433861844.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 13:31:03,248]\u001b[0m Trial 75 finished with value: 0.002889560036491529 and parameters: {'lr': 0.016743911042102926, 'hs': 7, 'dropout': 0.08811925184553279}. Best is trial 59 with value: 0.00225441433861844.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 13:31:17,305]\u001b[0m Trial 76 finished with value: 0.0028815657406199096 and parameters: {'lr': 0.028123665723332435, 'hs': 14, 'dropout': 0.067647080832722}. Best is trial 59 with value: 0.00225441433861844.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 13:31:26,676]\u001b[0m Trial 77 finished with value: 0.0036247259832467445 and parameters: {'lr': 0.03417732417118811, 'hs': 5, 'dropout': 0.10457508699558099}. Best is trial 59 with value: 0.00225441433861844.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 13:31:39,872]\u001b[0m Trial 78 finished with value: 0.002733003223414306 and parameters: {'lr': 0.021921091647648945, 'hs': 9, 'dropout': 0.07548361968281792}. Best is trial 59 with value: 0.00225441433861844.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 13:31:48,823]\u001b[0m Trial 79 finished with value: 0.0029551192872505736 and parameters: {'lr': 0.03894865362382059, 'hs': 8, 'dropout': 0.08380037185387705}. Best is trial 59 with value: 0.00225441433861844.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 13:32:00,723]\u001b[0m Trial 80 finished with value: 0.0025343154855883344 and parameters: {'lr': 0.01587155157685643, 'hs': 10, 'dropout': 0.05643204312519004}. Best is trial 59 with value: 0.00225441433861844.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 13:32:14,910]\u001b[0m Trial 81 finished with value: 0.002938720227442123 and parameters: {'lr': 0.008920848986257508, 'hs': 10, 'dropout': 0.059394987898229404}. Best is trial 59 with value: 0.00225441433861844.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 13:32:25,860]\u001b[0m Trial 82 finished with value: 0.002468448919661985 and parameters: {'lr': 0.0165108747998925, 'hs': 12, 'dropout': 0.055324511605136364}. Best is trial 59 with value: 0.00225441433861844.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 13:32:38,673]\u001b[0m Trial 83 finished with value: 0.0023158084615940667 and parameters: {'lr': 0.016130013592677225, 'hs': 11, 'dropout': 0.05342142611214424}. Best is trial 59 with value: 0.00225441433861844.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 13:32:50,054]\u001b[0m Trial 84 finished with value: 0.0026846346281589 and parameters: {'lr': 0.01570818505382233, 'hs': 13, 'dropout': 0.05396234963109715}. Best is trial 59 with value: 0.00225441433861844.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 13:33:03,336]\u001b[0m Trial 85 finished with value: 0.002519766797618264 and parameters: {'lr': 0.011539343787154842, 'hs': 15, 'dropout': 0.05038222337040479}. Best is trial 59 with value: 0.00225441433861844.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 13:33:15,464]\u001b[0m Trial 86 finished with value: 0.0022861596133780353 and parameters: {'lr': 0.013034580177360911, 'hs': 16, 'dropout': 0.050002279365572264}. Best is trial 59 with value: 0.00225441433861844.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 13:33:36,410]\u001b[0m Trial 87 finished with value: 0.002876233232329434 and parameters: {'lr': 0.0025195254600135983, 'hs': 16, 'dropout': 0.06441717707731977}. Best is trial 59 with value: 0.00225441433861844.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 13:33:48,341]\u001b[0m Trial 88 finished with value: 0.002671825258597712 and parameters: {'lr': 0.018678856957619918, 'hs': 16, 'dropout': 0.05056142776385223}. Best is trial 59 with value: 0.00225441433861844.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 13:34:00,775]\u001b[0m Trial 89 finished with value: 0.0023784907156105817 and parameters: {'lr': 0.013001843894264457, 'hs': 11, 'dropout': 0.06242586759211266}. Best is trial 59 with value: 0.00225441433861844.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 13:34:14,631]\u001b[0m Trial 90 finished with value: 0.0030968850092210885 and parameters: {'lr': 0.006901847805468975, 'hs': 12, 'dropout': 0.06703915488894052}. Best is trial 59 with value: 0.00225441433861844.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 13:34:27,451]\u001b[0m Trial 91 finished with value: 0.00257168608742327 and parameters: {'lr': 0.012803946935565002, 'hs': 14, 'dropout': 0.06278746816655079}. Best is trial 59 with value: 0.00225441433861844.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 13:34:39,888]\u001b[0m Trial 92 finished with value: 0.002984683049532809 and parameters: {'lr': 0.018040457300778273, 'hs': 11, 'dropout': 0.05623251832277292}. Best is trial 59 with value: 0.00225441433861844.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 13:34:51,134]\u001b[0m Trial 93 finished with value: 0.0025945488303132755 and parameters: {'lr': 0.025040194452344962, 'hs': 18, 'dropout': 0.0617474071705954}. Best is trial 59 with value: 0.00225441433861844.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 13:35:03,713]\u001b[0m Trial 94 finished with value: 0.0025905913238823118 and parameters: {'lr': 0.009667913247746487, 'hs': 13, 'dropout': 0.07184204609828262}. Best is trial 59 with value: 0.00225441433861844.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 13:35:15,714]\u001b[0m Trial 95 finished with value: 0.0029363296871884517 and parameters: {'lr': 0.026896312003751338, 'hs': 12, 'dropout': 0.06648537306516214}. Best is trial 59 with value: 0.00225441433861844.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 13:35:27,806]\u001b[0m Trial 96 finished with value: 0.0026848741307800516 and parameters: {'lr': 0.014227828424601692, 'hs': 11, 'dropout': 0.05411376626698796}. Best is trial 59 with value: 0.00225441433861844.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 13:35:41,094]\u001b[0m Trial 97 finished with value: 0.0026388166685472982 and parameters: {'lr': 0.03163493050139337, 'hs': 15, 'dropout': 0.058564611332666355}. Best is trial 59 with value: 0.00225441433861844.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 13:35:54,968]\u001b[0m Trial 98 finished with value: 0.002501888295411861 and parameters: {'lr': 0.011869398966260058, 'hs': 12, 'dropout': 0.05309218807282917}. Best is trial 59 with value: 0.00225441433861844.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 13:36:11,446]\u001b[0m Trial 99 finished with value: 0.0024678213196153258 and parameters: {'lr': 0.005322897794026632, 'hs': 14, 'dropout': 0.050278958353524264}. Best is trial 59 with value: 0.00225441433861844.\u001b[0m\n",
      "('TKAN', 1.3196156853703907e+17, 143699836.33571208, 4.602592076657459, 0.3031526665388876, 0.7794866597330243, 'FF', True, 'bce')\n",
      "('TKAN', 9.097829445937242e+16, 151787448.57061037, 12.644940323783397, 0.35244408833443863, 0.8495337003432977, 'FF', True, 'bce')\n",
      "('TKAN', 2.095232500315527e+17, 166886336.0943793, 6.992735357866713, 0.34429005034345467, 0.7131931332560114, 'FF', True, 'bce')\n",
      "('TKAN', 1.3554464927359862e+17, 148056133.64277413, 4.464776194795184, 0.31402209166470896, 0.8169440485613223, 'FF', True, 'bce')\n",
      "('TKAN', 1.6313447055278595e+17, 173499049.3670431, 8.974113051038886, 0.3695410368536224, 0.7923663906973057, 'FF', True, 'bce')\n",
      "\u001b[32m[I 2025-11-01 13:39:55,700]\u001b[0m A new study created in memory with name: no-name-28e3cd1a-1988-4c00-972d-1f899fd52e44\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 13:40:03,333]\u001b[0m Trial 0 finished with value: 2.2062730119642664 and parameters: {'lr': 0.039825208387876995, 'hs': 2, 'dropout': 0.07538681723914693}. Best is trial 0 with value: 2.2062730119642664.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 13:40:13,992]\u001b[0m Trial 1 finished with value: 2.2644859478257997 and parameters: {'lr': 0.01871132755937225, 'hs': 7, 'dropout': 0.102771374938119}. Best is trial 0 with value: 2.2062730119642664.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 13:41:00,362]\u001b[0m Trial 2 finished with value: 2.213927888433496 and parameters: {'lr': 0.05838427163060634, 'hs': 13, 'dropout': 0.09136791106649454}. Best is trial 0 with value: 2.2062730119642664.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 13:42:22,537]\u001b[0m Trial 3 finished with value: 4.361406510140771 and parameters: {'lr': 0.09203914006017198, 'hs': 5, 'dropout': 0.1476717678814244}. Best is trial 0 with value: 2.2062730119642664.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 13:43:48,703]\u001b[0m Trial 4 finished with value: 1.8161474456041298 and parameters: {'lr': 0.07444083430967612, 'hs': 20, 'dropout': 0.1125134225151653}. Best is trial 4 with value: 1.8161474456041298.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 13:43:59,211]\u001b[0m Trial 5 finished with value: 1.7924938754174857 and parameters: {'lr': 0.06376289558025801, 'hs': 22, 'dropout': 0.08373192684977981}. Best is trial 5 with value: 1.7924938754174857.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 13:44:11,882]\u001b[0m Trial 6 finished with value: 1.8592763498217562 and parameters: {'lr': 0.05056641732223348, 'hs': 26, 'dropout': 0.159619211714601}. Best is trial 5 with value: 1.7924938754174857.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 13:44:21,244]\u001b[0m Trial 7 finished with value: 1.7171376416785256 and parameters: {'lr': 0.05611731559182284, 'hs': 17, 'dropout': 0.17007236154471808}. Best is trial 7 with value: 1.7171376416785256.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 13:45:07,821]\u001b[0m Trial 8 finished with value: 3.947909510216155 and parameters: {'lr': 0.04200750177398386, 'hs': 9, 'dropout': 0.19583905223640835}. Best is trial 7 with value: 1.7171376416785256.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 13:45:19,487]\u001b[0m Trial 9 finished with value: 2.791235554319418 and parameters: {'lr': 0.007528868264417255, 'hs': 4, 'dropout': 0.1930717574865159}. Best is trial 7 with value: 1.7171376416785256.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 13:46:09,782]\u001b[0m Trial 10 finished with value: 1.9268729267881965 and parameters: {'lr': 0.09644888377646892, 'hs': 30, 'dropout': 0.05382122533531425}. Best is trial 7 with value: 1.7171376416785256.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 13:47:35,098]\u001b[0m Trial 11 finished with value: 2.386272778778597 and parameters: {'lr': 0.07052482994882364, 'hs': 20, 'dropout': 0.14435597095668673}. Best is trial 7 with value: 1.7171376416785256.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 13:47:43,200]\u001b[0m Trial 12 finished with value: 1.7525528491957847 and parameters: {'lr': 0.0704474451066109, 'hs': 15, 'dropout': 0.17021911346202492}. Best is trial 7 with value: 1.7171376416785256.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 13:47:53,109]\u001b[0m Trial 13 finished with value: 1.7732058647858018 and parameters: {'lr': 0.08096184871237741, 'hs': 14, 'dropout': 0.17120760993390505}. Best is trial 7 with value: 1.7171376416785256.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 13:48:01,637]\u001b[0m Trial 14 finished with value: 2.0643754627504958 and parameters: {'lr': 0.030315836885659787, 'hs': 14, 'dropout': 0.1763769301731942}. Best is trial 7 with value: 1.7171376416785256.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 13:48:49,757]\u001b[0m Trial 15 finished with value: 1.8003187489002885 and parameters: {'lr': 0.07962151807640323, 'hs': 17, 'dropout': 0.1392401809822889}. Best is trial 7 with value: 1.7171376416785256.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 13:48:57,762]\u001b[0m Trial 16 finished with value: 1.7427679006352783 and parameters: {'lr': 0.05556880459295497, 'hs': 11, 'dropout': 0.12424650176330425}. Best is trial 7 with value: 1.7171376416785256.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 13:49:43,130]\u001b[0m Trial 17 finished with value: 1.7812233272940485 and parameters: {'lr': 0.05357012468607365, 'hs': 11, 'dropout': 0.1279324192277986}. Best is trial 7 with value: 1.7171376416785256.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 13:49:52,356]\u001b[0m Trial 18 finished with value: 1.6982218084293184 and parameters: {'lr': 0.03231297065591378, 'hs': 25, 'dropout': 0.11609827392807218}. Best is trial 18 with value: 1.6982218084293184.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 13:50:04,061]\u001b[0m Trial 19 finished with value: 1.9692758490805844 and parameters: {'lr': 0.030426613357754467, 'hs': 26, 'dropout': 0.11587380526391218}. Best is trial 18 with value: 1.6982218084293184.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 13:50:20,562]\u001b[0m Trial 20 finished with value: 2.538237685489396 and parameters: {'lr': 0.004447328155978814, 'hs': 30, 'dropout': 0.15607083630119972}. Best is trial 18 with value: 1.6982218084293184.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 13:50:30,273]\u001b[0m Trial 21 finished with value: 1.8242937395826961 and parameters: {'lr': 0.04252115649230532, 'hs': 24, 'dropout': 0.1337174097395893}. Best is trial 18 with value: 1.6982218084293184.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 13:50:40,398]\u001b[0m Trial 22 finished with value: 1.7351639870278348 and parameters: {'lr': 0.028355686306522063, 'hs': 10, 'dropout': 0.11787825419901517}. Best is trial 18 with value: 1.6982218084293184.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 13:50:51,509]\u001b[0m Trial 23 finished with value: 1.9233998092638231 and parameters: {'lr': 0.02160476930899751, 'hs': 18, 'dropout': 0.10265177784276813}. Best is trial 18 with value: 1.6982218084293184.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 13:51:01,837]\u001b[0m Trial 24 finished with value: 1.864942777388956 and parameters: {'lr': 0.031033084284334208, 'hs': 27, 'dropout': 0.11052688865798113}. Best is trial 18 with value: 1.6982218084293184.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 13:51:14,044]\u001b[0m Trial 25 finished with value: 1.8163932980836428 and parameters: {'lr': 0.017996395093663596, 'hs': 32, 'dropout': 0.06797553960041397}. Best is trial 18 with value: 1.6982218084293184.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 13:51:25,944]\u001b[0m Trial 26 finished with value: 1.9143240387782015 and parameters: {'lr': 0.011348747739178211, 'hs': 17, 'dropout': 0.09488726810615125}. Best is trial 18 with value: 1.6982218084293184.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 13:51:35,941]\u001b[0m Trial 27 finished with value: 1.8781883725226975 and parameters: {'lr': 0.035492388912144554, 'hs': 22, 'dropout': 0.11995328498269253}. Best is trial 18 with value: 1.6982218084293184.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 13:51:46,693]\u001b[0m Trial 28 finished with value: 2.108501585236913 and parameters: {'lr': 0.024362944411100894, 'hs': 9, 'dropout': 0.15491344638603657}. Best is trial 18 with value: 1.6982218084293184.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 13:51:58,215]\u001b[0m Trial 29 finished with value: 1.722570571830034 and parameters: {'lr': 0.04517808414680991, 'hs': 20, 'dropout': 0.07439895072890709}. Best is trial 18 with value: 1.6982218084293184.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 13:52:07,485]\u001b[0m Trial 30 finished with value: 1.6208158071800511 and parameters: {'lr': 0.04496584806312198, 'hs': 23, 'dropout': 0.06650518026888956}. Best is trial 30 with value: 1.6208158071800511.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 13:52:17,090]\u001b[0m Trial 31 finished with value: 1.545928476642284 and parameters: {'lr': 0.04558384162665799, 'hs': 23, 'dropout': 0.06933932400845183}. Best is trial 31 with value: 1.545928476642284.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 13:52:27,544]\u001b[0m Trial 32 finished with value: 1.572354475075131 and parameters: {'lr': 0.06205196794781173, 'hs': 23, 'dropout': 0.06013755599255472}. Best is trial 31 with value: 1.545928476642284.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 13:52:36,026]\u001b[0m Trial 33 finished with value: 1.5118487633750972 and parameters: {'lr': 0.047673947236753174, 'hs': 24, 'dropout': 0.053614621240827734}. Best is trial 33 with value: 1.5118487633750972.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 13:53:27,924]\u001b[0m Trial 34 finished with value: 2.4288433209799147 and parameters: {'lr': 0.06217263343914231, 'hs': 23, 'dropout': 0.05481649448828942}. Best is trial 33 with value: 1.5118487633750972.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 13:53:40,265]\u001b[0m Trial 35 finished with value: 1.5393848603716596 and parameters: {'lr': 0.04679036306722131, 'hs': 29, 'dropout': 0.06415124790575033}. Best is trial 33 with value: 1.5118487633750972.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 13:53:51,645]\u001b[0m Trial 36 finished with value: 1.521520017881982 and parameters: {'lr': 0.06254133537996358, 'hs': 29, 'dropout': 0.06044850479017488}. Best is trial 33 with value: 1.5118487633750972.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 13:54:05,141]\u001b[0m Trial 37 finished with value: 1.5615133546960165 and parameters: {'lr': 0.0484455188776196, 'hs': 28, 'dropout': 0.0833552568478789}. Best is trial 33 with value: 1.5118487633750972.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 13:54:54,548]\u001b[0m Trial 38 finished with value: 3.1073661647201454 and parameters: {'lr': 0.04978170488993253, 'hs': 29, 'dropout': 0.07924279991823738}. Best is trial 33 with value: 1.5118487633750972.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 13:55:05,706]\u001b[0m Trial 39 finished with value: 1.543555964169436 and parameters: {'lr': 0.03855710562737975, 'hs': 32, 'dropout': 0.05026448933906738}. Best is trial 33 with value: 1.5118487633750972.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 13:55:17,684]\u001b[0m Trial 40 finished with value: 1.61336816902839 and parameters: {'lr': 0.03764246854904547, 'hs': 31, 'dropout': 0.060702414303934894}. Best is trial 33 with value: 1.5118487633750972.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 13:55:27,137]\u001b[0m Trial 41 finished with value: 1.4987208303617054 and parameters: {'lr': 0.03868426403909875, 'hs': 32, 'dropout': 0.07028362664267446}. Best is trial 41 with value: 1.4987208303617054.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 13:55:37,951]\u001b[0m Trial 42 finished with value: 1.523962152305285 and parameters: {'lr': 0.05866747121467985, 'hs': 32, 'dropout': 0.05295884634300603}. Best is trial 41 with value: 1.4987208303617054.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 13:56:35,121]\u001b[0m Trial 43 finished with value: 2.770616488399078 and parameters: {'lr': 0.05866759677017573, 'hs': 28, 'dropout': 0.06093604391937591}. Best is trial 41 with value: 1.4987208303617054.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 13:57:25,565]\u001b[0m Trial 44 finished with value: 3.257626479467348 and parameters: {'lr': 0.06938208488434698, 'hs': 30, 'dropout': 0.09093853447585856}. Best is trial 41 with value: 1.4987208303617054.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 13:57:34,408]\u001b[0m Trial 45 finished with value: 1.4887099956708731 and parameters: {'lr': 0.05248094367536342, 'hs': 32, 'dropout': 0.07423179797383433}. Best is trial 45 with value: 1.4887099956708731.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 13:58:27,025]\u001b[0m Trial 46 finished with value: 1.9085687611948405 and parameters: {'lr': 0.06555665184589908, 'hs': 32, 'dropout': 0.07428092214894108}. Best is trial 45 with value: 1.4887099956708731.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 13:59:20,722]\u001b[0m Trial 47 finished with value: 2.779932635897987 and parameters: {'lr': 0.05160597753337474, 'hs': 31, 'dropout': 0.0557455950819048}. Best is trial 45 with value: 1.4887099956708731.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 14:01:34,956]\u001b[0m Trial 48 finished with value: 4.317125479750422 and parameters: {'lr': 0.08280834914222107, 'hs': 27, 'dropout': 0.0819852732454663}. Best is trial 45 with value: 1.4887099956708731.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 14:01:44,294]\u001b[0m Trial 49 finished with value: 1.5774970232368766 and parameters: {'lr': 0.05946518455998537, 'hs': 29, 'dropout': 0.08932090697370973}. Best is trial 45 with value: 1.4887099956708731.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 14:03:16,111]\u001b[0m Trial 50 finished with value: 3.242937157615387 and parameters: {'lr': 0.07546308651335949, 'hs': 26, 'dropout': 0.06949153021555975}. Best is trial 45 with value: 1.4887099956708731.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 14:04:48,023]\u001b[0m Trial 51 finished with value: 3.004559637059798 and parameters: {'lr': 0.05593890100238972, 'hs': 29, 'dropout': 0.05037840391497698}. Best is trial 45 with value: 1.4887099956708731.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 14:05:37,233]\u001b[0m Trial 52 finished with value: 1.6127120595903783 and parameters: {'lr': 0.06707836071803144, 'hs': 31, 'dropout': 0.06504453616012046}. Best is trial 45 with value: 1.4887099956708731.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 14:05:44,954]\u001b[0m Trial 53 finished with value: 1.6632312045642763 and parameters: {'lr': 0.0525665142482054, 'hs': 28, 'dropout': 0.07640237707422773}. Best is trial 45 with value: 1.4887099956708731.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 14:05:59,303]\u001b[0m Trial 54 finished with value: 1.6212972309986462 and parameters: {'lr': 0.03981167962138424, 'hs': 30, 'dropout': 0.05945000713427937}. Best is trial 45 with value: 1.4887099956708731.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 14:06:11,429]\u001b[0m Trial 55 finished with value: 1.6082850517987601 and parameters: {'lr': 0.04824918120784031, 'hs': 32, 'dropout': 0.07145963873621752}. Best is trial 45 with value: 1.4887099956708731.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 14:06:24,728]\u001b[0m Trial 56 finished with value: 1.6285423704630204 and parameters: {'lr': 0.03473806622094152, 'hs': 25, 'dropout': 0.06370459684876416}. Best is trial 45 with value: 1.4887099956708731.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 14:07:13,654]\u001b[0m Trial 57 finished with value: 2.442983306815212 and parameters: {'lr': 0.05603473034986489, 'hs': 2, 'dropout': 0.05575540235466485}. Best is trial 45 with value: 1.4887099956708731.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 14:07:25,680]\u001b[0m Trial 58 finished with value: 1.617975279481821 and parameters: {'lr': 0.04330209909051088, 'hs': 27, 'dropout': 0.09590104468126363}. Best is trial 45 with value: 1.4887099956708731.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 14:08:18,663]\u001b[0m Trial 59 finished with value: 2.8591390776649397 and parameters: {'lr': 0.05871420095720994, 'hs': 31, 'dropout': 0.08501780582785734}. Best is trial 45 with value: 1.4887099956708731.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 14:09:13,364]\u001b[0m Trial 60 finished with value: 3.060085109523662 and parameters: {'lr': 0.07426496327153828, 'hs': 29, 'dropout': 0.050101051121041726}. Best is trial 45 with value: 1.4887099956708731.\u001b[0m\n",
      "\u001b[32m[I 2025-11-01 14:09:25,875]\u001b[0m Trial 61 finished with value: 1.6261375062434382 and parameters: {'lr': 0.03925170597891222, 'hs': 32, 'dropout': 0.05037725334534595}. Best is trial 45 with value: 1.4887099956708731.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "\u001b[32m[I 2025-10-30 06:40:33,173]\u001b[0m Trial 48 finished with value: 0.021328714886820126 and parameters: {'lr': 0.008268285387427469, 'hs': 21, 'dropout': 0.06489939096012384}. Best is trial 41 with value: 0.020010322959780336.\u001b[0m\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "\u001b[32m[I 2025-10-30 06:40:39,956]\u001b[0m Trial 49 finished with value: 0.023300488343669434 and parameters: {'lr': 0.00975474474528676, 'hs': 16, 'dropout': 0.1328667629007407}. Best is trial 41 with value: 0.020010322959780336.\u001b[0m\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "('LSTM', 1.0927421406613508e+18, 883126739.1740448, -0.4580117463560405, 'ARIMA', True, 'beta')\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "('LSTM', 1.0662118152988095e+18, 885107015.5406271, -0.4337960859656267, 'ARIMA', True, 'beta')\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "('LSTM', 1.1776624472289759e+18, 920578107.7380737, -0.4895234856619981, 'ARIMA', True, 'beta')\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "('LSTM', 1.1069019087305504e+18, 864184206.5800437, -0.33581331386467617, 'ARIMA', True, 'beta')\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "('LSTM', 8.39699806192201e+17, 842120831.8057023, -1.2924682657002373, 'ARIMA', True, 'beta')\n",
      "\u001b[32m[I 2025-10-30 06:45:59,506]\u001b[0m A new study created in memory with name: no-name-25bcc161-45bb-4b11-a83f-d1a2d30a4b29\u001b[0m\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "\u001b[32m[I 2025-10-30 06:46:07,042]\u001b[0m Trial 0 finished with value: 0.00235465897897037 and parameters: {'lr': 0.007536580977427676, 'hs': 29, 'dropout': 0.12994729704541064}. Best is trial 0 with value: 0.00235465897897037.\u001b[0m\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "\u001b[32m[I 2025-10-30 06:46:21,900]\u001b[0m Trial 1 finished with value: 0.0037050262609168824 and parameters: {'lr': 0.0012413191133765675, 'hs': 30, 'dropout': 0.1448928741057417}. Best is trial 0 with value: 0.00235465897897037.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "\u001b[32m[I 2025-10-30 06:46:33,415]\u001b[0m Trial 2 finished with value: 0.0034286828645880944 and parameters: {'lr': 0.004163862591815302, 'hs': 9, 'dropout': 0.13300965914925694}. Best is trial 0 with value: 0.00235465897897037.\u001b[0m\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "\u001b[32m[I 2025-10-30 06:46:43,009]\u001b[0m Trial 3 finished with value: 0.0022945395151650985 and parameters: {'lr': 0.005158914223915607, 'hs': 22, 'dropout': 0.06963816052774775}. Best is trial 3 with value: 0.0022945395151650985.\u001b[0m\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "\u001b[32m[I 2025-10-30 06:46:51,626]\u001b[0m Trial 4 finished with value: 0.0028996731355311503 and parameters: {'lr': 0.009798753804702218, 'hs': 11, 'dropout': 0.07991116477996292}. Best is trial 3 with value: 0.0022945395151650985.\u001b[0m\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "\u001b[32m[I 2025-10-30 06:47:02,239]\u001b[0m Trial 5 finished with value: 0.003555293130166185 and parameters: {'lr': 0.005999617357360666, 'hs': 4, 'dropout': 0.06026690848718266}. Best is trial 3 with value: 0.0022945395151650985.\u001b[0m\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "\u001b[32m[I 2025-10-30 06:47:13,748]\u001b[0m Trial 6 finished with value: 0.0030895946793250257 and parameters: {'lr': 0.003648716659283844, 'hs': 12, 'dropout': 0.10475337779081771}. Best is trial 3 with value: 0.0022945395151650985.\u001b[0m\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "\u001b[32m[I 2025-10-30 06:47:23,592]\u001b[0m Trial 7 finished with value: 0.002467835586890921 and parameters: {'lr': 0.005023493935671683, 'hs': 16, 'dropout': 0.0874397190344825}. Best is trial 3 with value: 0.0022945395151650985.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "\u001b[32m[I 2025-10-30 06:47:32,770]\u001b[0m Trial 8 finished with value: 0.003446738215887767 and parameters: {'lr': 0.009135221220665766, 'hs': 9, 'dropout': 0.16587560209252866}. Best is trial 3 with value: 0.0022945395151650985.\u001b[0m\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "\u001b[32m[I 2025-10-30 06:47:48,471]\u001b[0m Trial 9 finished with value: 0.0031217801730974077 and parameters: {'lr': 0.0015500035249650858, 'hs': 26, 'dropout': 0.084965982545608}. Best is trial 3 with value: 0.0022945395151650985.\u001b[0m\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "\u001b[32m[I 2025-10-30 06:47:55,210]\u001b[0m Trial 10 finished with value: 0.002613503609926978 and parameters: {'lr': 0.007094090211905547, 'hs': 22, 'dropout': 0.1855346174842241}. Best is trial 3 with value: 0.0022945395151650985.\u001b[0m\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "\u001b[32m[I 2025-10-30 06:48:00,615]\u001b[0m Trial 11 finished with value: 0.002250085794637469 and parameters: {'lr': 0.007652171255965646, 'hs': 32, 'dropout': 0.1170396292868759}. Best is trial 11 with value: 0.002250085794637469.\u001b[0m\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "\u001b[32m[I 2025-10-30 06:48:07,911]\u001b[0m Trial 12 finished with value: 0.002191200730187747 and parameters: {'lr': 0.007658273751800081, 'hs': 23, 'dropout': 0.050516240249409955}. Best is trial 12 with value: 0.002191200730187747.\u001b[0m\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "\u001b[32m[I 2025-10-30 06:48:16,816]\u001b[0m Trial 13 finished with value: 0.002343969709945978 and parameters: {'lr': 0.008111877882893038, 'hs': 23, 'dropout': 0.051751142939108534}. Best is trial 12 with value: 0.002191200730187747.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "\u001b[32m[I 2025-10-30 06:48:25,839]\u001b[0m Trial 14 finished with value: 0.002314099623201893 and parameters: {'lr': 0.006951845258638331, 'hs': 32, 'dropout': 0.10379098327996372}. Best is trial 12 with value: 0.002191200730187747.\u001b[0m\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "\u001b[32m[I 2025-10-30 06:48:34,260]\u001b[0m Trial 15 finished with value: 0.002733991912038897 and parameters: {'lr': 0.00896746931979562, 'hs': 17, 'dropout': 0.10777068458093647}. Best is trial 12 with value: 0.002191200730187747.\u001b[0m\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "\u001b[32m[I 2025-10-30 06:48:41,570]\u001b[0m Trial 16 finished with value: 0.002525863076972354 and parameters: {'lr': 0.0060804456134143136, 'hs': 27, 'dropout': 0.1555014696685152}. Best is trial 12 with value: 0.002191200730187747.\u001b[0m\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "\u001b[32m[I 2025-10-30 06:48:52,607]\u001b[0m Trial 17 finished with value: 0.0031750793109985454 and parameters: {'lr': 0.0028803008189848397, 'hs': 24, 'dropout': 0.17365200415501264}. Best is trial 12 with value: 0.002191200730187747.\u001b[0m\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "\u001b[32m[I 2025-10-30 06:48:59,491]\u001b[0m Trial 18 finished with value: 0.0026924671899461947 and parameters: {'lr': 0.008410598271307551, 'hs': 19, 'dropout': 0.19290807850581404}. Best is trial 12 with value: 0.002191200730187747.\u001b[0m\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "\u001b[32m[I 2025-10-30 06:49:39,347]\u001b[0m Trial 19 finished with value: 0.01526645627525966 and parameters: {'lr': 9.206780485355136e-05, 'hs': 32, 'dropout': 0.11653196845569651}. Best is trial 12 with value: 0.002191200730187747.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "\u001b[32m[I 2025-10-30 06:49:46,994]\u001b[0m Trial 20 finished with value: 0.0025183720365582925 and parameters: {'lr': 0.009902928148737718, 'hs': 19, 'dropout': 0.14156567195106431}. Best is trial 12 with value: 0.002191200730187747.\u001b[0m\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "\u001b[32m[I 2025-10-30 06:49:56,085]\u001b[0m Trial 21 finished with value: 0.002183974295126214 and parameters: {'lr': 0.0058440943592461695, 'hs': 26, 'dropout': 0.06705726317415453}. Best is trial 21 with value: 0.002183974295126214.\u001b[0m\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "\u001b[32m[I 2025-10-30 06:50:05,334]\u001b[0m Trial 22 finished with value: 0.002212963162778842 and parameters: {'lr': 0.006062121286814598, 'hs': 27, 'dropout': 0.06879368166561822}. Best is trial 21 with value: 0.002183974295126214.\u001b[0m\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "\u001b[32m[I 2025-10-30 06:50:11,463]\u001b[0m Trial 23 finished with value: 0.0021919473411977472 and parameters: {'lr': 0.006301616786833091, 'hs': 26, 'dropout': 0.06655598041729667}. Best is trial 21 with value: 0.002183974295126214.\u001b[0m\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "\u001b[32m[I 2025-10-30 06:50:21,077]\u001b[0m Trial 24 finished with value: 0.002128556279837506 and parameters: {'lr': 0.006496165860130108, 'hs': 25, 'dropout': 0.050285006173618314}. Best is trial 24 with value: 0.002128556279837506.\u001b[0m\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "\u001b[32m[I 2025-10-30 06:50:30,972]\u001b[0m Trial 25 finished with value: 0.002348126120887396 and parameters: {'lr': 0.004375259687448014, 'hs': 21, 'dropout': 0.05017459429407485}. Best is trial 24 with value: 0.002128556279837506.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "\u001b[32m[I 2025-10-30 06:50:39,900]\u001b[0m Trial 26 finished with value: 0.0024658624151274178 and parameters: {'lr': 0.005328177890917322, 'hs': 25, 'dropout': 0.0909589335990904}. Best is trial 24 with value: 0.002128556279837506.\u001b[0m\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "\u001b[32m[I 2025-10-30 06:50:48,499]\u001b[0m Trial 27 finished with value: 0.0023935727176527507 and parameters: {'lr': 0.0067888679743189915, 'hs': 29, 'dropout': 0.059261361275246636}. Best is trial 24 with value: 0.002128556279837506.\u001b[0m\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "\u001b[32m[I 2025-10-30 06:50:56,789]\u001b[0m Trial 28 finished with value: 0.002393999189180559 and parameters: {'lr': 0.00800793282691575, 'hs': 19, 'dropout': 0.07923325958005795}. Best is trial 24 with value: 0.002128556279837506.\u001b[0m\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "\u001b[32m[I 2025-10-30 06:51:05,768]\u001b[0m Trial 29 finished with value: 0.0022230819615842623 and parameters: {'lr': 0.007404121973750125, 'hs': 29, 'dropout': 0.07381552801441622}. Best is trial 24 with value: 0.002128556279837506.\u001b[0m\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "\u001b[32m[I 2025-10-30 06:51:16,649]\u001b[0m Trial 30 finished with value: 0.0030061809910398606 and parameters: {'lr': 0.0027830682478292754, 'hs': 15, 'dropout': 0.09427476566076651}. Best is trial 24 with value: 0.002128556279837506.\u001b[0m\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "\u001b[32m[I 2025-10-30 06:51:25,691]\u001b[0m Trial 31 finished with value: 0.0023329827584210425 and parameters: {'lr': 0.006434294721416983, 'hs': 25, 'dropout': 0.06174939765747482}. Best is trial 24 with value: 0.002128556279837506.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "\u001b[32m[I 2025-10-30 06:51:35,094]\u001b[0m Trial 32 finished with value: 0.0022151891327599517 and parameters: {'lr': 0.005873481263424454, 'hs': 28, 'dropout': 0.05046302844806919}. Best is trial 24 with value: 0.002128556279837506.\u001b[0m\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "\u001b[32m[I 2025-10-30 06:51:43,099]\u001b[0m Trial 33 finished with value: 0.0024576848460857 and parameters: {'lr': 0.004203991149431666, 'hs': 24, 'dropout': 0.06389527677848868}. Best is trial 24 with value: 0.002128556279837506.\u001b[0m\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "\u001b[32m[I 2025-10-30 06:51:51,839]\u001b[0m Trial 34 finished with value: 0.002398345289220548 and parameters: {'lr': 0.005421139799213294, 'hs': 21, 'dropout': 0.07300514655284375}. Best is trial 24 with value: 0.002128556279837506.\u001b[0m\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "\u001b[32m[I 2025-10-30 06:51:59,601]\u001b[0m Trial 35 finished with value: 0.002248257228425783 and parameters: {'lr': 0.006571917482511979, 'hs': 26, 'dropout': 0.057737692009864686}. Best is trial 24 with value: 0.002128556279837506.\u001b[0m\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "\u001b[32m[I 2025-10-30 06:52:07,962]\u001b[0m Trial 36 finished with value: 0.0022581244218617337 and parameters: {'lr': 0.004668397014014714, 'hs': 20, 'dropout': 0.07890112451627561}. Best is trial 24 with value: 0.002128556279837506.\u001b[0m\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "\u001b[32m[I 2025-10-30 06:52:16,396]\u001b[0m Trial 37 finished with value: 0.002104776521535009 and parameters: {'lr': 0.008786167388712728, 'hs': 30, 'dropout': 0.06690505254916872}. Best is trial 37 with value: 0.002104776521535009.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "\u001b[32m[I 2025-10-30 06:52:24,287]\u001b[0m Trial 38 finished with value: 0.0031469232273247487 and parameters: {'lr': 0.008894613726822117, 'hs': 4, 'dropout': 0.09590003714557825}. Best is trial 37 with value: 0.002104776521535009.\u001b[0m\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "\u001b[32m[I 2025-10-30 06:52:32,829]\u001b[0m Trial 39 finished with value: 0.0021611150417767595 and parameters: {'lr': 0.008613874742267328, 'hs': 30, 'dropout': 0.05771806372304085}. Best is trial 37 with value: 0.002104776521535009.\u001b[0m\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "\u001b[32m[I 2025-10-30 06:52:40,749]\u001b[0m Trial 40 finished with value: 0.0022265167837430956 and parameters: {'lr': 0.009486882847758999, 'hs': 30, 'dropout': 0.07411649501594371}. Best is trial 37 with value: 0.002104776521535009.\u001b[0m\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "\u001b[32m[I 2025-10-30 06:52:50,031]\u001b[0m Trial 41 finished with value: 0.0021694111130156654 and parameters: {'lr': 0.00855452503324251, 'hs': 30, 'dropout': 0.05648131366997193}. Best is trial 37 with value: 0.002104776521535009.\u001b[0m\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "\u001b[32m[I 2025-10-30 06:52:58,576]\u001b[0m Trial 42 finished with value: 0.0022072333316871147 and parameters: {'lr': 0.008500630391464476, 'hs': 30, 'dropout': 0.05669964171733944}. Best is trial 37 with value: 0.002104776521535009.\u001b[0m\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "\u001b[32m[I 2025-10-30 06:53:06,726]\u001b[0m Trial 43 finished with value: 0.002406143964532289 and parameters: {'lr': 0.009255558017419936, 'hs': 30, 'dropout': 0.0674491375801014}. Best is trial 37 with value: 0.002104776521535009.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "\u001b[32m[I 2025-10-30 06:53:14,474]\u001b[0m Trial 44 finished with value: 0.009656367603658704 and parameters: {'lr': 0.008574538765234868, 'hs': 2, 'dropout': 0.08150111918566302}. Best is trial 37 with value: 0.002104776521535009.\u001b[0m\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "\u001b[32m[I 2025-10-30 06:53:21,579]\u001b[0m Trial 45 finished with value: 0.002333596630823207 and parameters: {'lr': 0.007389364542578484, 'hs': 31, 'dropout': 0.05939968659468844}. Best is trial 37 with value: 0.002104776521535009.\u001b[0m\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "\u001b[32m[I 2025-10-30 06:53:28,194]\u001b[0m Trial 46 finished with value: 0.0023942042520601696 and parameters: {'lr': 0.007896766399416161, 'hs': 28, 'dropout': 0.07127378006534549}. Best is trial 37 with value: 0.002104776521535009.\u001b[0m\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "\u001b[32m[I 2025-10-30 06:53:39,627]\u001b[0m Trial 47 finished with value: 0.0025254444977458935 and parameters: {'lr': 0.003605929644419375, 'hs': 28, 'dropout': 0.08665186700197754}. Best is trial 37 with value: 0.002104776521535009.\u001b[0m\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "\u001b[32m[I 2025-10-30 06:53:47,399]\u001b[0m Trial 48 finished with value: 0.002268328919277352 and parameters: {'lr': 0.009370638778533625, 'hs': 31, 'dropout': 0.056254135289796545}. Best is trial 37 with value: 0.002104776521535009.\u001b[0m\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "\u001b[32m[I 2025-10-30 06:53:55,097]\u001b[0m Trial 49 finished with value: 0.0022264169683208546 and parameters: {'lr': 0.00997771390154512, 'hs': 31, 'dropout': 0.06376077970199892}. Best is trial 37 with value: 0.002104776521535009.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "('LSTM', 1.7009880978826768e+17, 160554549.5048026, 0.7864915674771878, 'ARIMA', True, 'bce')\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "('LSTM', 1.4280981057223627e+17, 163217873.84986198, 0.8226182515571584, 'ARIMA', True, 'bce')\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "('LSTM', 9.2244588249776e+16, 135439789.07065275, 0.8088188559292857, 'ARIMA', True, 'bce')\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "('LSTM', 6.454218328443199e+16, 130359780.88279203, 0.8438834336939052, 'ARIMA', True, 'bce')\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "('LSTM', 1.777379012703171e+17, 169022123.03643182, 0.7307776966661192, 'ARIMA', True, 'bce')\n",
      "\u001b[32m[I 2025-10-30 06:56:18,611]\u001b[0m A new study created in memory with name: no-name-24119d69-cdca-425a-9b12-dee3c09bc6c2\u001b[0m\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1394: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.gru(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1394: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.gru(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1394: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.gru(\n",
      "\u001b[32m[I 2025-10-30 06:56:29,954]\u001b[0m Trial 0 finished with value: 0.02556765002560717 and parameters: {'lr': 0.003940241646682268, 'hs': 10, 'dropout': 0.11597374819937235}. Best is trial 0 with value: 0.02556765002560717.\u001b[0m\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1394: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.gru(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1394: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.gru(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1394: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.gru(\n",
      "\u001b[32m[I 2025-10-30 06:56:38,326]\u001b[0m Trial 1 finished with value: 0.023879473715407922 and parameters: {'lr': 0.008436947807244815, 'hs': 9, 'dropout': 0.17086761381191337}. Best is trial 1 with value: 0.023879473715407922.\u001b[0m\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1394: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.gru(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1394: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.gru(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1394: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.gru(\n",
      "\u001b[32m[I 2025-10-30 06:56:48,998]\u001b[0m Trial 2 finished with value: 0.024970798111783866 and parameters: {'lr': 0.001953209685759818, 'hs': 18, 'dropout': 0.19782526388498345}. Best is trial 1 with value: 0.023879473715407922.\u001b[0m\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1394: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.gru(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1394: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.gru(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1394: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.gru(\n",
      "\u001b[32m[I 2025-10-30 06:56:59,454]\u001b[0m Trial 3 finished with value: 0.023815077665751524 and parameters: {'lr': 0.008789558185709628, 'hs': 20, 'dropout': 0.08595390359554886}. Best is trial 3 with value: 0.023815077665751524.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1394: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.gru(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1394: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.gru(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1394: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.gru(\n",
      "\u001b[32m[I 2025-10-30 06:57:25,710]\u001b[0m Trial 4 finished with value: 0.03314149049263374 and parameters: {'lr': 0.0013501853088902732, 'hs': 3, 'dropout': 0.11715142581304074}. Best is trial 3 with value: 0.023815077665751524.\u001b[0m\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1394: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.gru(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1394: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.gru(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1394: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.gru(\n",
      "\u001b[32m[I 2025-10-30 06:57:32,897]\u001b[0m Trial 5 finished with value: 0.02485875378406942 and parameters: {'lr': 0.008515075787380354, 'hs': 19, 'dropout': 0.10619196154397939}. Best is trial 3 with value: 0.023815077665751524.\u001b[0m\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1394: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.gru(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1394: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.gru(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1394: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.gru(\n",
      "\u001b[32m[I 2025-10-30 06:57:40,814]\u001b[0m Trial 6 finished with value: 0.02224486364538847 and parameters: {'lr': 0.006043172326956716, 'hs': 16, 'dropout': 0.12525356888878603}. Best is trial 6 with value: 0.02224486364538847.\u001b[0m\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1394: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.gru(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1394: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.gru(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1394: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.gru(\n",
      "\u001b[32m[I 2025-10-30 06:57:49,889]\u001b[0m Trial 7 finished with value: 0.02535442161610596 and parameters: {'lr': 0.005661009669362265, 'hs': 12, 'dropout': 0.12883799849424554}. Best is trial 6 with value: 0.02224486364538847.\u001b[0m\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1394: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.gru(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1394: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.gru(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1394: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.gru(\n",
      "\u001b[32m[I 2025-10-30 06:57:58,950]\u001b[0m Trial 8 finished with value: 0.026783116001767766 and parameters: {'lr': 0.008904045201036869, 'hs': 10, 'dropout': 0.16609487212856505}. Best is trial 6 with value: 0.02224486364538847.\u001b[0m\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1394: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.gru(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1394: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.gru(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1394: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.gru(\n",
      "\u001b[32m[I 2025-10-30 06:58:07,007]\u001b[0m Trial 9 finished with value: 0.022081630162957725 and parameters: {'lr': 0.004220243049174236, 'hs': 24, 'dropout': 0.1401558290294213}. Best is trial 9 with value: 0.022081630162957725.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1394: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.gru(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1394: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.gru(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1394: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.gru(\n",
      "\u001b[32m[I 2025-10-30 06:58:15,795]\u001b[0m Trial 10 finished with value: 0.01969486003686791 and parameters: {'lr': 0.0036034732410865794, 'hs': 30, 'dropout': 0.06667770623617954}. Best is trial 10 with value: 0.01969486003686791.\u001b[0m\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1394: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.gru(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1394: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.gru(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1394: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.gru(\n",
      "\u001b[32m[I 2025-10-30 06:58:25,483]\u001b[0m Trial 11 finished with value: 0.020872679690659274 and parameters: {'lr': 0.003971563338117171, 'hs': 30, 'dropout': 0.07146607167600838}. Best is trial 10 with value: 0.01969486003686791.\u001b[0m\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1394: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.gru(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1394: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.gru(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1394: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.gru(\n",
      "\u001b[32m[I 2025-10-30 06:58:35,701]\u001b[0m Trial 12 finished with value: 0.021885857431733075 and parameters: {'lr': 0.0026994954114202764, 'hs': 30, 'dropout': 0.05058509702604504}. Best is trial 10 with value: 0.01969486003686791.\u001b[0m\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1394: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.gru(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1394: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.gru(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1394: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.gru(\n",
      "\u001b[32m[I 2025-10-30 06:58:55,142]\u001b[0m Trial 13 finished with value: 0.02350631826730473 and parameters: {'lr': 0.0005770668042771234, 'hs': 32, 'dropout': 0.054944922968261756}. Best is trial 10 with value: 0.01969486003686791.\u001b[0m\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1394: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.gru(\n"
     ]
    }
   ],
   "source": [
    "!python experiments.py --imputation FF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26317921",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3.10 install torchdata==0.8.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c8a0b7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "print(torch.version.cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9aa77da",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3.10 install torchvision==0.18.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd258879",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3.10 install torchtext==0.18.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc103f1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12ced98c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3.10 install torch==2.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e9856a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3.10 install sktime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0b91876",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3.10 install -U dgl -f https://data.dgl.ai/wheels/torch-2.3/cu121/repo.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c5a8cb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3.10 install pydantic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6389887e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3.10 install pytorch_forecasting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e25ece39",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3.10 install timekan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a3d6682",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3.10 install torchvision==0.24.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeea73a0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
