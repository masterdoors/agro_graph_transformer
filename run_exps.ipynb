{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66bdca18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "\u001b[32m[I 2025-10-30 06:40:33,173]\u001b[0m Trial 48 finished with value: 0.021328714886820126 and parameters: {'lr': 0.008268285387427469, 'hs': 21, 'dropout': 0.06489939096012384}. Best is trial 41 with value: 0.020010322959780336.\u001b[0m\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "\u001b[32m[I 2025-10-30 06:40:39,956]\u001b[0m Trial 49 finished with value: 0.023300488343669434 and parameters: {'lr': 0.00975474474528676, 'hs': 16, 'dropout': 0.1328667629007407}. Best is trial 41 with value: 0.020010322959780336.\u001b[0m\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "('LSTM', 1.0927421406613508e+18, 883126739.1740448, -0.4580117463560405, 'ARIMA', True, 'beta')\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "('LSTM', 1.0662118152988095e+18, 885107015.5406271, -0.4337960859656267, 'ARIMA', True, 'beta')\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "('LSTM', 1.1776624472289759e+18, 920578107.7380737, -0.4895234856619981, 'ARIMA', True, 'beta')\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "('LSTM', 1.1069019087305504e+18, 864184206.5800437, -0.33581331386467617, 'ARIMA', True, 'beta')\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "('LSTM', 8.39699806192201e+17, 842120831.8057023, -1.2924682657002373, 'ARIMA', True, 'beta')\n",
      "\u001b[32m[I 2025-10-30 06:45:59,506]\u001b[0m A new study created in memory with name: no-name-25bcc161-45bb-4b11-a83f-d1a2d30a4b29\u001b[0m\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "\u001b[32m[I 2025-10-30 06:46:07,042]\u001b[0m Trial 0 finished with value: 0.00235465897897037 and parameters: {'lr': 0.007536580977427676, 'hs': 29, 'dropout': 0.12994729704541064}. Best is trial 0 with value: 0.00235465897897037.\u001b[0m\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "\u001b[32m[I 2025-10-30 06:46:21,900]\u001b[0m Trial 1 finished with value: 0.0037050262609168824 and parameters: {'lr': 0.0012413191133765675, 'hs': 30, 'dropout': 0.1448928741057417}. Best is trial 0 with value: 0.00235465897897037.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "\u001b[32m[I 2025-10-30 06:46:33,415]\u001b[0m Trial 2 finished with value: 0.0034286828645880944 and parameters: {'lr': 0.004163862591815302, 'hs': 9, 'dropout': 0.13300965914925694}. Best is trial 0 with value: 0.00235465897897037.\u001b[0m\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "\u001b[32m[I 2025-10-30 06:46:43,009]\u001b[0m Trial 3 finished with value: 0.0022945395151650985 and parameters: {'lr': 0.005158914223915607, 'hs': 22, 'dropout': 0.06963816052774775}. Best is trial 3 with value: 0.0022945395151650985.\u001b[0m\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "\u001b[32m[I 2025-10-30 06:46:51,626]\u001b[0m Trial 4 finished with value: 0.0028996731355311503 and parameters: {'lr': 0.009798753804702218, 'hs': 11, 'dropout': 0.07991116477996292}. Best is trial 3 with value: 0.0022945395151650985.\u001b[0m\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "\u001b[32m[I 2025-10-30 06:47:02,239]\u001b[0m Trial 5 finished with value: 0.003555293130166185 and parameters: {'lr': 0.005999617357360666, 'hs': 4, 'dropout': 0.06026690848718266}. Best is trial 3 with value: 0.0022945395151650985.\u001b[0m\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "\u001b[32m[I 2025-10-30 06:47:13,748]\u001b[0m Trial 6 finished with value: 0.0030895946793250257 and parameters: {'lr': 0.003648716659283844, 'hs': 12, 'dropout': 0.10475337779081771}. Best is trial 3 with value: 0.0022945395151650985.\u001b[0m\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "\u001b[32m[I 2025-10-30 06:47:23,592]\u001b[0m Trial 7 finished with value: 0.002467835586890921 and parameters: {'lr': 0.005023493935671683, 'hs': 16, 'dropout': 0.0874397190344825}. Best is trial 3 with value: 0.0022945395151650985.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "\u001b[32m[I 2025-10-30 06:47:32,770]\u001b[0m Trial 8 finished with value: 0.003446738215887767 and parameters: {'lr': 0.009135221220665766, 'hs': 9, 'dropout': 0.16587560209252866}. Best is trial 3 with value: 0.0022945395151650985.\u001b[0m\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "\u001b[32m[I 2025-10-30 06:47:48,471]\u001b[0m Trial 9 finished with value: 0.0031217801730974077 and parameters: {'lr': 0.0015500035249650858, 'hs': 26, 'dropout': 0.084965982545608}. Best is trial 3 with value: 0.0022945395151650985.\u001b[0m\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "\u001b[32m[I 2025-10-30 06:47:55,210]\u001b[0m Trial 10 finished with value: 0.002613503609926978 and parameters: {'lr': 0.007094090211905547, 'hs': 22, 'dropout': 0.1855346174842241}. Best is trial 3 with value: 0.0022945395151650985.\u001b[0m\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "\u001b[32m[I 2025-10-30 06:48:00,615]\u001b[0m Trial 11 finished with value: 0.002250085794637469 and parameters: {'lr': 0.007652171255965646, 'hs': 32, 'dropout': 0.1170396292868759}. Best is trial 11 with value: 0.002250085794637469.\u001b[0m\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "\u001b[32m[I 2025-10-30 06:48:07,911]\u001b[0m Trial 12 finished with value: 0.002191200730187747 and parameters: {'lr': 0.007658273751800081, 'hs': 23, 'dropout': 0.050516240249409955}. Best is trial 12 with value: 0.002191200730187747.\u001b[0m\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "\u001b[32m[I 2025-10-30 06:48:16,816]\u001b[0m Trial 13 finished with value: 0.002343969709945978 and parameters: {'lr': 0.008111877882893038, 'hs': 23, 'dropout': 0.051751142939108534}. Best is trial 12 with value: 0.002191200730187747.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "\u001b[32m[I 2025-10-30 06:48:25,839]\u001b[0m Trial 14 finished with value: 0.002314099623201893 and parameters: {'lr': 0.006951845258638331, 'hs': 32, 'dropout': 0.10379098327996372}. Best is trial 12 with value: 0.002191200730187747.\u001b[0m\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "\u001b[32m[I 2025-10-30 06:48:34,260]\u001b[0m Trial 15 finished with value: 0.002733991912038897 and parameters: {'lr': 0.00896746931979562, 'hs': 17, 'dropout': 0.10777068458093647}. Best is trial 12 with value: 0.002191200730187747.\u001b[0m\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "\u001b[32m[I 2025-10-30 06:48:41,570]\u001b[0m Trial 16 finished with value: 0.002525863076972354 and parameters: {'lr': 0.0060804456134143136, 'hs': 27, 'dropout': 0.1555014696685152}. Best is trial 12 with value: 0.002191200730187747.\u001b[0m\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "\u001b[32m[I 2025-10-30 06:48:52,607]\u001b[0m Trial 17 finished with value: 0.0031750793109985454 and parameters: {'lr': 0.0028803008189848397, 'hs': 24, 'dropout': 0.17365200415501264}. Best is trial 12 with value: 0.002191200730187747.\u001b[0m\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "\u001b[32m[I 2025-10-30 06:48:59,491]\u001b[0m Trial 18 finished with value: 0.0026924671899461947 and parameters: {'lr': 0.008410598271307551, 'hs': 19, 'dropout': 0.19290807850581404}. Best is trial 12 with value: 0.002191200730187747.\u001b[0m\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "\u001b[32m[I 2025-10-30 06:49:39,347]\u001b[0m Trial 19 finished with value: 0.01526645627525966 and parameters: {'lr': 9.206780485355136e-05, 'hs': 32, 'dropout': 0.11653196845569651}. Best is trial 12 with value: 0.002191200730187747.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "\u001b[32m[I 2025-10-30 06:49:46,994]\u001b[0m Trial 20 finished with value: 0.0025183720365582925 and parameters: {'lr': 0.009902928148737718, 'hs': 19, 'dropout': 0.14156567195106431}. Best is trial 12 with value: 0.002191200730187747.\u001b[0m\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "\u001b[32m[I 2025-10-30 06:49:56,085]\u001b[0m Trial 21 finished with value: 0.002183974295126214 and parameters: {'lr': 0.0058440943592461695, 'hs': 26, 'dropout': 0.06705726317415453}. Best is trial 21 with value: 0.002183974295126214.\u001b[0m\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "\u001b[32m[I 2025-10-30 06:50:05,334]\u001b[0m Trial 22 finished with value: 0.002212963162778842 and parameters: {'lr': 0.006062121286814598, 'hs': 27, 'dropout': 0.06879368166561822}. Best is trial 21 with value: 0.002183974295126214.\u001b[0m\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "\u001b[32m[I 2025-10-30 06:50:11,463]\u001b[0m Trial 23 finished with value: 0.0021919473411977472 and parameters: {'lr': 0.006301616786833091, 'hs': 26, 'dropout': 0.06655598041729667}. Best is trial 21 with value: 0.002183974295126214.\u001b[0m\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "\u001b[32m[I 2025-10-30 06:50:21,077]\u001b[0m Trial 24 finished with value: 0.002128556279837506 and parameters: {'lr': 0.006496165860130108, 'hs': 25, 'dropout': 0.050285006173618314}. Best is trial 24 with value: 0.002128556279837506.\u001b[0m\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "\u001b[32m[I 2025-10-30 06:50:30,972]\u001b[0m Trial 25 finished with value: 0.002348126120887396 and parameters: {'lr': 0.004375259687448014, 'hs': 21, 'dropout': 0.05017459429407485}. Best is trial 24 with value: 0.002128556279837506.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "\u001b[32m[I 2025-10-30 06:50:39,900]\u001b[0m Trial 26 finished with value: 0.0024658624151274178 and parameters: {'lr': 0.005328177890917322, 'hs': 25, 'dropout': 0.0909589335990904}. Best is trial 24 with value: 0.002128556279837506.\u001b[0m\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "\u001b[32m[I 2025-10-30 06:50:48,499]\u001b[0m Trial 27 finished with value: 0.0023935727176527507 and parameters: {'lr': 0.0067888679743189915, 'hs': 29, 'dropout': 0.059261361275246636}. Best is trial 24 with value: 0.002128556279837506.\u001b[0m\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "\u001b[32m[I 2025-10-30 06:50:56,789]\u001b[0m Trial 28 finished with value: 0.002393999189180559 and parameters: {'lr': 0.00800793282691575, 'hs': 19, 'dropout': 0.07923325958005795}. Best is trial 24 with value: 0.002128556279837506.\u001b[0m\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "\u001b[32m[I 2025-10-30 06:51:05,768]\u001b[0m Trial 29 finished with value: 0.0022230819615842623 and parameters: {'lr': 0.007404121973750125, 'hs': 29, 'dropout': 0.07381552801441622}. Best is trial 24 with value: 0.002128556279837506.\u001b[0m\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "\u001b[32m[I 2025-10-30 06:51:16,649]\u001b[0m Trial 30 finished with value: 0.0030061809910398606 and parameters: {'lr': 0.0027830682478292754, 'hs': 15, 'dropout': 0.09427476566076651}. Best is trial 24 with value: 0.002128556279837506.\u001b[0m\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "\u001b[32m[I 2025-10-30 06:51:25,691]\u001b[0m Trial 31 finished with value: 0.0023329827584210425 and parameters: {'lr': 0.006434294721416983, 'hs': 25, 'dropout': 0.06174939765747482}. Best is trial 24 with value: 0.002128556279837506.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "\u001b[32m[I 2025-10-30 06:51:35,094]\u001b[0m Trial 32 finished with value: 0.0022151891327599517 and parameters: {'lr': 0.005873481263424454, 'hs': 28, 'dropout': 0.05046302844806919}. Best is trial 24 with value: 0.002128556279837506.\u001b[0m\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "\u001b[32m[I 2025-10-30 06:51:43,099]\u001b[0m Trial 33 finished with value: 0.0024576848460857 and parameters: {'lr': 0.004203991149431666, 'hs': 24, 'dropout': 0.06389527677848868}. Best is trial 24 with value: 0.002128556279837506.\u001b[0m\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "\u001b[32m[I 2025-10-30 06:51:51,839]\u001b[0m Trial 34 finished with value: 0.002398345289220548 and parameters: {'lr': 0.005421139799213294, 'hs': 21, 'dropout': 0.07300514655284375}. Best is trial 24 with value: 0.002128556279837506.\u001b[0m\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "\u001b[32m[I 2025-10-30 06:51:59,601]\u001b[0m Trial 35 finished with value: 0.002248257228425783 and parameters: {'lr': 0.006571917482511979, 'hs': 26, 'dropout': 0.057737692009864686}. Best is trial 24 with value: 0.002128556279837506.\u001b[0m\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "\u001b[32m[I 2025-10-30 06:52:07,962]\u001b[0m Trial 36 finished with value: 0.0022581244218617337 and parameters: {'lr': 0.004668397014014714, 'hs': 20, 'dropout': 0.07890112451627561}. Best is trial 24 with value: 0.002128556279837506.\u001b[0m\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "\u001b[32m[I 2025-10-30 06:52:16,396]\u001b[0m Trial 37 finished with value: 0.002104776521535009 and parameters: {'lr': 0.008786167388712728, 'hs': 30, 'dropout': 0.06690505254916872}. Best is trial 37 with value: 0.002104776521535009.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "\u001b[32m[I 2025-10-30 06:52:24,287]\u001b[0m Trial 38 finished with value: 0.0031469232273247487 and parameters: {'lr': 0.008894613726822117, 'hs': 4, 'dropout': 0.09590003714557825}. Best is trial 37 with value: 0.002104776521535009.\u001b[0m\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "\u001b[32m[I 2025-10-30 06:52:32,829]\u001b[0m Trial 39 finished with value: 0.0021611150417767595 and parameters: {'lr': 0.008613874742267328, 'hs': 30, 'dropout': 0.05771806372304085}. Best is trial 37 with value: 0.002104776521535009.\u001b[0m\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "\u001b[32m[I 2025-10-30 06:52:40,749]\u001b[0m Trial 40 finished with value: 0.0022265167837430956 and parameters: {'lr': 0.009486882847758999, 'hs': 30, 'dropout': 0.07411649501594371}. Best is trial 37 with value: 0.002104776521535009.\u001b[0m\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "\u001b[32m[I 2025-10-30 06:52:50,031]\u001b[0m Trial 41 finished with value: 0.0021694111130156654 and parameters: {'lr': 0.00855452503324251, 'hs': 30, 'dropout': 0.05648131366997193}. Best is trial 37 with value: 0.002104776521535009.\u001b[0m\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "\u001b[32m[I 2025-10-30 06:52:58,576]\u001b[0m Trial 42 finished with value: 0.0022072333316871147 and parameters: {'lr': 0.008500630391464476, 'hs': 30, 'dropout': 0.05669964171733944}. Best is trial 37 with value: 0.002104776521535009.\u001b[0m\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "\u001b[32m[I 2025-10-30 06:53:06,726]\u001b[0m Trial 43 finished with value: 0.002406143964532289 and parameters: {'lr': 0.009255558017419936, 'hs': 30, 'dropout': 0.0674491375801014}. Best is trial 37 with value: 0.002104776521535009.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "\u001b[32m[I 2025-10-30 06:53:14,474]\u001b[0m Trial 44 finished with value: 0.009656367603658704 and parameters: {'lr': 0.008574538765234868, 'hs': 2, 'dropout': 0.08150111918566302}. Best is trial 37 with value: 0.002104776521535009.\u001b[0m\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "\u001b[32m[I 2025-10-30 06:53:21,579]\u001b[0m Trial 45 finished with value: 0.002333596630823207 and parameters: {'lr': 0.007389364542578484, 'hs': 31, 'dropout': 0.05939968659468844}. Best is trial 37 with value: 0.002104776521535009.\u001b[0m\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "\u001b[32m[I 2025-10-30 06:53:28,194]\u001b[0m Trial 46 finished with value: 0.0023942042520601696 and parameters: {'lr': 0.007896766399416161, 'hs': 28, 'dropout': 0.07127378006534549}. Best is trial 37 with value: 0.002104776521535009.\u001b[0m\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "\u001b[32m[I 2025-10-30 06:53:39,627]\u001b[0m Trial 47 finished with value: 0.0025254444977458935 and parameters: {'lr': 0.003605929644419375, 'hs': 28, 'dropout': 0.08665186700197754}. Best is trial 37 with value: 0.002104776521535009.\u001b[0m\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "\u001b[32m[I 2025-10-30 06:53:47,399]\u001b[0m Trial 48 finished with value: 0.002268328919277352 and parameters: {'lr': 0.009370638778533625, 'hs': 31, 'dropout': 0.056254135289796545}. Best is trial 37 with value: 0.002104776521535009.\u001b[0m\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "\u001b[32m[I 2025-10-30 06:53:55,097]\u001b[0m Trial 49 finished with value: 0.0022264169683208546 and parameters: {'lr': 0.00997771390154512, 'hs': 31, 'dropout': 0.06376077970199892}. Best is trial 37 with value: 0.002104776521535009.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "('LSTM', 1.7009880978826768e+17, 160554549.5048026, 0.7864915674771878, 'ARIMA', True, 'bce')\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "('LSTM', 1.4280981057223627e+17, 163217873.84986198, 0.8226182515571584, 'ARIMA', True, 'bce')\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "('LSTM', 9.2244588249776e+16, 135439789.07065275, 0.8088188559292857, 'ARIMA', True, 'bce')\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "('LSTM', 6.454218328443199e+16, 130359780.88279203, 0.8438834336939052, 'ARIMA', True, 'bce')\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1127: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.lstm(\n",
      "('LSTM', 1.777379012703171e+17, 169022123.03643182, 0.7307776966661192, 'ARIMA', True, 'bce')\n",
      "\u001b[32m[I 2025-10-30 06:56:18,611]\u001b[0m A new study created in memory with name: no-name-24119d69-cdca-425a-9b12-dee3c09bc6c2\u001b[0m\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1394: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.gru(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1394: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.gru(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1394: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.gru(\n",
      "\u001b[32m[I 2025-10-30 06:56:29,954]\u001b[0m Trial 0 finished with value: 0.02556765002560717 and parameters: {'lr': 0.003940241646682268, 'hs': 10, 'dropout': 0.11597374819937235}. Best is trial 0 with value: 0.02556765002560717.\u001b[0m\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1394: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.gru(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1394: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.gru(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1394: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.gru(\n",
      "\u001b[32m[I 2025-10-30 06:56:38,326]\u001b[0m Trial 1 finished with value: 0.023879473715407922 and parameters: {'lr': 0.008436947807244815, 'hs': 9, 'dropout': 0.17086761381191337}. Best is trial 1 with value: 0.023879473715407922.\u001b[0m\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1394: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.gru(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1394: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.gru(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1394: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.gru(\n",
      "\u001b[32m[I 2025-10-30 06:56:48,998]\u001b[0m Trial 2 finished with value: 0.024970798111783866 and parameters: {'lr': 0.001953209685759818, 'hs': 18, 'dropout': 0.19782526388498345}. Best is trial 1 with value: 0.023879473715407922.\u001b[0m\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1394: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.gru(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1394: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.gru(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1394: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.gru(\n",
      "\u001b[32m[I 2025-10-30 06:56:59,454]\u001b[0m Trial 3 finished with value: 0.023815077665751524 and parameters: {'lr': 0.008789558185709628, 'hs': 20, 'dropout': 0.08595390359554886}. Best is trial 3 with value: 0.023815077665751524.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1394: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.gru(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1394: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.gru(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1394: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.gru(\n",
      "\u001b[32m[I 2025-10-30 06:57:25,710]\u001b[0m Trial 4 finished with value: 0.03314149049263374 and parameters: {'lr': 0.0013501853088902732, 'hs': 3, 'dropout': 0.11715142581304074}. Best is trial 3 with value: 0.023815077665751524.\u001b[0m\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1394: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.gru(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1394: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.gru(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1394: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.gru(\n",
      "\u001b[32m[I 2025-10-30 06:57:32,897]\u001b[0m Trial 5 finished with value: 0.02485875378406942 and parameters: {'lr': 0.008515075787380354, 'hs': 19, 'dropout': 0.10619196154397939}. Best is trial 3 with value: 0.023815077665751524.\u001b[0m\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1394: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.gru(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1394: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.gru(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1394: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.gru(\n",
      "\u001b[32m[I 2025-10-30 06:57:40,814]\u001b[0m Trial 6 finished with value: 0.02224486364538847 and parameters: {'lr': 0.006043172326956716, 'hs': 16, 'dropout': 0.12525356888878603}. Best is trial 6 with value: 0.02224486364538847.\u001b[0m\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1394: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.gru(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1394: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.gru(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1394: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.gru(\n",
      "\u001b[32m[I 2025-10-30 06:57:49,889]\u001b[0m Trial 7 finished with value: 0.02535442161610596 and parameters: {'lr': 0.005661009669362265, 'hs': 12, 'dropout': 0.12883799849424554}. Best is trial 6 with value: 0.02224486364538847.\u001b[0m\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1394: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.gru(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1394: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.gru(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1394: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.gru(\n",
      "\u001b[32m[I 2025-10-30 06:57:58,950]\u001b[0m Trial 8 finished with value: 0.026783116001767766 and parameters: {'lr': 0.008904045201036869, 'hs': 10, 'dropout': 0.16609487212856505}. Best is trial 6 with value: 0.02224486364538847.\u001b[0m\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1394: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.gru(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1394: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.gru(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1394: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.gru(\n",
      "\u001b[32m[I 2025-10-30 06:58:07,007]\u001b[0m Trial 9 finished with value: 0.022081630162957725 and parameters: {'lr': 0.004220243049174236, 'hs': 24, 'dropout': 0.1401558290294213}. Best is trial 9 with value: 0.022081630162957725.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1394: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.gru(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1394: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.gru(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1394: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.gru(\n",
      "\u001b[32m[I 2025-10-30 06:58:15,795]\u001b[0m Trial 10 finished with value: 0.01969486003686791 and parameters: {'lr': 0.0036034732410865794, 'hs': 30, 'dropout': 0.06667770623617954}. Best is trial 10 with value: 0.01969486003686791.\u001b[0m\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1394: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.gru(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1394: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.gru(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1394: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.gru(\n",
      "\u001b[32m[I 2025-10-30 06:58:25,483]\u001b[0m Trial 11 finished with value: 0.020872679690659274 and parameters: {'lr': 0.003971563338117171, 'hs': 30, 'dropout': 0.07146607167600838}. Best is trial 10 with value: 0.01969486003686791.\u001b[0m\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1394: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.gru(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1394: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.gru(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1394: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.gru(\n",
      "\u001b[32m[I 2025-10-30 06:58:35,701]\u001b[0m Trial 12 finished with value: 0.021885857431733075 and parameters: {'lr': 0.0026994954114202764, 'hs': 30, 'dropout': 0.05058509702604504}. Best is trial 10 with value: 0.01969486003686791.\u001b[0m\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1394: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.gru(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1394: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.gru(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1394: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.gru(\n",
      "\u001b[32m[I 2025-10-30 06:58:55,142]\u001b[0m Trial 13 finished with value: 0.02350631826730473 and parameters: {'lr': 0.0005770668042771234, 'hs': 32, 'dropout': 0.054944922968261756}. Best is trial 10 with value: 0.01969486003686791.\u001b[0m\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:1394: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1479.)\n",
      "  result = _VF.gru(\n"
     ]
    }
   ],
   "source": [
    "!python experiments.py --imputation FF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26317921",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3.10 install torchdata==0.8.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c8a0b7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "print(torch.version.cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9aa77da",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3.10 install torchvision==0.18.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd258879",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3.10 install torchtext==0.18.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc103f1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12ced98c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3.10 install torch==2.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e9856a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3.10 install sktime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0b91876",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3.10 install -U dgl -f https://data.dgl.ai/wheels/torch-2.3/cu121/repo.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c5a8cb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3.10 install pydantic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6389887e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3.10 install pytorch_forecasting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e25ece39",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3.10 install timekan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a3d6682",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3.10 install torchvision==0.24.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeea73a0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
